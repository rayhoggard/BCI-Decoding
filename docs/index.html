<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ray Hoggard">

<title>Brain-Computer Interface Movement Decoding – BCI Movement Decoding</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-985aa47af68dae11cd4d235c71fb941e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="viewport" content="width=device-width, initial-scale=1">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation"><span class="header-section-number">1.1</span> Motivation</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">1.2</span> Background</a>
  <ul class="collapse">
  <li><a href="#what-is-a-bci" id="toc-what-is-a-bci" class="nav-link" data-scroll-target="#what-is-a-bci"><span class="header-section-number">1.2.1</span> What is a BCI?</a></li>
  <li><a href="#what-is-a-support-vector-machine-svm" id="toc-what-is-a-support-vector-machine-svm" class="nav-link" data-scroll-target="#what-is-a-support-vector-machine-svm"><span class="header-section-number">1.2.2</span> What is a Support Vector Machine (SVM)?</a></li>
  <li><a href="#what-is-a-support-vector-machine-svm-1" id="toc-what-is-a-support-vector-machine-svm-1" class="nav-link" data-scroll-target="#what-is-a-support-vector-machine-svm-1"><span class="header-section-number">1.2.3</span> What is a Support Vector Machine (SVM)?</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><span class="header-section-number">2</span> Methods</a>
  <ul class="collapse">
  <li><a href="#data-acquisition" id="toc-data-acquisition" class="nav-link" data-scroll-target="#data-acquisition"><span class="header-section-number">2.1</span> Data Acquisition</a></li>
  </ul></li>
  <li><a href="#methods-1" id="toc-methods-1" class="nav-link" data-scroll-target="#methods-1"><span class="header-section-number">3</span> Methods</a>
  <ul class="collapse">
  <li><a href="#data-acquisition-1" id="toc-data-acquisition-1" class="nav-link" data-scroll-target="#data-acquisition-1"><span class="header-section-number">3.1</span> Data Acquisition</a></li>
  <li><a href="#classification-with-support-vector-machines-svms" id="toc-classification-with-support-vector-machines-svms" class="nav-link" data-scroll-target="#classification-with-support-vector-machines-svms"><span class="header-section-number">3.2</span> Classification with Support Vector Machines (SVMs)</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation"><span class="header-section-number">3.2.1</span> Mathematical Formulation</a></li>
  <li><a href="#handling-non-linearity-the-kernel-trick" id="toc-handling-non-linearity-the-kernel-trick" class="nav-link" data-scroll-target="#handling-non-linearity-the-kernel-trick"><span class="header-section-number">3.2.2</span> Handling Non-Linearity: The Kernel Trick</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Brain-Computer Interface Movement Decoding</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ray Hoggard </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>This report details the process and findings of classifying Brain-Computer Interface (BCI) data to distinguish between different movement intentions (e.g., left vs.&nbsp;right arm) using electroencephalography (EEG) signals. The primary machine learning technique explored is Support Vector Machines (SVMs). The report covers the background of BCIs and SVMs, the methodology used for data acquisition, signal processing, classification, and evaluation, the key results obtained, and a discussion of their implications, addressing all required components.</p>
  </div>
</div>


</header>


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Brain-computer interfaces (BCIs) have been explored for several decades as a potential pathway for individuals with paralysis or other disabilities to regain interaction with the world (cite: Brain Computer Interfaces, a review). However, significant challenges remain, spanning from effectively collecting brain signal data to accurately processing that data to understand user intent.</p>
<ul>
<li><p><strong>Project Goals:</strong> This project introduces an approach for tackling one key aspect of this challenge: classifying electrical data recorded from the brain via electroencephalography (EEG) to decode a person’s intended movement. Specifically, we employ a machine learning model known as a Support Vector Machine (SVM) for this classification task.</p></li>
<li><p><strong>Approach Overview:</strong> The ultimate goal is to determine whether a person intends to move their left or right hand based solely on their brain activity, even without overt physical movement. The system enabling this is the BCI, which in this project comprises two main components: the EEG system for acquiring the raw neural signals, and the SVM algorithm for interpreting these signals and making the classification decision.</p></li>
<li><p><strong>Report Scope:</strong> By the conclusion of this report, we’ll provide a robust understanding of the SVM’s performance when applied to EEG data for movement intention decoding. This includes evaluating overall classification accuracy and analyzing specific factors within the model and data that influence this performance.</p></li>
</ul>
<section id="motivation" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">1.1</span> Motivation</h2>
<p>The need for advancements in BCI technology is underscored by the significant population that could benefit. According to the 2013 US Paralysis Prevalence &amp; Health Disparities Survey, nearly 5.4 million people live with paralysis (cite: prevelance and causes of paralysis). At that time, this represented almost 1.7% of all US citizens, and the number has likely grown since. Furthermore, paralysis often correlates with significant challenges in daily life and overall well-being; the survey noted that only 15.5% of these individuals were employed, and over 30% were smokers. Given the severe effects of paralysis, technologies that offer relief and restore function are crucial for improving quality of life. BCIs stand out as one of the most promising avenues to potentially restore movement capabilities and significantly enhance independence and well-being for affected individuals (Brain-Computer interfaces, a reivew).</p>
</section>
<section id="background" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="background"><span class="header-section-number">1.2</span> Background</h2>
<section id="what-is-a-bci" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="what-is-a-bci"><span class="header-section-number">1.2.1</span> What is a BCI?</h3>
<p>Our brain is constantly producing signals electrical signals that tell our body what to do. However, this process is complex and reliant on many components in our body, any one of which can break down. In people where this has occurred, a Brain-Computer Interface (BCI) can behave as a supplementary way for the brain’s signals to reach and control the body. At its core, a BCI acts as a communication bridge, translating brain activity into commands for an external device, which bypasses the body’s normal neuromuscular pathways (BCI: a review cite). In order for the BCI to do this, we need to be able to both read and interpret the signals of our brain in meaningful ways. In this project, we focus on non-invasive BCIs that use Electroencephalography (EEG) to measure these signals.</p>
<p>EEG employs sensors placed directly on the scalp to detect the brain’s electrical potentials. Although this is non-invasive and easier than many other methods, the signals acquired present their own challenges. The electrical signals are complex, often mixed with noise (from muscle movements, eye blinks, or external interference), and significantly attenuated by the skull and scalp tissues. Furthermore, using multiple electrodes simultaneously to capture sufficient spatial information results in very high-dimensional datasets[cite: 6], meaning each data point has many components to be considered. Effectively deciphering intended commands (like “move left” vs.&nbsp;“move right”) from this noisy, high-dimensional data requires sophisticated analysis techniques, which is where machine learning becomes essential.</p>
</section>
<section id="what-is-a-support-vector-machine-svm" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="what-is-a-support-vector-machine-svm"><span class="header-section-number">1.2.2</span> What is a Support Vector Machine (SVM)?</h3>
<ul>
<li>Provide a conceptual explanation of SVMs.</li>
<li>Explain the core idea: finding an optimal separating hyperplane, maximizing the margin, and the role of support vectors.</li>
<li><strong>SVM Suitability:</strong> Explain, in your own words, <em>why</em> an SVM classifier is well-suited for this specific binary classification problem[cite: 140]. Include an explanation, in your own words, of <em>why</em> SVMs can be effective with high-dimensional data even when the number of training observations is relatively small (N &lt; D scenario)[cite: 141].</li>
<li><strong>Other Applications:</strong> Briefly describe, in your own words, other potential application areas where this SVM approach might be useful beyond BCI[cite: 141].</li>
</ul>
</section>
<section id="what-is-a-support-vector-machine-svm-1" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="what-is-a-support-vector-machine-svm-1"><span class="header-section-number">1.2.3</span> What is a Support Vector Machine (SVM)?</h3>
<p>Support Vector Machines (SVMs) are supervised machine learning models designed primarily for classification tasks, meaning they learn to assign data points to predefined categories. Given a new data point, an SVM determines which category it most likely belongs to based on patterns learned from labeled training data - meaning we feed it some starting data that’s already assigned to categories, and it learns from that. In the context of this project, our categories are simply “intended left movement” and “intended right movement.” The SVM’s job is to analyze the features extracted from an EEG signal segment and classify it as belonging to either the “left” or “right” intention category.</p>
<p>At a high level,</p>
<p>Conceptually, an SVM works by trying to find the “best” boundary that separates the data points of different classes in the feature space. Imagine it like this: you’ve got a few 2-D data points, and you plot them along the xy plane. Each data point is either red or green. Assume for now that all the red points are clustered in the top right, and all the green points in bottom left. An SVM would draw a boundary separating these data points.</p>
<div id="a27977e9" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some sample data</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Red points in Quadrant 1</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X_red <span class="op">=</span> np.random.randn(<span class="dv">10</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="fl">0.4</span> <span class="op">+</span> [<span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Green points in Quadrant 3</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X_green <span class="op">=</span> np.random.randn(<span class="dv">10</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="fl">0.4</span> <span class="op">+</span> [<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack((X_red, X_green))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>]<span class="op">*</span><span class="dv">10</span> <span class="op">+</span> [<span class="dv">1</span>]<span class="op">*</span><span class="dv">10</span>)  <span class="co"># 0: red, 1: green</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the SVM model</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">'linear'</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh to plot the decision boundary</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">500</span>), np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">500</span>))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> clf.decision_function(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: just the points</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_red[:, <span class="dv">0</span>], X_red[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Class 0 (Red)'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_green[:, <span class="dv">0</span>], X_green[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'green'</span>, label<span class="op">=</span><span class="st">'Class 1 (Green)'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Red and Green Points'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X1'</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'X2'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: with decision boundary</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_red[:, <span class="dv">0</span>], X_red[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_green[:, <span class="dv">0</span>], X_green[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.contour(xx, yy, Z, levels<span class="op">=</span>[<span class="dv">0</span>], colors<span class="op">=</span><span class="st">'blue'</span>, linewidths<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'SVM Decision Boundary'</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X1'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'X2'</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="949" height="372" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In two dimensions (like above), this is a simple line. In higher dimensions (like our 204-dimensional EEG data), this boundary is a plane or a hyperplane. What makes SVMs distinct is their strategy for finding this boundary: they aim to maximize the <em>margin</em>, which is the distance between the separating hyperplane and the closest data points from <em>each</em> class. These closest points are called “support vectors” because they essentially support or define the position of the margin and the hyperplane. This principle of maximizing the margin often leads to classifiers that generalize well to new, unseen data. While the underlying mathematics involves optimization, the core idea is finding this maximal-margin separator.</p>
<p>SVMs are particularly well-suited for the challenges presented by EEG data in this BCI project. EEG data is inherently high-dimensional, and SVMs are known to be effective in such high-dimensional spaces, potentially mitigating the “curse of dimensionality” better than some other algorithms. They can perform well even when the number of training samples is not vastly larger than the number of features, a common scenario in BCI experiments, because of the emphasis on maximizing the margin. This can also help to reduce the impact of noise on our classifier.</p>
<p>SVMs have applications beyond BCI and neuroscience. They’ve been used to classify text in various ways (such as detecting spam emails) (cite: support vector machine active learning), detect faces (cite: face recognition) and even predict geological events (cite: seismic events). As we can see, this technology is widely applicable, and its continued study has many potential benefits.</p>
</section>
</section>
</section>
<section id="methods" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Methods</h1>
<ul>
<li>Provide an overview of the methodology: data acquisition, signal processing overview, SVM classification details, and the two-level cross-validation evaluation strategy.</li>
</ul>
<section id="data-acquisition" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="data-acquisition"><span class="header-section-number">2.1</span> Data Acquisition</h2>
<ul>
<li><strong>Data Source:</strong> Describe the source and nature of the EEG data used.</li>
<li><strong>EEG Setup:</strong> Detail the specifics: number of electrodes, data channels per electrode, total feature dimensionality (e.g., 204 features)[cite: 31].</li>
<li><strong>Experimental Conditions:</strong> Explain, in your own words, the two conditions under which data was collected (Overt vs.&nbsp;Imagined movement) and <em>why both</em> are relevant to BCI research[cite: 142]. Mention expected signal differences (e.g., weaker signals in Imagined)[cite: 10].</li>
<li><strong>Data Structure:</strong> State the number of trials per class per condition (e.g., 120 trials for movement 1, 120 for movement 2, total 240 per condition)[cite: 31]. Explicitly state how you are formulating the classification problem (e.g., defining Class 1 vs.&nbsp;Class 2, acknowledging the unknown left/right mapping)[cite: 52, 154].</li>
</ul>
</section>
</section>
<section id="methods-1" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Methods</h1>
<p>Our approach involves several key steps: first, getting the specific EEG datasets for overt and imagined movements; second, using the provided feature vectors with minimal extra signal processing; third, applying SVM classifiers to distinguish between movement intention classes; and finally, evaluating the classifier performance and tuning hyperparameters using a two-level cross-validation strategy.</p>
<section id="data-acquisition-1" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="data-acquisition-1"><span class="header-section-number">3.1</span> Data Acquisition</h2>
<p>The foundation of this project is the electroencephalography (EEG) data collected from a human subject.</p>
<ul>
<li><strong>EEG Setup:</strong> The specific setup used for this data involved 102 electrodes distributed across the scalp, as seen below (with each blue point being a single electrode). Each electrode measured information related to the local electrical field gradient, providing two distinct data components (one for the x-direction gradient, one for the y-direction gradient). This resulted in a total of 204 data channels or features for each recorded time point or trial.</li>
</ul>
<div id="a135f2e9" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"assets/BCIsensor_xy.csv"</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> df.iloc[:, <span class="dv">0</span>]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df.iloc[:, <span class="dv">1</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y, color<span class="op">=</span><span class="st">"blue"</span>, s<span class="op">=</span><span class="dv">20</span>, zorder<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (xi, yi) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x, y)):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        plt.text(xi, yi, <span class="bu">str</span>(i <span class="op">+</span> <span class="dv">1</span>), fontsize<span class="op">=</span><span class="dv">8</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'bottom'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"equal"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="540" height="389" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><strong>Experimental Conditions:</strong> Data was collected under two distinct conditions:
<ol type="1">
<li><strong>Overt Movement:</strong> The subject physically moved their left or right arm. Signals recorded during overt movement are expected to be stronger and potentially easier to classify.</li>
<li><strong>Imagined Movement:</strong> The subject <em>imagined</em> moving their left or right arm but remained physically still. These signals are typically weaker but are very important for BCIs designed for individuals who cannot perform physical movements.</li>
</ol></li>
<li><strong>Data Structure:</strong> For each condition (Overt and Imagined), the dataset contains 120 trials corresponding to “movement 1” and 120 trials corresponding to “movement 2”. The specific mapping of “movement 1” and “movement 2” to the actual left or right hand movement is unknown for this dataset. This gives a total of 240 trials per condition, with each individual trial represented as a 204-dimensional feature vector, corresponding to the measurements from the 204 data channels. For classification, we treat this as a binary problem distinguishing between the two movement types.</li>
</ul>
</section>
<section id="classification-with-support-vector-machines-svms" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="classification-with-support-vector-machines-svms"><span class="header-section-number">3.2</span> Classification with Support Vector Machines (SVMs)</h2>
<p>Before we look at the results of our classifier, let’s understand what’s happening under the hood. Remember that the core task of the SVM classifier in this project is to learn a decision boundary that separates the two classes of movement intention (Movement 1 vs.&nbsp;Movement 2) based on the 204-dimensional EEG feature vectors.</p>
<section id="mathematical-formulation" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="mathematical-formulation"><span class="header-section-number">3.2.1</span> Mathematical Formulation</h3>
<p>Let our training dataset consist of <span class="math inline">\(N\)</span> trials, where each trial <span class="math inline">\(i\)</span> has a feature vector <span class="math inline">\(x_i \in \mathbb{R}^{204}\)</span> and a corresponding class label <span class="math inline">\(y_i \in \{-1, +1\}\)</span> (representing, for example, Movement 1 and Movement 2). The goal of the linear SVM is to find the optimal separating hyperplane defined by a weight vector <span class="math inline">\(w\)</span> and a bias term <span class="math inline">\(b\)</span>. The hyperplane equation is <span class="math inline">\(w^T x + b = 0\)</span>.</p>
<p>But what happens if the data overlaps, or if the true boundary between classes isn’t a straight line? In such cases, no single linear hyperplane can perfectly separate all the data points. To handle potential overlap and non-separability even in the linear case, we use the <strong>soft-margin SVM formulation</strong>. This approach seeks a hyperplane that balances maximizing the margin between the classes and minimizing the number of classification errors, allowing some data points to be misclassified or fall within the margin boundary. This is achieved by solving the following optimization problem (cite: support vector machines, author is mammon):</p>
<p><span class="math display">\[
\min_{w, b, \xi} \frac{1}{2} w^T w + C \sum_{i=1}^{N} \xi_i
\]</span></p>
<p>subject to the constraints:</p>
<p><span class="math display">\[
y_i (w^T x_i + b) \ge 1 - \xi_i, \quad \text{for } i = 1, \dots, N
\]</span> <span class="math display">\[
\xi_i \ge 0, \quad \text{for } i = 1, \dots, N
\]</span></p>
<p><strong>Terms:</strong></p>
<ul>
<li><span class="math inline">\(w\)</span>: The weight vector, which is perpendicular to the separating hyperplane. Its magnitude <span class="math inline">\(||w|| = \sqrt{w^T w}\)</span> is inversely related to the margin width. Minimizing <span class="math inline">\(\frac{1}{2} w^T w\)</span> is equivalent to maximizing the margin (<span class="math inline">\(2/||w||\)</span>).</li>
<li><span class="math inline">\(b\)</span>: The bias term, which shifts the hyperplane parallel to itself without changing its orientation.</li>
<li><span class="math inline">\(x_i\)</span>: The 204-dimensional feature vector for the <span class="math inline">\(i\)</span>-th training trial.</li>
<li><span class="math inline">\(y_i\)</span>: The class label (+1 or -1) for the <span class="math inline">\(i\)</span>-th training trial.</li>
<li><span class="math inline">\(\xi_i\)</span>: These are non-negative <strong>slack variables</strong>. <span class="math inline">\(\xi_i\)</span> represents the degree to which the <span class="math inline">\(i\)</span>-th data point violates the margin constraint. If <span class="math inline">\(\xi_i = 0\)</span>, the point is correctly classified and on or outside its correct margin boundary. If <span class="math inline">\(0 &lt; \xi_i \le 1\)</span>, the point is correctly classified but falls within the margin. If <span class="math inline">\(\xi_i &gt; 1\)</span>, the point is misclassified.</li>
<li><span class="math inline">\(C\)</span>: This is the <strong>regularization parameter</strong>. Note: in our class slides we present a slightly different, but equivalent formulation, of the SVM. In this formulation, <span class="math inline">\(C = 1 / \alpha\)</span>. It controls the trade-off between maximizing the margin (associated with term <span class="math inline">\(\frac{1}{2} w^T w\)</span>) and minimizing the classification errors (associated with term <span class="math inline">\(C \sum \xi_i\)</span>).
<ul>
<li>A <em>large</em> <span class="math inline">\(C\)</span> imposes a high cost on misclassifications (<span class="math inline">\(\xi_i &gt; 0\)</span>), pushing the SVM to fit the training data more closely, potentially leading to a smaller margin and risking overfitting.</li>
<li>A <em>small</em> <span class="math inline">\(C\)</span> imposes a lower cost on misclassifications, allowing for a wider margin even if more training points are misclassified or within the margin, which may improve generalization but risks underfitting. Finding an appropriate value for <span class="math inline">\(C\)</span> is crucial for the performance of our model and will be done through cross-validation, which is explained further on.</li>
</ul></li>
</ul>
<p><strong>Optimization Problem Type:</strong></p>
<p>This mathematical formulation constitutes what we call a <strong>convex optimization problem</strong>. A convex optimization problem is a problem in which every local minimum is also a global minimum. Generally, a function is convex if it satisfies the following: <span class="math inline">\(f(\lambda x_1 + (1 - \lambda)x_2) \leq \lambda f(x_1) + (1 - \lambda)f(x_2), \quad \forall x_1, x_2, \lambda \in [0,1]\)</span>. Basically, imagine a bowl curving upwards. To be a convex optimization problem, both the function to be minimized and the constraints we apply to that function must be convex, which, examining our formulation for the SVM, is satisfied. To be more formal, we require (cite mlss2011): 1. The objective function term <span class="math inline">\(\frac{1}{2} w^T w\)</span> is quadratic in <span class="math inline">\(w\)</span> and thus convex. 2. The term <span class="math inline">\(C \sum \xi_i\)</span> is linear in <span class="math inline">\(\xi\)</span> and thus convex. 3. The sum of these two convex terms results in a convex overall objective function. 4. The constraints <span class="math inline">\(y_i (w^T x_i + b) \ge 1 - \xi_i\)</span> and <span class="math inline">\(\xi_i \ge 0\)</span> are all linear inequalities in <span class="math inline">\(w, b, \xi\)</span>. Linear inequalities define convex feasible regions (specifically, half-spaces), and the intersection of convex sets is also convex.</p>
<p>Because both the objective function and the feasible region are convex, the entire problem is convex. The convexity of this problem helps ensure that standard optimization algorithms can find a unique optimal solution for the hyperplane.</p>
</section>
<section id="handling-non-linearity-the-kernel-trick" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="handling-non-linearity-the-kernel-trick"><span class="header-section-number">3.2.2</span> Handling Non-Linearity: The Kernel Trick</h3>
<p>The linear SVM formulation described above finds a linear boundary. However, in many real-world problems, the relationship between features and classes might be non-linear, meaning a straight line or flat plane isn’t enough to separate the classes effectively. SVMs can handle this using <strong>kernels</strong>.</p>
<p>The core idea, often called the <strong>“kernel trick,”</strong> is to implicitly map the original input features <span class="math inline">\(x_i\)</span> into a much higher-dimensional space using a mapping function <span class="math inline">\(\phi(x)\)</span> (cite: improving support vector machines). In this higher-dimensional space, the data might become linearly separable, allowing us to find a linear hyperplane there. The “trick” is that we don’t need to explicitly compute the high-dimensional coordinates <span class="math inline">\(\phi(x_i)\)</span>. Instead, kernel functions <span class="math inline">\(K(x_i, x_j)\)</span> compute the dot product of the mapped vectors directly: <span class="math inline">\(K(x_i, x_j) = \phi(x_i)^T \phi(x_j)\)</span>. Since the SVM algorithm’s solution primarily depends on dot products between feature vectors (especially in its dual formulation), we can substitute <span class="math inline">\(x_i^T x_j\)</span> with <span class="math inline">\(K(x_i, x_j)\)</span> everywhere, effectively performing the classification in the high-dimensional space without the computational cost of explicitly transforming the data.</p>
<p><strong>Common Kernels:</strong> The baseline for this project is the <strong>linear kernel</strong> (<span class="math inline">\(K(x_i, x_j) = x_i^T x_j\)</span>). However, I experimented with two other kernels to see their performance, specifically:</p>
<ul>
<li><strong>Polynomial Kernel:</strong> <span class="math inline">\(K(x_i, x_j) = (\gamma x_i^T x_j + r)^d\)</span>. This kernel can model polynomial boundaries, and introduces hyperparameters: the degree <span class="math inline">\(d\)</span>, a scaling factor <span class="math inline">\(\gamma\)</span>, and a coefficient <span class="math inline">\(r\)</span> (cite: the kernel polynomial method).</li>
<li><strong>Radial Basis Function (RBF) Kernel (Gaussian Kernel):</strong> <span class="math inline">\(K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)\)</span>. This kernel is also capable of modeling complex boundaries. Its behavior is controlled by the hyperparameter <span class="math inline">\(\gamma\)</span>, which defines how far the influence of a single training example reaches. A small <span class="math inline">\(\gamma\)</span> means a broader influence (smoother boundary), while a large <span class="math inline">\(\gamma\)</span> means a more localized influence (potentially wigglier boundary, risk of overfitting) (cite: parameter selection in…). <em>Maybe visualization, 2D illustration of an SVM hyperplane, margin, and support vectors</em></li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>