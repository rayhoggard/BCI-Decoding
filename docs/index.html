<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ray Hoggard">

<title>Decoding Movement Intentions with Brain-Computer Interfaces – BCI Movement Decoding</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-985aa47af68dae11cd4d235c71fb941e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="viewport" content="width=device-width, initial-scale=1">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation"><span class="header-section-number">1.1</span> Motivation</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">1.2</span> Background</a>
  <ul class="collapse">
  <li><a href="#what-is-a-bci" id="toc-what-is-a-bci" class="nav-link" data-scroll-target="#what-is-a-bci"><span class="header-section-number">1.2.1</span> What is a BCI?</a></li>
  <li><a href="#what-is-a-support-vector-machine-svm" id="toc-what-is-a-support-vector-machine-svm" class="nav-link" data-scroll-target="#what-is-a-support-vector-machine-svm"><span class="header-section-number">1.2.2</span> What is a Support Vector Machine (SVM)?</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><span class="header-section-number">2</span> Methods</a>
  <ul class="collapse">
  <li><a href="#data-acquisition" id="toc-data-acquisition" class="nav-link" data-scroll-target="#data-acquisition"><span class="header-section-number">2.1</span> Data Acquisition</a></li>
  <li><a href="#classification-with-support-vector-machines-svms" id="toc-classification-with-support-vector-machines-svms" class="nav-link" data-scroll-target="#classification-with-support-vector-machines-svms"><span class="header-section-number">2.2</span> Classification with Support Vector Machines (SVMs)</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation"><span class="header-section-number">2.2.1</span> Mathematical Formulation</a></li>
  <li><a href="#handling-non-linearity-the-kernel-trick" id="toc-handling-non-linearity-the-kernel-trick" class="nav-link" data-scroll-target="#handling-non-linearity-the-kernel-trick"><span class="header-section-number">2.2.2</span> Handling Non-Linearity: The Kernel Trick</a></li>
  </ul></li>
  <li><a href="#model-evaluation-two-level-cross-validation" id="toc-model-evaluation-two-level-cross-validation" class="nav-link" data-scroll-target="#model-evaluation-two-level-cross-validation"><span class="header-section-number">2.3</span> Model Evaluation: Two-Level Cross-Validation</a></li>
  <li><a href="#formulating-the-classification-problem" id="toc-formulating-the-classification-problem" class="nav-link" data-scroll-target="#formulating-the-classification-problem"><span class="header-section-number">2.4</span> Formulating the Classification Problem</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3</span> Results</a>
  <ul class="collapse">
  <li><a href="#linear-kernel-classification-performance" id="toc-linear-kernel-classification-performance" class="nav-link" data-scroll-target="#linear-kernel-classification-performance"><span class="header-section-number">3.1</span> Linear Kernel Classification Performance</a>
  <ul class="collapse">
  <li><a href="#same-train-scenarios-linear-svm" id="toc-same-train-scenarios-linear-svm" class="nav-link" data-scroll-target="#same-train-scenarios-linear-svm"><span class="header-section-number">3.1.1</span> Same-Train Scenarios (Linear SVM)</a></li>
  <li><a href="#cross-train-scenarios-linear-svm" id="toc-cross-train-scenarios-linear-svm" class="nav-link" data-scroll-target="#cross-train-scenarios-linear-svm"><span class="header-section-number">3.1.2</span> Cross-Train Scenarios (Linear SVM)</a></li>
  <li><a href="#feature-importance-linear-svm-weights" id="toc-feature-importance-linear-svm-weights" class="nav-link" data-scroll-target="#feature-importance-linear-svm-weights"><span class="header-section-number">3.1.3</span> Feature Importance (Linear SVM Weights)</a></li>
  </ul></li>
  <li><a href="#rbf-kernel-classification-performance" id="toc-rbf-kernel-classification-performance" class="nav-link" data-scroll-target="#rbf-kernel-classification-performance"><span class="header-section-number">3.2</span> RBF Kernel Classification Performance</a>
  <ul class="collapse">
  <li><a href="#same-train-scenarios-rbf-svm" id="toc-same-train-scenarios-rbf-svm" class="nav-link" data-scroll-target="#same-train-scenarios-rbf-svm"><span class="header-section-number">3.2.1</span> Same-Train Scenarios (RBF SVM)</a></li>
  <li><a href="#feature-importance-rbf-svm" id="toc-feature-importance-rbf-svm" class="nav-link" data-scroll-target="#feature-importance-rbf-svm"><span class="header-section-number">3.2.2</span> Feature Importance (RBF SVM)</a></li>
  </ul></li>
  <li><a href="#polynomial-kernel-classification-performance" id="toc-polynomial-kernel-classification-performance" class="nav-link" data-scroll-target="#polynomial-kernel-classification-performance"><span class="header-section-number">3.3</span> Polynomial Kernel Classification Performance</a>
  <ul class="collapse">
  <li><a href="#same-train-scenarios-polynomial-svm" id="toc-same-train-scenarios-polynomial-svm" class="nav-link" data-scroll-target="#same-train-scenarios-polynomial-svm"><span class="header-section-number">3.3.1</span> Same-Train Scenarios (Polynomial SVM)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">4</span> Conclusions</a></li>
  <li><a href="#collaborations" id="toc-collaborations" class="nav-link" data-scroll-target="#collaborations"><span class="header-section-number">5</span> Collaborations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Decoding Movement Intentions with Brain-Computer Interfaces</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ray Hoggard </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>In this presentation, we’ll explore how we used a machine learning technique called Support Vector Machines (SVMs) to interpret brain activity (EEG signals) and figure out whether someone intends to move their left or right hand. We looked at data from both actual physical movements and purely imagined movements, evaluating how well our SVM approach—using linear, RBF, and polynomial kernels—could distinguish between the two intentions in different scenarios. We’ll walk through our methods, including how we defined the classification problem and ensured our performance measures were reliable, and discuss what the results tell us about the potential and challenges of using SVMs for brain-computer interfaces.</p>
  </div>
</div>


</header>


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>For decades now, researchers have been exploring brain-computer interfaces (BCIs) as a potential way for individuals with paralysis or other severe motor disabilities to interact with the world again. It’s an incredible field, but it comes with significant hurdles – everything from reliably picking up brain signals to accurately figuring out what those signals actually mean in terms of user intent.</p>
<ul>
<li><p><strong>Project Goals:</strong> What we’re focusing on in this project is one specific piece of that puzzle: how can we classify electrical brain activity, measured non-invasively using electroencephalography (EEG), to understand someone’s intended movement? To do this, we employed a machine learning tool called a Support Vector Machine, or SVM, exploring different configurations (kernels) to see what works best.</p></li>
<li><p><strong>Approach Overview:</strong> Our main goal here was to see if we could determine, just by looking at EEG data, whether a person was intending to move their left hand or their right hand, even if they weren’t physically moving at all. The BCI system enabling this has two parts in our setup: the EEG system that captures the raw brain signals, and the SVM algorithm that interprets those signals and makes the left-vs-right classification.</p></li>
<li><p><strong>Report Scope:</strong> By the end of this discussion, we’ll have a clear picture of how well different SVM variants perform when applied to this EEG data for decoding movement intentions. We’ll look at the overall accuracy and ROC curves, compare linear, RBF, and polynomial kernels, and also look into some specific factors within the model and the data that influenced how well they worked.</p></li>
</ul>
<section id="motivation" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">1.1</span> Motivation</h2>
<p>Why is this so important? Well, advancements in BCIs could genuinely change lives for a vast number of people. For some perspective, the 2013 US Paralysis Prevalence &amp; Health Disparities Survey found that nearly 5.4 million people were living with paralysis <span class="citation" data-cites="armour_prevalence_2016">(<a href="#ref-armour_prevalence_2016" role="doc-biblioref">Armour et al., 2016</a>)</span>. Back then, that was almost 1.7% of the entire US population, and that number has likely only grown. Paralysis often brings profound challenges – the same survey noted that only about 15.5% of these individuals were employed, and over 30% were smokers. Given these severe impacts, technologies that can offer some relief or restore function are incredibly valuable for improving quality of life. BCIs stand out as one of the most promising paths forward to potentially restore movement capabilities and significantly enhance well-being for those affected.</p>
</section>
<section id="background" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="background"><span class="header-section-number">1.2</span> Background</h2>
<section id="what-is-a-bci" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="what-is-a-bci"><span class="header-section-number">1.2.1</span> What is a BCI?</h3>
<p>Let’s start with the basics. Our brain is constantly sending out electrical signals that orchestrate everything our body does. But this communication network relies on many biological components, and if any part breaks down, the connection can be lost. For people experiencing this, a Brain-Computer Interface (BCI) offers an alternative route, or a way for the brain’s commands to bypass the damaged pathways and still control external devices or even their own limbs <span class="citation" data-cites="nicolas-alonso_brain_2012">(<a href="#ref-nicolas-alonso_brain_2012" role="doc-biblioref">Nicolas-Alonso &amp; Gomez-Gil, 2012</a>)</span>. Ultimately, a BCI acts as a translator, converting brain activity directly into control signals. To make this happen, we need two key things: a way to ‘listen’ to the brain’s signals, and a way to make sense of them. In this project, we’re focusing on non-invasive BCIs that use Electroencephalography (EEG) to pick up those signals.</p>
<p>EEG works by placing sensors on the scalp to detect the tiny electrical fields generated by brain activity. While it has the major advantage of being non-invasive (no surgery required!), the signals we get face some challenges. They’re incredibly complex, often buried in noise (from muscle twitches, eye blinks, even electrical interference from nearby devices), and they’re weakened as they pass through the skull and scalp. Plus, to get a good spatial picture of brain activity, we use many electrodes simultaneously, which results in very high-dimensional data – meaning each snapshot of brain activity has lots of different measurements to consider. Trying to reliably pull out a specific intention, like “move left” versus “move right,” from this noisy, high-dimensional stream requires robust analysis tools. That’s where machine learning comes into play.</p>
</section>
<section id="what-is-a-support-vector-machine-svm" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="what-is-a-support-vector-machine-svm"><span class="header-section-number">1.2.2</span> What is a Support Vector Machine (SVM)?</h3>
<p>Okay, so how do we actually interpret these complex brain signals? That’s where Support Vector Machines, or SVMs, come in. SVMs are a type of supervised machine learning model that are good at classification tasks – essentially, sorting data into predefined categories. You train an SVM by showing it examples that are already labeled (like EEG snippets labeled as “intended left” or “intended right”). The SVM learns the patterns distinguishing these categories. Then, when you give it a <em>new</em>, unlabeled piece of data (a new EEG segment), it uses what it learned to predict which category that new data point belongs to. In our case, the SVM’s job is to look at the features extracted from an EEG signal and decide: “left intention” or “right intention”?</p>
<p>So, how does an SVM actually do this? Conceptually, it tries to find the “best” possible boundary to separate the data points belonging to different classes. Imagine you have a scatter plot with red dots and green dots. If the red dots are mostly in one area and the green dots in another, an SVM tries to draw a line (or, in higher dimensions, a plane or hyperplane) that separates them.</p>
<div id="fig-svm-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-svm-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="output/visualizations/SVM_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-svm-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: SVM
</figcaption>
</figure>
</div>
<p>See the plot on the right (in the imagined figure)? That blue line is the decision boundary found by the SVM. What makes SVMs special is <em>how</em> they find this boundary. They don’t just pick any line that separates the groups; they specifically look for the line that creates the largest possible “buffer zone” or <em>margin</em> between itself and the closest points from <em>each</em> class. These closest points, the ones right up against the edge of this buffer zone, are called the “support vectors” – they’re critical because they dictate exactly where the boundary and margin end up. This principle of maximizing the margin often helps the SVM generalize better, meaning it performs well not just on the data it was trained on, but also on new, unseen data. While the math behind it involves optimization, the core idea is finding this widest possible separation. We implement SVMs in our project using sklearn <span class="citation" data-cites="noauthor_svc_nodate">(<a href="#ref-noauthor_svc_nodate" role="doc-biblioref"><span>“<span>SVC</span>,”</span> n.d.</a>)</span>.</p>
<p>SVMs turn out to be perfect for the kind of EEG data we’re dealing with. EEG data, with signals from many electrodes, is naturally high-dimensional (204 dimensions in our case!). SVMs are known to handle high-dimensional spaces effectively, potentially better than some other algorithms that can suffer from the “curse of dimensionality.” They can also work well even when the number of training examples isn’t vastly larger than the number of features, which is relevant here. Plus, the focus on maximizing the margin can make the resulting classifier somewhat robust to noise, which is always a concern with EEG.</p>
<p>And SVMs aren’t just for BCIs! They’re used in all sorts of areas, like classifying text (think spam email filters) <span class="citation" data-cites="mammone_support_2009">(<a href="#ref-mammone_support_2009" role="doc-biblioref">Mammone et al., 2009</a>)</span>, recognizing faces in images <span class="citation" data-cites="guo_face_2000">(<a href="#ref-guo_face_2000" role="doc-biblioref">Guo et al., 2000</a>)</span>, and even predicting things like seismic events <span class="citation" data-cites="hearst_support_1998">(<a href="#ref-hearst_support_1998" role="doc-biblioref">Hearst et al., 1998</a>)</span>. So, it’s a versatile technology with broad applications, making its study valuable in many contexts.</p>
</section>
</section>
</section>
<section id="methods" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Methods</h1>
<p>Alright, let’s walk through how we actually tackled this classification problem. Our process involved a few key stages: getting the right EEG data, preparing it for the SVM, applying the SVM classifier itself, defining the problem formally, and then carefully evaluating how well it performed using a technique called two-level cross-validation.</p>
<section id="data-acquisition" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="data-acquisition"><span class="header-section-number">2.1</span> Data Acquisition</h2>
<p>The foundation for everything is the EEG data. This data came from a single human subject.</p>
<ul>
<li><strong>EEG Setup:</strong> The recording setup used 102 electrodes placed across the scalp – you can see the layout in the plot below (each blue dot is an electrode). Each electrode provided two pieces of information related to the local electrical field gradient (one for the x-direction, one for the y-direction). So, for every moment in time or trial we recorded, we ended up with 204 distinct data channels or features.</li>
</ul>
<div id="fig-electrodes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-electrodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="output/visualizations/electrodes.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-electrodes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Electrode Positioning
</figcaption>
</figure>
</div>
<ul>
<li><strong>Experimental Conditions:</strong> The data was collected under two different conditions:
<ol type="1">
<li><strong>Overt Movement:</strong> In this case, the subject physically moved their left or right arm. We’d expect the brain signals during actual movement to be relatively strong and potentially easier to classify.</li>
<li><strong>Imagined Movement:</strong> Here, the subject simply <em>thought about</em> moving their left or right arm, but stayed still. These signals are usually much fainter and harder to detect, but they’re crucial for BCIs intended for people who can’t physically move.</li>
</ol></li>
<li><strong>Data Structure:</strong> For both the Overt and Imagined conditions, we have a dataset containing 120 trials labeled “movement 1” and 120 trials labeled “movement 2”. Now, for this specific dataset, we don’t actually know which label corresponds to “left” and which to “right,” but for classification purposes, that doesn’t matter – we just need to distinguish between the two types. This gives us 240 trials in total for each condition (Overt and Imagined). Each single trial is represented by that 204-dimensional feature vector we talked about (the readings from all 204 data channels). Our task is essentially a binary classification problem: given a 204-dimensional vector, decide if it belongs to “movement 1” or “movement 2”.</li>
</ul>
</section>
<section id="classification-with-support-vector-machines-svms" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="classification-with-support-vector-machines-svms"><span class="header-section-number">2.2</span> Classification with Support Vector Machines (SVMs)</h2>
<p>Now, before we jump into the results, let’s look more precisely what the SVM is doing under the hood. Remember, its core job is to find that optimal boundary (a hyperplane in our 204-dimensional space) that best separates the EEG patterns corresponding to “Movement 1” from those corresponding to “Movement 2”.</p>
<section id="mathematical-formulation" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="mathematical-formulation"><span class="header-section-number">2.2.1</span> Mathematical Formulation</h3>
<p>If we have <span class="math inline">\(N\)</span> training trials, each with a feature vector <span class="math inline">\(x_i\)</span> (our 204 EEG measurements) and a known class label <span class="math inline">\(y_i\)</span> (which we’ll represent as either -1 for Movement 1 or +1 for Movement 2), a linear SVM tries to find a weight vector <span class="math inline">\(w\)</span> and a bias term <span class="math inline">\(b\)</span>. These define the separating hyperplane with the equation <span class="math inline">\(w^T x + b = 0\)</span>.</p>
<p>But what if the data isn’t perfectly separable with a straight line (or flat plane)? What if some “Movement 1” points are mixed in with “Movement 2” points? That’s where the <strong>soft-margin SVM</strong> comes in handy. Instead of insisting on perfect separation, it tries to find a balance: maximize the margin (the buffer zone) while also minimizing the number of points that end up on the wrong side of the boundary or inside the margin. This is done by solving an optimization problem <span class="citation" data-cites="hearst_support_1998">(<a href="#ref-hearst_support_1998" role="doc-biblioref">Hearst et al., 1998</a>)</span>:</p>
<p><span class="math display">\[
\min_{w, b, \xi} \frac{1}{2} w^T w + C \sum_{i=1}^{N} \xi_i
\]</span></p>
<p>subject to the constraints:</p>
<p><span class="math display">\[
y_i (w^T x_i + b) \ge 1 - \xi_i, \quad \text{for } i = 1, \dots, N
\]</span> <span class="math display">\[
\xi_i \ge 0, \quad \text{for } i = 1, \dots, N
\]</span></p>
<p>Breaking down these terms:</p>
<ul>
<li><span class="math inline">\(w\)</span>: The weight vector. It’s perpendicular to the separating boundary. Minimizing its squared length (<span class="math inline">\(\frac{1}{2} w^T w\)</span>) is equivalent to maximizing the margin width.</li>
<li><span class="math inline">\(b\)</span>: The bias term. It shifts the boundary position without changing its orientation.</li>
<li><span class="math inline">\(x_i\)</span>: The 204-dimensional feature vector for the <span class="math inline">\(i\)</span>-th trial.</li>
<li><span class="math inline">\(y_i\)</span>: The class label (+1 or -1) for the <span class="math inline">\(i\)</span>-th trial.</li>
<li><span class="math inline">\(\xi_i\)</span>: These are the <strong>slack variables</strong>. Think of them as measuring how much a data point <span class="math inline">\(i\)</span> “violates” the margin. If <span class="math inline">\(\xi_i = 0\)</span>, the point is correctly classified and outside the margin. If it’s between 0 and 1, it’s correctly classified but inside the margin. If <span class="math inline">\(\xi_i &gt; 1\)</span>, the point is actually misclassified.</li>
<li><span class="math inline">\(C\)</span>: This is the <strong>regularization parameter</strong>. (It’s equivalent to <span class="math inline">\(1/\alpha\)</span> in our class notes, and is what sklearn uses.) <span class="math inline">\(C\)</span> controls the trade-off:
<ul>
<li>A <em>large</em> <span class="math inline">\(C\)</span> puts a heavy penalty on misclassifications (large <span class="math inline">\(\xi_i\)</span>). The SVM will try very hard to classify all training points correctly, possibly leading to a narrower margin and potentially “overfitting” the training data (meaning it might not generalize well to new data).</li>
<li>A <em>small</em> <span class="math inline">\(C\)</span> is more tolerant of misclassifications. It allows for a potentially wider margin, even if some training points end up on the wrong side or within it. This might lead to better generalization but risks “underfitting” if the margin becomes too wide and ignores the underlying structure. Finding the right value for <span class="math inline">\(C\)</span> (and potentially other parameters like <span class="math inline">\(\gamma\)</span> or <span class="math inline">\(d\)</span> for non-linear kernels) is key to getting good performance, and we use a process called cross-validation (which we’ll discuss next) to do this.</li>
</ul></li>
</ul>
<p><strong>Why this works (Convex Optimization):</strong></p>
<p>This mathematical setup is what’s known as a <strong>convex optimization problem</strong>. This matters because for convex problems, we’re guaranteed that any solution we find that looks like the best locally is the globally best solution. There’s no risk of getting stuck in a suboptimal valley. The objective function (the part we’re minimizing) and the constraints (the rules we must follow) are all convex (they curve upwards like a bowl, mathematically speaking), which makes the whole problem convex <span class="citation" data-cites="hearst_support_1998">(<a href="#ref-hearst_support_1998" role="doc-biblioref">Hearst et al., 1998</a>)</span>. This mathematical property is a big reason why SVMs are so reliable and widely used – we know we can find the single best boundary according to our criteria (at least for the linear case and standard formulations).</p>
</section>
<section id="handling-non-linearity-the-kernel-trick" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="handling-non-linearity-the-kernel-trick"><span class="header-section-number">2.2.2</span> Handling Non-Linearity: The Kernel Trick</h3>
<p>The formulation we just looked at finds a <em>linear</em> boundary (a flat plane). But what if the real distinction between “Movement 1” and “Movement 2” in our EEG data follows a more complex, curved pattern? SVMs have a way to handle this known as the <strong>“kernel trick.”</strong></p>
<p>The basic idea is to imagine mapping our original 204-dimensional data into an even <em>higher</em>-dimensional space <span class="citation" data-cites="weise_kernel_2006">(<a href="#ref-weise_kernel_2006" role="doc-biblioref">Weiße et al., 2006</a>)</span>. In this new, much more complex space, the data might magically become linearly separable again. The “trick” is that we don’t actually have to compute the coordinates in this potentially massive new space. Instead, we use <strong>kernel functions</strong> <span class="math inline">\(K(x_i, x_j)\)</span>. These functions directly calculate what the dot product <em>would be</em> between the mapped vectors <span class="math inline">\(\phi(x_i)\)</span> and <span class="math inline">\(\phi(x_j)\)</span> in that high-dimensional space (<span class="math inline">\(K(x_i, x_j) = \phi(x_i)^T \phi(x_j)\)</span>). Since the math behind the SVM solution relies heavily on these dot products, we can just swap out the simple dot product <span class="math inline">\(x_i^T x_j\)</span> with our chosen kernel function <span class="math inline">\(K(x_i, x_j)\)</span>. This lets us effectively find a non-linear boundary in the original space without the excess computation of actually working in the super-high-dimensional mapped space.</p>
<p><strong>Common Kernels:</strong> In this project, we explored three common kernel options:</p>
<ul>
<li><strong>Linear Kernel:</strong> <span class="math inline">\(K(x_i, x_j) = x_i^T x_j\)</span>. This is our baseline, assuming a linear separation is sufficient.</li>
<li><strong>Polynomial Kernel:</strong> <span class="math inline">\(K(x_i, x_j) = (\gamma x_i^T x_j + r)^d\)</span>. This allows for curved boundaries defined by polynomials. It introduces new hyperparameters like the degree <span class="math inline">\(d\)</span>, a scaling factor <span class="math inline">\(\gamma\)</span>, and a coefficient <span class="math inline">\(r\)</span> that need tuning.</li>
<li><strong>Radial Basis Function (RBF) Kernel (or Gaussian Kernel):</strong> <span class="math inline">\(K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)\)</span>. This is a very popular kernel capable of creating highly complex decision boundaries. It uses a hyperparameter <span class="math inline">\(\gamma\)</span> that controls the “reach” or influence of each training point. Both <span class="math inline">\(C\)</span> and <span class="math inline">\(\gamma\)</span> need to be tuned <span class="citation" data-cites="han_parameter_2012">(<a href="#ref-han_parameter_2012" role="doc-biblioref">Han et al., 2012</a>)</span>.</li>
</ul>
</section>
</section>
<section id="model-evaluation-two-level-cross-validation" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="model-evaluation-two-level-cross-validation"><span class="header-section-number">2.3</span> Model Evaluation: Two-Level Cross-Validation</h2>
<p>Now we have our SVM model variants (linear, RBF, polynomial). But how do we know which one is actually best for our problem? And how do we choose the optimal values for their hyperparameters (like <span class="math inline">\(C\)</span> for all, <span class="math inline">\(\gamma\)</span> for RBF/Poly, <span class="math inline">\(d\)</span> for Poly)? We need a reliable way to estimate how well each classifier configuration will perform on new, unseen data.</p>
<p>As discussed before, just tuning hyperparameters and evaluating on the same cross-validation splits can lead to inflated performance estimates. To avoid this and get a truly unbiased evaluation while also performing hyperparameter tuning, we rely on <strong>two-level cross-validation</strong> <span class="citation" data-cites="noauthor_cross-validation_nodate">(<a href="#ref-noauthor_cross-validation_nodate" role="doc-biblioref"><em>Cross-Validation of Component Models</em>, n.d.</a>)</span>.</p>
<p><strong>The Process:</strong></p>
<ol type="1">
<li><strong>Outer Loop (Performance Estimation):</strong> Divides the data into <span class="math inline">\(k_{outer}\)</span> folds (here, 6). Each fold serves once as the final, held-out test set.</li>
<li><strong>Inner Loop (Hyperparameter Tuning):</strong> Operates <em>only</em> on the <span class="math inline">\(k_{outer}-1\)</span> folds designated as the outer training set for that round. It performs its <em>own</em> cross-validation (using <span class="math inline">\(k_{inner}=5\)</span> folds) to find the best hyperparameters (e.g., best combination of <span class="math inline">\(C\)</span> and <span class="math inline">\(\gamma\)</span> for RBF, or <span class="math inline">\(C\)</span>, <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(d\)</span>, <span class="math inline">\(r\)</span> for Poly) <em>without</em> seeing the outer test fold.</li>
<li><strong>Training and Testing:</strong> Once the inner loop selects the best hyperparameters for that outer fold, a new SVM model is trained with those parameters on the entire outer training set. This model is then evaluated once on the held-out outer test set.</li>
</ol>
<p>This entire process is repeated for all <span class="math inline">\(k_{outer}\)</span> outer folds, and the final performance metrics (accuracy, AUC) are averaged across the results from these outer test sets. This rigorous approach ensures that our hyperparameter tuning doesn’t bias our final performance evaluation, giving us a fair comparison between the different kernel types and a reliable estimate of their true generalization ability. We applied this entire two-level CV procedure separately for the linear, RBF, and polynomial SVMs.</p>
</section>
<section id="formulating-the-classification-problem" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="formulating-the-classification-problem"><span class="header-section-number">2.4</span> Formulating the Classification Problem</h2>
<p>Explicitly defining the machine learning problem we’re solving here:</p>
<ul>
<li><p><strong>Input Features:</strong> For each trial (either an overt or imagined movement), the input to our classifier is a single vector <span class="math inline">\(x \in \mathbb{R}^{204}\)</span>. This vector represents the measurements from the 102 electrodes, with each electrode contributing two gradient values (x and y direction), resulting in 204 features per trial. These features are assumed to capture the spatial and electrical characteristics of the brain activity during that trial.</p></li>
<li><p><strong>Target Classes:</strong> The output we want to predict is a binary class label, <span class="math inline">\(y \in \{\text{"Movement 1"}, \text{"Movement 2"}\}\)</span>. As mentioned, the dataset labels these generically, corresponding to the intention to move either the left or right hand (mapping unknown). Our goal is to distinguish between these two distinct intentions based on the EEG features.</p></li>
<li><p><strong>Objective:</strong> The objective is to learn a classification function <span class="math inline">\(f: \mathbb{R}^{204} \rightarrow \{\text{"Movement 1"}, \text{"Movement 2"}\}\)</span> using a Support Vector Machine. We aim to find the function <span class="math inline">\(f\)</span> (defined by the SVM’s parameters, including the kernel type and its associated hyperparameters like <span class="math inline">\(C\)</span>, <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(d\)</span>) that minimizes classification errors on unseen data. The two-level cross-validation procedure is our method for estimating this generalization performance and for selecting the optimal hyperparameters for each kernel type based on the training data.</p></li>
</ul>
</section>
</section>
<section id="results" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Results</h1>
<p>So, let’s dive into the results. We applied our SVM classifiers (Linear, RBF, and Polynomial kernels) using the two-level cross-validation approach. We’ll look at performance for the Same-Train scenarios (Overt-train/Overt-test and Imagined-train/Imagined-test) and the Cross-Train scenarios (Overt-train/Imagined-test and Imagined-train/Overt-test). We’ll compare the kernels based on ROC curves and accuracy scores.</p>
<section id="linear-kernel-classification-performance" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="linear-kernel-classification-performance"><span class="header-section-number">3.1</span> Linear Kernel Classification Performance</h2>
<p>We’ll start with the baseline linear SVM.</p>
<section id="same-train-scenarios-linear-svm" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="same-train-scenarios-linear-svm"><span class="header-section-number">3.1.1</span> Same-Train Scenarios (Linear SVM)</h3>
<ul>
<li><p><strong>ROC Curves:</strong> Figures 1 and 2 show the ROC curves for the linear SVM when trained and tested on Overt and Imagined data, respectively.</p>
<div id="fig-roc-overt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-overt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="output/plots/overt_lin_roc_individual.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-overt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Individual Fold and Overall ROC Curves for Same-Train Overt (Linear SVM)
</figcaption>
</figure>
</div>
<div id="fig-roc-imagined" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-imagined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="output/plots/img_lin_roc_individual.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-imagined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Individual Fold and Overall ROC Curves for Same-Train Imagined (Linear SVM)
</figcaption>
</figure>
</div>
<p><em>Interpretation:</em> As previously discussed, the linear SVM performed better on Overt data (Avg AUC in Fig. 1) than Imagined data (Avg AUC in Fig. 2), likely due to stronger signals. Performance was reasonably consistent across folds for Overt, but more variable for Imagined.</p></li>
<li><p><strong>Accuracy:</strong> Tables 1 and 2 show the fold-by-fold accuracy and optimal <span class="math inline">\(C\)</span> values.</p>
<p><strong>Table 1:</strong> Per-Fold Accuracy Results for Same-Train Overt (Linear SVM)</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Fold</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Optimal C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1/6</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">2/6</td>
<td style="text-align: center;">0.9750</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3/6</td>
<td style="text-align: center;">0.9750</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">4/6</td>
<td style="text-align: center;">0.9000</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5/6</td>
<td style="text-align: center;">0.9750</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">6/6</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Avg</strong></td>
<td style="text-align: center;"><strong>0.9542</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Std Dev</strong></td>
<td style="text-align: center;"><strong>0.0264</strong></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><strong>Table 2:</strong> Per-Fold Accuracy Results for Same-Train Imagined (Linear SVM)</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Fold</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Optimal C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1/6</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">2/6</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3/6</td>
<td style="text-align: center;">0.8000</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="even">
<td style="text-align: center;">4/6</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5/6</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">6/6</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Avg</strong></td>
<td style="text-align: center;"><strong>0.8875</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Std Dev</strong></td>
<td style="text-align: center;"><strong>0.0580</strong></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><em>Interpretation:</em> Average accuracy was ~95.4% for Overt and ~88.8% for Imagined, confirming the ROC trend. Variability across folds was higher for Imagined data.</p></li>
</ul>
</section>
<section id="cross-train-scenarios-linear-svm" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="cross-train-scenarios-linear-svm"><span class="header-section-number">3.1.2</span> Cross-Train Scenarios (Linear SVM)</h3>
<ul>
<li><p><strong>ROC Curves:</strong> Figures 3 and 4 show the ROC curves for the cross-training scenarios using the linear SVM.</p>
<div id="fig-roc-xt-img-ov" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-xt-img-ov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="output/plots/xt_img_ov_lin_roc.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-xt-img-ov-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: ROC Curve for Cross-Training: Train Imagined -&gt; Test Overt (Linear SVM)
</figcaption>
</figure>
</div>
<div id="fig-roc-xt-ov-img" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-xt-ov-img-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="output/plots/xt_ov_img_lin_roc.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-xt-ov-img-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: ROC Curve for Cross-Training: Train Overt -&gt; Test Imagined (Linear SVM)
</figcaption>
</figure>
</div>
<p><em>Comparison:</em> As noted before, training on Imagined and testing on Overt (Fig 3, ~91.3% Acc, AUC 0.97) performed slightly better than training on Overt and testing on Imagined (Fig 4, ~89.2% Acc, AUC 0.96). Both fell between the same-train performances, suggesting some shared patterns but also distinct differences between the conditions.</p></li>
</ul>
</section>
<section id="feature-importance-linear-svm-weights" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="feature-importance-linear-svm-weights"><span class="header-section-number">3.1.3</span> Feature Importance (Linear SVM Weights)</h3>
<ul>
<li><p><strong>Spatial and Stem Plots:</strong> Figures 5-8 visualize the weights for the linear SVM.</p>
<p><img src="output/plots/overt_lin_weights_brain.png" id="fig-weights-brain-overt" class="img-fluid" alt="SVM Weight Magnitude on Brain Surface (Overt Data, Fold 1, Linear SVM)"> <img src="output/plots/overt_lin_weights_stem.png" id="fig-weights-stem-overt" class="img-fluid" alt="Signed SVM Weights per Channel (Overt Data, Fold 1, Linear SVM), Top 6 Highlighted"> <img src="output/plots/img_lin_weights_brain.png" id="fig-weights-brain-imagined" class="img-fluid" alt="SVM Weight Magnitude on Brain Surface (Imagined Data, Fold 1, Linear SVM)"> <img src="output/plots/img_lin_weights_stem.png" id="fig-weights-stem-imagined" class="img-fluid" alt="Signed SVM Weights per Channel (Imagined Data, Fold 1, Linear SVM), Top 6 Highlighted"></p>
<p><em>Interpretation:</em> The weights highlighted channels primarily over sensorimotor areas, consistent with neuroscience. The primary motor cortex lies in the middle of the brain, which is the most heavily weighted part. There was significant overlap in the most important channels (e.g., 128, 136, 140) between Overt and Imagined conditions, supporting the idea of shared neural substrates.</p></li>
</ul>
</section>
</section>
<section id="rbf-kernel-classification-performance" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="rbf-kernel-classification-performance"><span class="header-section-number">3.2</span> RBF Kernel Classification Performance</h2>
<p>Now let’s consider the performance when using the Radial Basis Function (RBF) kernel, which allows for more complex, non-linear decision boundaries. The hyperparameters <span class="math inline">\(C\)</span> and <span class="math inline">\(\gamma\)</span> were tuned simultaneously in the inner loop of the two-level CV.</p>
<section id="same-train-scenarios-rbf-svm" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="same-train-scenarios-rbf-svm"><span class="header-section-number">3.2.1</span> Same-Train Scenarios (RBF SVM)</h3>
<ul>
<li><p><strong>ROC Curves:</strong> Let’s assume Figures 9 and 10 show the ROC curves for the RBF SVM on Overt and Imagined data, respectively.</p>
<p><img src="output/plots/overt_rbf_roc_individual.png" id="fig-weights-stem-imagined" class="img-fluid" alt="ROC for same-train rbf overt"> <img src="output/plots/img_rbf_roc_individual.png" id="fig-weights-stem-imagined" class="img-fluid" alt="ROC for same-train rbf imagined"></p>
<p><em>Interpretation:</em> The average AUC for the Overt data (Figure 9) is approximately 0.99, suggesting excellent performance, slightly higher than the linear kernel’s average. For the Imagined data (Figure 10), the average AUC is around 0.94, which is also higher than the linear kernel’s average (~0.92 based on visual inspection of Fig 2), indicating the RBF kernel might be capturing useful non-linearities, especially in the more challenging Imagined condition. Variability between folds still seems present, particularly for Imagined data.</p></li>
<li><p><strong>Accuracy:</strong> Similarly, assume Tables 3 and 4 present the accuracy results and the optimal <span class="math inline">\((C, \gamma)\)</span> pairs chosen for each fold.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Fold</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Optimal C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1/6</td>
<td style="text-align: center;">0.9750</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">2/6</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3/6</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">4/6</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5/6</td>
<td style="text-align: center;">0.9750</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">6/6</td>
<td style="text-align: center;">0.8750</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Avg</strong></td>
<td style="text-align: center;"><strong>0.9333</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Std Dev</strong></td>
<td style="text-align: center;"><strong>0.0354</strong></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Fold</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Optimal C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1/6</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">2/6</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3/6</td>
<td style="text-align: center;">0.8000</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">4/6</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5/6</td>
<td style="text-align: center;">0.8250</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">6/6</td>
<td style="text-align: center;">0.8750</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Avg</strong></td>
<td style="text-align: center;"><strong>0.8542</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Std Dev</strong></td>
<td style="text-align: center;"><strong>0.0417</strong></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><em>Interpretation:</em> Comparing the average accuracies, the RBF kernel achieved ~93.3% on Overt data (Table 3) and ~85.4% on Imagined data (Table 4). Interestingly, these average accuracies are slightly lower than those achieved by the linear kernel (95.4% Overt, 88.8% Imagined). This contrasts with the AUC results and suggests that while the RBF kernel might offer better separation overall (higher AUC), the optimal decision threshold for maximizing accuracy might be different, or the added complexity might slightly hurt performance at the standard threshold, perhaps due to overfitting on some folds despite cross-validation. The optimal <span class="math inline">\(C\)</span> values tend to be higher (1 or 10) than often seen for the linear kernel, potentially indicating the need for a tighter fit when using the flexible RBF kernel.</p></li>
</ul>
</section>
<section id="feature-importance-rbf-svm" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="feature-importance-rbf-svm"><span class="header-section-number">3.2.2</span> Feature Importance (RBF SVM)</h3>
<p><img src="output/plots/overt_rbf_weights_brain.png" id="fig-weights-rbf-overt" class="img-fluid" alt="Weight visualization for same-train rbf overt"> <img src="output/plots/img_rbf_weights_brain.png" id="fig-weights-rbf-overt" class="img-fluid" alt="Weight visualization for same-train rbf overt"></p>
<ul>
<li><strong>Weight Interpretation:</strong> Visualizing feature importance for non-linear kernels like RBF isn’t as straightforward as plotting the weights (<span class="math inline">\(w\)</span>) in the linear case. With a linear SVM, the weights directly correspond to the importance of a feature, but the same association doesn’t exist with different kernels. Therefore, we look at the performance metrics and also use approximations to map weights back to support vectors. However, these heatmaps still show a general distribution of “weights” in the same locations as the linear SVM.</li>
</ul>
</section>
</section>
<section id="polynomial-kernel-classification-performance" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="polynomial-kernel-classification-performance"><span class="header-section-number">3.3</span> Polynomial Kernel Classification Performance</h2>
<p>Finally, let’s consider the Polynomial kernel. This kernel also allows for non-linear boundaries, with complexity controlled by the polynomial degree <span class="math inline">\(d\)</span> (along with hyperparameters <span class="math inline">\(C\)</span>, <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(r\)</span>, all tuned via the inner CV loop).</p>
<section id="same-train-scenarios-polynomial-svm" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="same-train-scenarios-polynomial-svm"><span class="header-section-number">3.3.1</span> Same-Train Scenarios (Polynomial SVM)</h3>
<ul>
<li><p><strong>ROC Curves:</strong> Assume Figures 13 and 14 show the ROC curves for the Polynomial SVM on Overt and Imagined data.</p>
<p><img src="output/plots/overt_poly_roc_individual.png" id="fig-weights-stem-imagined" class="img-fluid" alt="ROC for same-train poly overt"> <img src="output/plots/img_poly_roc_individual.png" id="fig-weights-stem-imagined" class="img-fluid" alt="ROC for same-train poly overt"></p>
<p><em>Interpretation:</em> The average AUC for the Overt data (Figure 13) is approximately 0.98, slightly below the RBF kernel but comparable to the linear kernel. However, for the Imagined data (Figure 14), the average AUC drops significantly to around 0.89, which is much worse than both the linear (~0.92) and RBF (~0.94) kernels. This suggests the polynomial kernel might be struggling or potentially overfitting on the more difficult Imagined dataset.</p></li>
<li><p><strong>Accuracy:</strong> Assume Tables 5 and 6 show the accuracy results and the optimal hyperparameter sets (<span class="math inline">\(C, \gamma, d, r\)</span>) for each fold.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Fold</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Optimal C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1/6</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">2/6</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3/6</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">100</td>
</tr>
<tr class="even">
<td style="text-align: center;">4/6</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5/6</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">6/6</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Avg</strong></td>
<td style="text-align: center;"><strong>0.9508</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Std Dev</strong></td>
<td style="text-align: center;"><strong>0.0252</strong></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Fold</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Optimal C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1/6</td>
<td style="text-align: center;">0.8250</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">2/6</td>
<td style="text-align: center;">0.8750</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3/6</td>
<td style="text-align: center;">0.7750</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">4/6</td>
<td style="text-align: center;">0.7750</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5/6</td>
<td style="text-align: center;">0.8250</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">6/6</td>
<td style="text-align: center;">0.8250</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Avg</strong></td>
<td style="text-align: center;"><strong>0.8167</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Std Dev</strong></td>
<td style="text-align: center;"><strong>0.0366</strong></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><em>Interpretation:</em> The average accuracy for the Polynomial kernel is ~95.1% on Overt data (Table 5), which is very close to the linear kernel’s performance. However, on the Imagined data (Table 6), the average accuracy drops to ~81.7%, noticeably lower than both the linear (~88.8%) and RBF (~85.4%) kernels. This confirms the trend seen in the AUCs – the polynomial kernel seems less effective for the imagined movement data in this experiment. Optimal <span class="math inline">\(C\)</span> values varied widely, including a large value of 100 in one Overt fold.</p>
<p><img src="output/plots/overt_poly_weights_brain.png" id="fig-weights-poly-overt" class="img-fluid" alt="Weight visualization for same-train poly overt"> <img src="output/plots/img_poly_weights_brain.png" id="fig-weights-poly-img" class="img-fluid" alt="Weight visualization for same-train poly overt"></p>
<p><em>Interpretation:</em> Similar as all the other heatmaps, we see the weights concentrated in the motor cortices.</p></li>
</ul>
</section>
</section>
</section>
<section id="conclusions" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Conclusions</h1>
<p>So, wrapping things up, what did we find? This project aimed to demonstrate and compare how different Support Vector Machine configurations—using linear, RBF, and polynomial kernels—could interpret complex EEG signals to decode movement intentions. We focused on distinguishing between two movement types using signals from both overt and imagined actions.</p>
<p><strong>Interpretation of Results &amp; Scenario Comparison:</strong> Our analysis confirmed that SVMs can achieve high classification accuracy, with the linear kernel already providing strong baseline performance, particularly on the Overt data (~95.4% accuracy, ~0.98 AUC). Performance on Imagined data was consistently lower (~88.8% accuracy, ~0.92 AUC for linear), highlighting the challenge of decoding weaker signals. The exploration of non-linear kernels yielded mixed results. The RBF kernel showed slightly better AUC values (Overt ~0.99, Imagined ~0.94) but slightly lower average accuracies (Overt ~93.3%, Imagined ~85.4%) compared to the linear kernel, suggesting potential subtle overfitting or threshold effects despite capturing useful non-linearities. The Polynomial kernel performed comparably to linear on Overt data (~95.1% accuracy, ~0.98 AUC) but significantly worse on Imagined data (~81.7% accuracy, ~0.89 AUC), indicating its specific form of non-linearity was not beneficial and possibly detrimental for the harder task. Overall, the linear kernel provided the most consistent and robust performance across both accuracy and AUC, especially considering the Imagined data performance.</p>
<p>Cross-domain performance revealed interesting patterns for the linear kernel, with training on Imagined and testing on Overt (~91.3% Acc) working slightly better than the reverse (~89.2% Acc). The results consistently showed cross-train accuracies falling between the same-train Overt and Imagined performance, indicating shared but distinct neural patterns.</p>
<p><strong>Factors, Limits, and Improvements:</strong> The signal strength difference between Overt and Imagined EEG remains a primary factor. Trial-to-trial variability and the information content of specific channels also play key roles. Limitations include the single-subject nature of the data and the unknown left/right mapping. While we explored different kernels, finding the linear kernel often performed best, further improvements could involve exploring other machine learning algorithms (like deep learning), testing on more subjects, and focusing on real-time implementation. The two-level cross-validation was crucial for obtaining reliable performance estimates and making fair comparisons between the SVM kernels.</p>
<p><strong>Domain Knowledge &amp; Model Insights:</strong> The weight analysis for the linear SVM provided neuroscientific validation, highlighting activity in expected motor areas, particularly the primary motor cortex. Even the approximate weight visualizations for non-linear kernels pointed towards similar brain regions, suggesting the core spatial information is robust. However, the performance differences indicate that the way these kernels combined the information mattered, with the non-linear approaches not consistently improving upon the linear baseline here, and sometimes hurting performance (especially Polynomial on Imagined data).</p>
<p>Considering cross-data training, the ideal approach probably involves leveraging the strengths of different data types and possibly models. Transfer learning or domain adaptation techniques remain promising avenues for building BCIs that work well in challenging real-world scenarios like decoding imagined movements, potentially starting with robust linear models.</p>
<p><strong>Future Directions:</strong> This comparative analysis lays groundwork for future studies. Directly comparing SVMs (particularly linear, given its strong performance here) with other classifiers like specialized deep learning models designed for EEG could be insightful. Addressing the need for real-time processing and reducing reliance on extensive calibration are vital practical steps. Importantly, validating these decoding approaches across diverse participants, including the target population with motor impairments, is necessary to gauge clinical potential.</p>
<p>In conclusion, this project explored the application of various SVM kernels to the challenging task of EEG-based movement intention decoding. By systematically evaluating linear and non-linear approaches using robust two-level cross-validation, we gained insights into their respective strengths and weaknesses for this BCI application, finding that the linear kernel offered the most reliable performance overall. While challenges persist, especially for imagined movements and cross-condition generalization, this work contributes to the ongoing effort to develop powerful assistive technologies like BCIs and understand their optimal implementation in the medical field.</p>
</section>
<section id="collaborations" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Collaborations</h1>
<p>There were four main people I collaborated with:</p>
<p>Wanghley Soares Martin: I learned about this entire report format from him, as well as some information on using github pages and quarto to produce academic writing. He was very helpful in my exploration phase.</p>
<p>Peter Banyas: I discussed with peter how I could get approximate weights from non-linear kernels, and he recommended some approaches that I ended up using (getting the support vector coefficients)</p>
<p>Will Neuner: I talked with Will on LDOC about his regularization parameters, and we looked more closely at the implementation of two-level CV.</p>
<p>Evan Wen: Discussed results of implementing non-linear kernels and their implications.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-armour_prevalence_2016" class="csl-entry" role="listitem">
Armour, B. S., Courtney-Long, E. A., Fox, M. H., Fredine, H., &amp; Cahill, A. (2016). Prevalence and <span>Causes</span> of <span>Paralysis</span>—<span>United</span> <span>States</span>, 2013. <em>American Journal of Public Health</em>, <em>106</em>(10), 1855–1857. <a href="https://doi.org/10.2105/AJPH.2016.303270">https://doi.org/10.2105/AJPH.2016.303270</a>
</div>
<div id="ref-noauthor_cross-validation_nodate" class="csl-entry" role="listitem">
<em>Cross-validation of component models: <span>A</span> critical look at current methods <span></span> <span>Analytical</span> and <span>Bioanalytical</span> <span>Chemistry</span></em>. (n.d.). Retrieved April 25, 2025, from <a href="https://link.springer.com/article/10.1007/s00216-007-1790-1">https://link.springer.com/article/10.1007/s00216-007-1790-1</a>
</div>
<div id="ref-guo_face_2000" class="csl-entry" role="listitem">
Guo, G., Li, S. Z., &amp; Chan, K. (2000). Face recognition by support vector machines. <em>Proceedings <span>Fourth</span> <span>IEEE</span> <span>International</span> <span>Conference</span> on <span>Automatic</span> <span>Face</span> and <span>Gesture</span> <span>Recognition</span> (<span>Cat</span>. <span>No</span>. <span>PR00580</span>)</em>, 196–201. <a href="https://doi.org/10.1109/AFGR.2000.840634">https://doi.org/10.1109/AFGR.2000.840634</a>
</div>
<div id="ref-han_parameter_2012" class="csl-entry" role="listitem">
Han, S., Qubo, C., &amp; Meng, H. (2012). Parameter selection in <span>SVM</span> with <span>RBF</span> kernel function. <em>World <span>Automation</span> <span>Congress</span> 2012</em>, 1–4. <a href="https://ieeexplore.ieee.org/document/6321759/">https://ieeexplore.ieee.org/document/6321759/</a>
</div>
<div id="ref-hearst_support_1998" class="csl-entry" role="listitem">
Hearst, M. A., Dumais, S. T., Osuna, E., Platt, J., &amp; Scholkopf, B. (1998). Support vector machines. <em>IEEE Intelligent Systems and Their Applications</em>, <em>13</em>(4), 18–28. <a href="https://doi.org/10.1109/5254.708428">https://doi.org/10.1109/5254.708428</a>
</div>
<div id="ref-mammone_support_2009" class="csl-entry" role="listitem">
Mammone, A., Turchi, M., &amp; Cristianini, N. (2009). Support vector machines. <em>WIREs Computational Statistics</em>, <em>1</em>(3), 283–289. <a href="https://doi.org/10.1002/wics.49">https://doi.org/10.1002/wics.49</a>
</div>
<div id="ref-nicolas-alonso_brain_2012" class="csl-entry" role="listitem">
Nicolas-Alonso, L. F., &amp; Gomez-Gil, J. (2012). Brain <span>Computer</span> <span>Interfaces</span>, a <span>Review</span>. <em>Sensors</em>, <em>12</em>(2), 1211–1279. <a href="https://doi.org/10.3390/s120201211">https://doi.org/10.3390/s120201211</a>
</div>
<div id="ref-noauthor_svc_nodate" class="csl-entry" role="listitem">
<span>SVC</span>. (n.d.). In <em>scikit-learn</em>. Retrieved April 20, 2025, from <a href="https://scikit-learn/stable/modules/generated/sklearn.svm.SVC.html">https://scikit-learn/stable/modules/generated/sklearn.svm.SVC.html</a>
</div>
<div id="ref-weise_kernel_2006" class="csl-entry" role="listitem">
Weiße, A., Wellein, G., Alvermann, A., &amp; Fehske, H. (2006). The kernel polynomial method. <em>Reviews of Modern Physics</em>, <em>78</em>(1), 275–306. <a href="https://doi.org/10.1103/RevModPhys.78.275">https://doi.org/10.1103/RevModPhys.78.275</a>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>