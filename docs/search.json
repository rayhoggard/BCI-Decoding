[
  {
    "objectID": "notebooks/Untitled.html",
    "href": "notebooks/Untitled.html",
    "title": "BCI Movement Decoding",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\n# Suppress ConvergenceWarning from SVC\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\ndef load_and_prepare_data(file1, file2, label1=0, label2=1):\n    \"\"\"Loads two CSV files, assigns labels, and combines them.\"\"\"\n    df1 = pd.read_csv(file1, header=None)\n    df2 = pd.read_csv(file2, header=None)\n\n    # Assign labels\n    y1 = np.full(df1.shape[0], label1)\n    y2 = np.full(df2.shape[0], label2)\n\n    # Combine features and labels\n    X = pd.concat([df1, df2], ignore_index=True).values\n    y = np.concatenate([y1, y2])\n\n    return X, y\n\ndef perform_two_level_cv(X, y, outer_k=6, inner_k=5, C_range=None):\n    \"\"\"Performs two-level cross-validation for SVM hyperparameter tuning.\"\"\"\n    if C_range is None:\n        # Define the search space for the regularization parameter C\n        C_range = np.logspace(-3, 3, 7) # [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\n    outer_cv = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=42)\n    inner_cv = StratifiedKFold(n_splits=inner_k, shuffle=True, random_state=42)\n\n    outer_fold_accuracies = []\n    optimal_Cs = []\n\n    print(f\"\\nStarting {outer_k}-fold outer cross-validation...\")\n\n    # Outer loop for model evaluation\n    for i, (train_outer_idx, test_outer_idx) in enumerate(outer_cv.split(X, y)):\n        print(f\"  Outer Fold {i+1}/{outer_k}\")\n        X_train_outer, X_test_outer = X[train_outer_idx], X[test_outer_idx]\n        y_train_outer, y_test_outer = y[train_outer_idx], y[test_outer_idx]\n\n        # --- Data Scaling (Outer Fold) ---\n        scaler = StandardScaler()\n        X_train_outer_scaled = scaler.fit_transform(X_train_outer)\n        X_test_outer_scaled = scaler.transform(X_test_outer) # Use same scaler fit on training data\n\n        inner_loop_results = {} # {C: [fold1_acc, fold2_acc, ...]}\n\n        # Inner loop for hyperparameter tuning (finding best C)\n        print(f\"    Starting {inner_k}-fold inner cross-validation for hyperparameter tuning...\")\n        for C_val in C_range:\n            inner_fold_accuracies = []\n            for j, (train_inner_idx, val_inner_idx) in enumerate(inner_cv.split(X_train_outer_scaled, y_train_outer)):\n                # print(f\"      Inner Fold {j+1}/{inner_k} for C={C_val}\") # Uncomment for very detailed logging\n                X_train_inner, X_val_inner = X_train_outer_scaled[train_inner_idx], X_train_outer_scaled[val_inner_idx]\n                y_train_inner, y_val_inner = y_train_outer[train_inner_idx], y_train_outer[val_inner_idx]\n\n                # Train SVM with current C on inner training fold\n                svm_inner = SVC(kernel='linear', C=C_val, random_state=42, probability=False) # Probability=False can speed up\n                svm_inner.fit(X_train_inner, y_train_inner)\n\n                # Evaluate on inner validation fold\n                y_pred_inner = svm_inner.predict(X_val_inner)\n                accuracy = accuracy_score(y_val_inner, y_pred_inner)\n                inner_fold_accuracies.append(accuracy)\n\n            # Store average accuracy for this C value across inner folds\n            inner_loop_results[C_val] = np.mean(inner_fold_accuracies)\n            # print(f\"      Average Inner Accuracy for C={C_val:.3f}: {inner_loop_results[C_val]:.4f}\") # Uncomment for detail\n\n        # Find the best C based on inner loop average accuracy\n        best_C = max(inner_loop_results, key=inner_loop_results.get)\n        optimal_Cs.append(best_C)\n        print(f\"    Optimal C found for Outer Fold {i+1}: {best_C} (Avg Inner Acc: {inner_loop_results[best_C]:.4f})\")\n\n        # Train the final SVM for this outer fold using the best C on the *entire* outer training set\n        print(f\"    Training final SVM for Outer Fold {i+1} with C={best_C}...\")\n        final_svm = SVC(kernel='linear', C=best_C, random_state=42)\n        final_svm.fit(X_train_outer_scaled, y_train_outer)\n\n        # Evaluate the final SVM on the outer test set\n        y_pred_outer = final_svm.predict(X_test_outer_scaled)\n        outer_accuracy = accuracy_score(y_test_outer, y_pred_outer)\n        outer_fold_accuracies.append(outer_accuracy)\n        print(f\"  Outer Fold {i+1}/{outer_k} Accuracy: {outer_accuracy:.4f}\")\n\n    average_accuracy = np.mean(outer_fold_accuracies)\n    std_dev_accuracy = np.std(outer_fold_accuracies)\n\n    print(f\"\\nFinished cross-validation.\")\n    print(f\"Optimal C for each outer fold: {optimal_Cs}\")\n    print(f\"Accuracy for each outer fold: {[f'{acc:.4f}' for acc in outer_fold_accuracies]}\")\n    print(f\"Average Accuracy across all outer folds: {average_accuracy:.4f}\")\n    print(f\"Standard Deviation of Accuracy across outer folds: {std_dev_accuracy:.4f}\")\n\n    return average_accuracy, std_dev_accuracy, optimal_Cs, outer_fold_accuracies\n\n\n# --- Main Execution ---\n\n# Define file paths (assuming they are in the current directory or accessible)\nimg_file1 = '../../feaSubEImg_1.csv'\nimg_file2 = '../../feaSubEImg_2.csv'\novert_file1 = '../../feaSubEOvert_1.csv'\novert_file2 = '../../feaSubEOvert_2.csv'\n\n# Load data\nprint(\"Loading Imagined Movement Data...\")\nX_img, y_img = load_and_prepare_data(img_file1, img_file2)\nprint(f\"Imagined Data Shape: X={X_img.shape}, y={y_img.shape}\")\nprint(f\"Imagined Data Labels: {np.unique(y_img, return_counts=True)}\")\n\n\nprint(\"\\nLoading Overt Movement Data...\")\nX_overt, y_overt = load_and_prepare_data(overt_file1, overt_file2)\nprint(f\"Overt Data Shape: X={X_overt.shape}, y={y_overt.shape}\")\nprint(f\"Overt Data Labels: {np.unique(y_overt, return_counts=True)}\")\n\n\n# --- Run Two-Level CV for Imagined Data ---\nprint(\"\\n--- Starting Analysis for Imagined Movement Data ---\")\nimg_avg_acc, img_std_acc, img_opt_C, img_fold_accs = perform_two_level_cv(X_img, y_img)\n\n\n# --- Run Two-Level CV for Overt Data ---\nprint(\"\\n--- Starting Analysis for Overt Movement Data ---\")\novert_avg_acc, overt_std_acc, overt_opt_C, overt_fold_accs = perform_two_level_cv(X_overt, y_overt)\n\n\n# --- Final Summary ---\nprint(\"\\n\\n--- Results Summary ---\")\nprint(f\"Imagined Movement Data:\")\nprint(f\"  - Average Accuracy: {img_avg_acc:.4f}\")\nprint(f\"  - Std Dev Accuracy: {img_std_acc:.4f}\")\nprint(f\"  - Optimal C per fold: {img_opt_C}\")\nprint(f\"  - Fold Accuracies: {[f'{acc:.4f}' for acc in img_fold_accs]}\")\n\nprint(f\"\\nOvert Movement Data:\")\nprint(f\"  - Average Accuracy: {overt_avg_acc:.4f}\")\nprint(f\"  - Std Dev Accuracy: {overt_std_acc:.4f}\")\nprint(f\"  - Optimal C per fold: {overt_opt_C}\")\nprint(f\"  - Fold Accuracies: {[f'{acc:.4f}' for acc in overt_fold_accs]}\")\n\nLoading Imagined Movement Data...\nImagined Data Shape: X=(408, 120), y=(408,)\nImagined Data Labels: (array([0, 1]), array([204, 204]))\n\nLoading Overt Movement Data...\nOvert Data Shape: X=(408, 120), y=(408,)\nOvert Data Labels: (array([0, 1]), array([204, 204]))\n\n--- Starting Analysis for Imagined Movement Data ---\n\nStarting 6-fold outer cross-validation...\n  Outer Fold 1/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 1: 0.1 (Avg Inner Acc: 0.7735)\n    Training final SVM for Outer Fold 1 with C=0.1...\n  Outer Fold 1/6 Accuracy: 0.7059\n  Outer Fold 2/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 2: 0.1 (Avg Inner Acc: 0.7294)\n    Training final SVM for Outer Fold 2 with C=0.1...\n  Outer Fold 2/6 Accuracy: 0.7206\n  Outer Fold 3/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 3: 0.1 (Avg Inner Acc: 0.7382)\n    Training final SVM for Outer Fold 3 with C=0.1...\n  Outer Fold 3/6 Accuracy: 0.7206\n  Outer Fold 4/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 4: 0.01 (Avg Inner Acc: 0.7147)\n    Training final SVM for Outer Fold 4 with C=0.01...\n  Outer Fold 4/6 Accuracy: 0.7647\n  Outer Fold 5/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 5: 0.1 (Avg Inner Acc: 0.7441)\n    Training final SVM for Outer Fold 5 with C=0.1...\n  Outer Fold 5/6 Accuracy: 0.7794\n  Outer Fold 6/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 6: 0.1 (Avg Inner Acc: 0.7235)\n    Training final SVM for Outer Fold 6 with C=0.1...\n  Outer Fold 6/6 Accuracy: 0.7647\n\nFinished cross-validation.\nOptimal C for each outer fold: [0.1, 0.1, 0.1, 0.01, 0.1, 0.1]\nAccuracy for each outer fold: ['0.7059', '0.7206', '0.7206', '0.7647', '0.7794', '0.7647']\nAverage Accuracy across all outer folds: 0.7426\nStandard Deviation of Accuracy across outer folds: 0.0278\n\n--- Starting Analysis for Overt Movement Data ---\n\nStarting 6-fold outer cross-validation...\n  Outer Fold 1/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 1: 0.1 (Avg Inner Acc: 0.7412)\n    Training final SVM for Outer Fold 1 with C=0.1...\n  Outer Fold 1/6 Accuracy: 0.6912\n  Outer Fold 2/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 2: 0.01 (Avg Inner Acc: 0.7324)\n    Training final SVM for Outer Fold 2 with C=0.01...\n  Outer Fold 2/6 Accuracy: 0.6765\n  Outer Fold 3/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 3: 0.1 (Avg Inner Acc: 0.7471)\n    Training final SVM for Outer Fold 3 with C=0.1...\n  Outer Fold 3/6 Accuracy: 0.7206\n  Outer Fold 4/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 4: 0.01 (Avg Inner Acc: 0.6971)\n    Training final SVM for Outer Fold 4 with C=0.01...\n  Outer Fold 4/6 Accuracy: 0.8088\n  Outer Fold 5/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 5: 0.1 (Avg Inner Acc: 0.7265)\n    Training final SVM for Outer Fold 5 with C=0.1...\n  Outer Fold 5/6 Accuracy: 0.7941\n  Outer Fold 6/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 6: 1.0 (Avg Inner Acc: 0.6853)\n    Training final SVM for Outer Fold 6 with C=1.0...\n  Outer Fold 6/6 Accuracy: 0.6765\n\nFinished cross-validation.\nOptimal C for each outer fold: [0.1, 0.01, 0.1, 0.01, 0.1, 1.0]\nAccuracy for each outer fold: ['0.6912', '0.6765', '0.7206', '0.8088', '0.7941', '0.6765']\nAverage Accuracy across all outer folds: 0.7279\nStandard Deviation of Accuracy across outer folds: 0.0542\n\n\n--- Results Summary ---\nImagined Movement Data:\n  - Average Accuracy: 0.7426\n  - Std Dev Accuracy: 0.0278\n  - Optimal C per fold: [0.1, 0.1, 0.1, 0.01, 0.1, 0.1]\n  - Fold Accuracies: ['0.7059', '0.7206', '0.7206', '0.7647', '0.7794', '0.7647']\n\nOvert Movement Data:\n  - Average Accuracy: 0.7279\n  - Std Dev Accuracy: 0.0542\n  - Optimal C per fold: [0.1, 0.01, 0.1, 0.01, 0.1, 1.0]\n  - Fold Accuracies: ['0.6912', '0.6765', '0.7206', '0.8088', '0.7941', '0.6765']\n\n\n\n# bci_movement_decoding.py\n\"\"\"\nMini‑Project #2 — Brain‑Computer Interface Movement Decoding\n===========================================================\n\nThis script reproduces **all** data‑generation tasks described in the\nproject hand‑out and report guidance.  Running it once will\n\n* load the provided CSV files\n* build the Imagined and Overt data sets (204 features × 240 trials each)\n* perform two‑level cross‑validation (6 outer folds ✕ 5 inner folds) for a\n  **linear SVM** baseline on each data set, choosing α ∈ {0.01, 1, 100, 10000}\n* draw the required visualisations (spatial weight maps, stem plots, per‑fold\n  and aggregate ROC curves)\n* list the six dominant channels for fold #1 of each data set\n* evaluate the two cross‑training scenarios (train overt → test imagined and\n  vice‑versa) with hyper‑parameter selection on the training domain only\n* save every figure under `outputs/` and cache raw numpy arrays for further\n  analysis if desired.\n\nThe code is completely self‑contained **except** for the standard scientific\nPython stack (NumPy, SciPy, scikit‑learn, Matplotlib, Pandas) and requires\nno manual tweaking once the data files are present in the working directory.\n\"\"\"\nfrom __future__ import annotations\nimport os\nimport itertools\nfrom pathlib import Path\nfrom typing import Tuple, List, Dict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import griddata\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\n# -----------------------------------------------------------------------------\n# Config\n# -----------------------------------------------------------------------------\nDATA_DIR = Path('.')          # directory containing the CSVs\nOUTPUT_DIR = Path('outputs')  # where all artefacts are written\nOUTPUT_DIR.mkdir(exist_ok=True)\n\nALPHAS = np.array([1e-2, 1, 1e2, 1e4])  # regularisation grid (α)\nINNER_FOLDS = 5\nOUTER_FOLDS = 6\nRNG = 42\n\n# -----------------------------------------------------------------------------\n# Data loading helpers\n# -----------------------------------------------------------------------------\n\ndef _load_feature_csv(fname: str) -&gt; np.ndarray:\n    \"\"\"Return (trials, features) float64 array from a 204×120 CSV.\"\"\"\n    m = pd.read_csv(DATA_DIR / fname, header=None).values  # shape 204×120\n    return m.T  # 120 trials × 204 features\n\ndef load_datasets() -&gt; Dict[str, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Return dict: {'imagined': (X, y), 'overt': (X, y)}.\"\"\"\n    img1 = _load_feature_csv('../../feaSubEImg_1.csv')\n    img2 = _load_feature_csv('../../feaSubEImg_2.csv')\n    ovt1 = _load_feature_csv('../../feaSubEOvert_1.csv')\n    ovt2 = _load_feature_csv('../../feaSubEOvert_2.csv')\n\n    def _make_ds(cls1, cls2):\n        X = np.vstack([cls1, cls2])         # 240 × 204\n        y = np.hstack([np.zeros(cls1.shape[0]), np.ones(cls2.shape[0])])\n        return X, y\n\n    return {\n        'imagined': _make_ds(img1, img2),\n        'overt':    _make_ds(ovt1, ovt2),\n    }\n\n# -----------------------------------------------------------------------------\n# Visualisation helpers\n# -----------------------------------------------------------------------------\n\ndef plot_weights_brain(w: np.ndarray, xy: np.ndarray, title: str, fname: str):\n    \"\"\"Interpolate |w| onto a regular grid and produce a pcolor heat‑map.\"\"\"\n    # Each electrode has two consecutive channels (Ex, Ey) at same coords.\n    coords = np.repeat(xy, 2, axis=0)  # (204, 2)\n    x, y = coords[:, 0], coords[:, 1]\n    mag = np.abs(w)\n    # regular grid covering the point cloud bounds\n    grid_x, grid_y = np.mgrid[x.min():x.max():200j, y.min():y.max():200j]\n    grid_z = griddata(coords, mag, (grid_x, grid_y), method='cubic', fill_value=np.nan)\n\n    plt.figure(figsize=(5, 5))\n    pc = plt.pcolor(grid_x, grid_y, grid_z, shading='auto')\n    plt.scatter(x, y, c=mag, s=20, edgecolors='k', cmap='viridis')\n    plt.axis('equal'); plt.axis('off'); plt.title(title)\n    plt.colorbar(pc, label='|weight|')\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n\ndef stem_weights(w: np.ndarray, title: str, fname: str, top_k: int = 6):\n    plt.figure(figsize=(8, 4))\n    markerline, stemlines, baseline = plt.stem(range(len(w)), w)\n    plt.setp(markerline, markersize=3)\n    # highlight top‑k channels\n    top_idx = np.argsort(np.abs(w))[-top_k:]\n    plt.scatter(top_idx, w[top_idx], s=60, facecolors='r', edgecolors='k', zorder=3, label='top‑k')\n    plt.axhline(0, color='k', linewidth=0.8)\n    plt.title(title); plt.xlabel('Channel index'); plt.ylabel('Signed weight')\n    plt.legend(); plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n    # also dump a text list for convenience\n    with open(OUTPUT_DIR / (fname.replace('.png', '_top_channels.txt')), 'w') as fp:\n        fp.write('\\n'.join(f'{i}: {w[i]:+.4f}' for i in top_idx[::-1]))\n\n\ndef plot_roc(fprs: List[np.ndarray], tprs: List[np.ndarray], aucs: List[float],\n             mean_fpr: np.ndarray, mean_tpr: np.ndarray, mean_auc: float,\n             title: str, fname: str):\n    plt.figure(figsize=(6, 6))\n    for i, (fpr, tpr, a) in enumerate(zip(fprs, tprs, aucs), start=1):\n        plt.plot(fpr, tpr, lw=1, alpha=0.7, label=f'fold {i} (AUC {a:.2f})')\n    plt.plot(mean_fpr, mean_tpr, 'k', lw=2.5, label=f'aggregate (AUC {mean_auc:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--', lw=0.8)\n    plt.xlabel('False positive rate'); plt.ylabel('True positive rate')\n    plt.title(title); plt.legend(); plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n# -----------------------------------------------------------------------------\n# Core model / CV routine\n# -----------------------------------------------------------------------------\n\ndef _select_c(X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Return best C chosen by INNER_FOLDS CV (argmax mean accuracy).\"\"\"\n    skf_inner = StratifiedKFold(n_splits=INNER_FOLDS, shuffle=True, random_state=RNG)\n    best_c, best_acc = None, -np.inf\n    for alpha in ALPHAS:\n        C = 1.0 / alpha\n        accs = []\n        for train_idx, val_idx in skf_inner.split(X, y):\n            Xtr, Xval = X[train_idx], X[val_idx]\n            ytr, yval = y[train_idx], y[val_idx]\n            scaler = StandardScaler().fit(Xtr)\n            Xtr_s = scaler.transform(Xtr)\n            Xval_s = scaler.transform(Xval)\n            clf = SVC(kernel='linear', C=C, random_state=RNG)\n            clf.fit(Xtr_s, ytr)\n            yhat = np.sign(clf.decision_function(Xval_s))  # threshold 0 decision\n            accs.append(accuracy_score(yval, (yhat &gt; 0).astype(int)))\n        mean_acc = np.mean(accs)\n        if mean_acc &gt; best_acc:\n            best_acc, best_c = mean_acc, C\n    return best_c\n\n\ndef two_level_cv(X: np.ndarray, y: np.ndarray, label: str, coords: np.ndarray):\n    \"\"\"Perform 6×5 two‑level CV; write all artefacts to disk.\"\"\"\n    skf_outer = StratifiedKFold(n_splits=OUTER_FOLDS, shuffle=True, random_state=RNG)\n\n    fprs, tprs, aucs = [], [], []\n    all_scores, all_labels = [], []\n    weights_fold1 = None\n\n    for fold, (train_idx, test_idx) in enumerate(skf_outer.split(X, y), start=1):\n        Xtr, Xte = X[train_idx], X[test_idx]\n        ytr, yte = y[train_idx], y[test_idx]\n\n        best_c = _select_c(Xtr, ytr)\n        scaler = StandardScaler().fit(Xtr)\n        Xtr_s = scaler.transform(Xtr)\n        Xte_s = scaler.transform(Xte)\n        clf = SVC(kernel='linear', C=best_c, random_state=RNG)\n        clf.fit(Xtr_s, ytr)\n\n        # store weights from fold 1 for visualisation\n        if fold == 1:\n            weights_fold1 = clf.coef_.ravel().copy()\n\n        scores = clf.decision_function(Xte_s)\n        fpr, tpr, _ = roc_curve(yte, scores)\n        fprs.append(fpr); tprs.append(tpr)\n        aucs.append(auc(fpr, tpr))\n        all_scores.append(scores); all_labels.append(yte)\n\n        # per‑fold accuracy\n        acc = accuracy_score(yte, (scores &gt; 0).astype(int))\n        print(f\"[{label}] Fold {fold}: C = {best_c:.3g}, AUC = {aucs[-1]:.3f}, ACC = {acc:.3f}\")\n\n    # aggregate ROC\n    all_scores = np.hstack(all_scores)\n    all_labels = np.hstack(all_labels)\n    mean_fpr, mean_tpr, _ = roc_curve(all_labels, all_scores)\n    mean_auc = auc(mean_fpr, mean_tpr)\n\n    # save ROC figure\n    plot_roc(fprs, tprs, aucs, mean_fpr, mean_tpr, mean_auc,\n             title=f'{label.capitalize()} data — 6‑fold CV ROC',\n             fname=f'roc_{label}.png')\n\n    # save weight visualisations for fold 1\n    plot_weights_brain(weights_fold1, coords,\n                       title=f'{label.capitalize()} fold 1 — |weights| map',\n                       fname=f'weights_map_{label}.png')\n    stem_weights(weights_fold1,\n                 title=f'{label.capitalize()} fold 1 — signed channel weights',\n                 fname=f'weights_stem_{label}.png')\n\n    # dump aggregate arrays for future use\n    np.savez(OUTPUT_DIR / f'roc_raw_{label}.npz',\n         fprs=np.array(fprs, dtype=object),\n         tprs=np.array(tprs, dtype=object),\n         aucs=np.array(aucs),\n         agg_fpr=mean_fpr,\n         agg_tpr=mean_tpr,\n         agg_auc=mean_auc)\n\n    np.save(OUTPUT_DIR / f'weights_fold1_{label}.npy', weights_fold1)\n\n    print(f\"[{label}] Aggregate AUC = {mean_auc:.3f}\\n\")\n\n# -----------------------------------------------------------------------------\n# Cross‑train evaluation\n# -----------------------------------------------------------------------------\n\ndef cross_train_eval(train: Tuple[np.ndarray, np.ndarray],\n                     test: Tuple[np.ndarray, np.ndarray],\n                     train_name: str, test_name: str):\n    Xtr, ytr = train\n    Xte, yte = test\n\n    best_c = _select_c(Xtr, ytr)\n    scaler = StandardScaler().fit(Xtr)\n    clf = SVC(kernel='linear', C=best_c, random_state=RNG)\n    clf.fit(scaler.transform(Xtr), ytr)\n\n    scores = clf.decision_function(scaler.transform(Xte))\n    fpr, tpr, _ = roc_curve(yte, scores)\n    auc_val = auc(fpr, tpr)\n    acc = accuracy_score(yte, (scores &gt; 0).astype(int))\n\n    title = f'Train {train_name} → Test {test_name} (AUC {auc_val:.2f})'\n    plt.figure(figsize=(5, 5))\n    plt.plot(fpr, tpr, lw=2, label=f'AUC = {auc_val:.2f}')\n    plt.plot([0, 1], [0, 1], 'k--', lw=0.8)\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(title); plt.legend(); plt.tight_layout()\n    fname = f'roc_cross_{train_name}_to_{test_name}.png'\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n    print(f'[Cross‑train] {title}, ACC = {acc:.3f}, C = {best_c:.3g}\\n')\n\n# -----------------------------------------------------------------------------\n# Main entry‑point\n# -----------------------------------------------------------------------------\n\ndef main():\n    print('Loading data …')\n    datasets = load_datasets()\n    coords = pd.read_csv(DATA_DIR / '../../BCIsensor_xy.csv', header=None).values  # 102×2\n\n    for label, (X, y) in datasets.items():\n        print(f'Running two‑level CV on {label} data')\n        two_level_cv(X, y, label, coords)\n\n    # cross‑train scenarios\n    print('Running cross‑training scenarios')\n    cross_train_eval(datasets['overt'], datasets['imagined'], 'overt', 'imagined')\n    cross_train_eval(datasets['imagined'], datasets['overt'], 'imagined', 'overt')\n\n    print('All tasks complete. Figures and arrays available under outputs/.')\n\n\nif __name__ == '__main__':\n    main()\n\nLoading data …\nRunning two‑level CV on imagined data\n[imagined] Fold 1: C = 0.01, AUC = 0.960, ACC = 0.850\n[imagined] Fold 2: C = 100, AUC = 1.000, ACC = 0.950\n[imagined] Fold 3: C = 0.01, AUC = 0.908, ACC = 0.800\n[imagined] Fold 4: C = 0.01, AUC = 0.992, ACC = 0.925\n[imagined] Fold 5: C = 100, AUC = 0.963, ACC = 0.825\n[imagined] Fold 6: C = 0.01, AUC = 0.978, ACC = 0.900\n\n\n/var/folders/hv/8cbg61nd45x1vstcpkvdygcw0000gr/T/ipykernel_63180/607345600.py:96: MatplotlibDeprecationWarning: Getting the array from a PolyQuadMesh will return the full array in the future (uncompressed). To get this behavior now set the PolyQuadMesh with a 2D array .set_array(data2d).\n  plt.colorbar(pc, label='|weight|')\n\n\n[imagined] Aggregate AUC = 0.959\n\nRunning two‑level CV on overt data\n[overt] Fold 1: C = 100, AUC = 1.000, ACC = 0.975\n[overt] Fold 2: C = 100, AUC = 1.000, ACC = 0.975\n[overt] Fold 3: C = 100, AUC = 1.000, ACC = 0.975\n[overt] Fold 4: C = 100, AUC = 0.997, ACC = 0.975\n[overt] Fold 5: C = 100, AUC = 1.000, ACC = 0.950\n[overt] Fold 6: C = 100, AUC = 0.990, ACC = 0.950\n\n\n/var/folders/hv/8cbg61nd45x1vstcpkvdygcw0000gr/T/ipykernel_63180/607345600.py:96: MatplotlibDeprecationWarning: Getting the array from a PolyQuadMesh will return the full array in the future (uncompressed). To get this behavior now set the PolyQuadMesh with a 2D array .set_array(data2d).\n  plt.colorbar(pc, label='|weight|')\n\n\n[overt] Aggregate AUC = 0.998\n\nRunning cross‑training scenarios\n[Cross‑train] Train overt → Test imagined (AUC 0.96), ACC = 0.892, C = 100\n\n[Cross‑train] Train imagined → Test overt (AUC 0.98), ACC = 0.912, C = 100\n\nAll tasks complete. Figures and arrays available under outputs/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "",
    "text": "Welcome to my project. Here’s an equation: \\(f(x) = w^T x + b\\).\n\n\nMore details about the background…"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "",
    "text": "More details about the background…"
  },
  {
    "objectID": "index.html#data-acquisition",
    "href": "index.html#data-acquisition",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nDetails on how data was acquired…"
  },
  {
    "objectID": "index.html#signal-processing",
    "href": "index.html#signal-processing",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "2.2 Signal Processing",
    "text": "2.2 Signal Processing\nDetails on signal processing steps…\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.random.rand(5)\nprint(data) # Output the data if needed\n\nplt.figure()\nplt.plot(data, marker='o')\nplt.title(\"Random Data\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\nplt.grid(True)\nplt.show()\n\n[0.66173006 0.82062585 0.08149636 0.5091209  0.0275372 ]\n\n\n\n\n\n\n\n\nFigure 1: Plot of five random numbers."
  },
  {
    "objectID": "index.html#classification-performance",
    "href": "index.html#classification-performance",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "3.1 Classification Performance",
    "text": "3.1 Classification Performance\nOur movement intention decoding approach achieved promising results across multiple subjects. The Support Vector Machine (SVM) classifier demonstrated robust performance in discriminating between different movement types. Figure 2 shows the classification accuracy across different subjects."
  },
  {
    "objectID": "index.html#feature-importance",
    "href": "index.html#feature-importance",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "3.2 Feature Importance",
    "text": "3.2 Feature Importance\nAnalysis of feature importance revealed that temporal features in the alpha and beta frequency bands were most informative for movement decoding. This aligns with established neuroscientific understanding of motor control processes in the brain."
  },
  {
    "objectID": "index.html#comparison-with-previous-work",
    "href": "index.html#comparison-with-previous-work",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "4.1 Comparison with Previous Work",
    "text": "4.1 Comparison with Previous Work\nOur approach builds upon previous BCI research while addressing several key limitations. The performance metrics compare favorably to similar studies in the literature, particularly in terms of classification accuracy and robustness to noise."
  }
]