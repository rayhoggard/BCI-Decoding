[
  {
    "objectID": "notebooks/Untitled.html",
    "href": "notebooks/Untitled.html",
    "title": "BCI Movement Decoding",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv(\"../assets/BCIsensor_xy.csv\", header=None)\nx = df.iloc[:, 0]\ny = df.iloc[:, 1]\nplt.plot(x, y)\nplt.scatter(x, y, color=\"blue\", s=20, zorder=3)\nfor i, (xi, yi) in enumerate(zip(x, y)):\n        plt.text(xi, yi, str(i + 1), fontsize=8, ha='right', va='bottom')\nplt.axis(\"equal\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import griddata\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, make_scorer\nimport time\nimport warnings\n\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n\ndef load_and_prepare_data(file1, file2, label1=0, label2=1):\n    print(f\"Loading data: {file1} (Label {label1}), {file2} (Label {label2})\")\n    try:\n        df1 = pd.read_csv(file1, header=None).T\n        df2 = pd.read_csv(file2, header=None).T\n        if df1.shape[1] != 204 or df2.shape[1] != 204:\n             print(f\"Warning: Expected 204 features. Got {df1.shape[1]} & {df2.shape[1]}.\")\n        y1 = np.full(df1.shape[0], label1)\n        y2 = np.full(df2.shape[0], label2)\n        X = pd.concat([df1, df2], ignore_index=True).values\n        y = np.concatenate([y1, y2])\n        print(f\"Data loaded. X shape: {X.shape}, y shape: {y.shape}\")\n        return X, y\n    except FileNotFoundError as e:\n        print(f\"Error: File not found - {e}.\")\n        return None, None\n    except Exception as e:\n        print(f\"Error loading data files: {e}\")\n        return None, None\n\ndef load_sensor_locations(filename=\"BCIsensor_xy.csv\"):\n    print(f\"Loading sensor locations from: {filename}\")\n    try:\n        locations = pd.read_csv(filename, header=None, names=['x', 'y']).values\n        if locations.shape[0] != 102 or locations.shape[1] != 2:\n            print(f\"Warning: Expected 102x2 shape, got {locations.shape}.\")\n        print(f\"Sensor locations loaded. Shape: {locations.shape}\")\n        return locations[:, 0], locations[:, 1]\n    except FileNotFoundError:\n        print(f\"Error: Sensor location file '{filename}' not found. Please ensure it's in the correct directory (e.g., '../assets/').\")\n        return None, None\n    except Exception as e:\n        print(f\"Error loading sensor locations: {e}\")\n        return None, None\n\ndef approximate_svm_weights(svm_model):\n    try:\n        weights = svm_model.dual_coef_ @ svm_model.support_vectors_\n        return weights.flatten()\n    except Exception as e:\n        print(f\"Error approximating SVM weights: {e}\")\n        return None\n\n\ndef perform_two_level_cv(X, y, data_label=\"Data\", kernel='linear', gamma='scale', degree=3, C_range=None, outer_k=6, inner_k=5):\n    if C_range is None:\n      C_range = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] # Default range\n\n    outer_cv = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=42)\n    inner_cv = StratifiedKFold(n_splits=inner_k, shuffle=True, random_state=42)\n\n    outer_fold_accuracies, optimal_Cs = [], []\n    all_true_labels, all_decision_scores = [], []\n    outer_fold_true_labels, outer_fold_scores = [], []\n    fold1_coefficients = None\n\n    print(f\"\\n--- Starting {outer_k}-Fold Outer CV for {data_label} Data (Kernel: {kernel}) ---\")\n    start_time_cv = time.time()\n\n    for i, (train_outer_idx, test_outer_idx) in enumerate(outer_cv.split(X, y)):\n        fold_start_time = time.time()\n        print(f\"  Outer Fold {i+1}/{outer_k}\")\n        X_train_outer, X_test_outer = X[train_outer_idx], X[test_outer_idx]\n        y_train_outer, y_test_outer = y[train_outer_idx], y[test_outer_idx]\n\n        scaler = StandardScaler()\n        X_train_outer_scaled = scaler.fit_transform(X_train_outer)\n        X_test_outer_scaled = scaler.transform(X_test_outer)\n\n        best_inner_acc = -1\n        best_C_for_fold = C_range[0]\n        for C_val in C_range:\n            inner_fold_accuracies = []\n            for j, (train_inner_idx, val_inner_idx) in enumerate(inner_cv.split(X_train_outer_scaled, y_train_outer)):\n                X_train_inner, X_val_inner = X_train_outer_scaled[train_inner_idx], X_train_outer_scaled[val_inner_idx]\n                y_train_inner, y_val_inner = y_train_outer[train_inner_idx], y_train_outer[val_inner_idx]\n                svm_inner = SVC(kernel=kernel, C=C_val, gamma=gamma, degree=degree,\n                                random_state=42, probability=False, max_iter=10000)\n                svm_inner.fit(X_train_inner, y_train_inner)\n                accuracy = svm_inner.score(X_val_inner, y_val_inner)\n                inner_fold_accuracies.append(accuracy)\n            avg_inner_acc = np.mean(inner_fold_accuracies)\n            if avg_inner_acc &gt; best_inner_acc:\n                best_inner_acc = avg_inner_acc\n                best_C_for_fold = C_val\n\n        optimal_Cs.append(best_C_for_fold)\n\n        final_svm = SVC(kernel=kernel, C=best_C_for_fold, gamma=gamma, degree=degree,\n                        random_state=42, probability=False, max_iter=20000)\n        final_svm.fit(X_train_outer_scaled, y_train_outer)\n\n        outer_accuracy = final_svm.score(X_test_outer_scaled, y_test_outer)\n        outer_fold_accuracies.append(outer_accuracy)\n        decision_scores = final_svm.decision_function(X_test_outer_scaled)\n        all_true_labels.extend(y_test_outer)\n        all_decision_scores.extend(decision_scores)\n        outer_fold_true_labels.append(y_test_outer)\n        outer_fold_scores.append(decision_scores)\n\n        if i == 0:\n            if kernel == 'linear':\n                fold1_coefficients = final_svm.coef_.flatten()\n                print(\"    Stored actual coefficients for linear kernel (Fold 1).\")\n            else:\n                print(f\"    Attempting to approximate coefficients for {kernel} kernel (Fold 1)...\")\n                fold1_coefficients = approximate_svm_weights(final_svm)\n                if fold1_coefficients is not None:\n                    print(\"    Successfully approximated and stored coefficients.\")\n                else:\n                     print(\"    Approximation failed or not possible.\")\n\n        fold_end_time = time.time()\n        print(f\"  Outer Fold {i+1} completed. Accuracy: {outer_accuracy:.4f}. Optimal C: {best_C_for_fold}. Time: {fold_end_time - fold_start_time:.1f}s\")\n\n    end_time_cv = time.time()\n    average_accuracy = np.mean(outer_fold_accuracies)\n    std_dev_accuracy = np.std(outer_fold_accuracies)\n\n    print(f\"\\nFinished {outer_k}-fold CV for {data_label} data (Kernel: {kernel}).\")\n    print(f\"Total CV Time: {end_time_cv - start_time_cv:.1f}s\")\n    print(f\"Optimal C found per fold: {optimal_Cs}\")\n    print(f\"Accuracy per fold: {[f'{acc:.4f}' for acc in outer_fold_accuracies]}\")\n    print(f\"Average Accuracy: {average_accuracy:.4f} +/- {std_dev_accuracy:.4f}\")\n\n    results = {\n        'avg_accuracy': average_accuracy, 'std_accuracy': std_dev_accuracy,\n        'optimal_Cs': optimal_Cs, 'fold_accuracies': outer_fold_accuracies,\n        'all_true_labels': np.array(all_true_labels),\n        'all_decision_scores': np.array(all_decision_scores),\n        'fold1_coefficients': fold1_coefficients, # Contains actual (linear) or approximated weights\n        'outer_fold_true_labels': outer_fold_true_labels,\n        'outer_fold_scores': outer_fold_scores,\n        'kernel': kernel # Store kernel used\n    }\n    return results\n\n\ndef tune_hyperparameters(X_train, y_train, kernel, C_range, gamma_range=None, degree_range=None, n_splits=5):\n    print(f\"    Tuning hyperparameters (Kernel: {kernel}) using {n_splits}-fold CV...\")\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train) # Scale data once before tuning\n\n    param_grid = {'C': C_range}\n    if kernel in ['rbf', 'poly', 'sigmoid'] and gamma_range:\n        param_grid['gamma'] = gamma_range\n    if kernel == 'poly' and degree_range:\n        param_grid['degree'] = degree_range\n\n    scorer = make_scorer(accuracy_score)\n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    grid_search = GridSearchCV(SVC(kernel=kernel, random_state=42, probability=False, max_iter=10000),\n                               param_grid, scoring=scorer, cv=cv, n_jobs=-1) # Use all available CPU cores\n    try:\n        grid_search.fit(X_train_scaled, y_train)\n        print(f\"    Best parameters found: {grid_search.best_params_} (Best Score: {grid_search.best_score_:.4f})\")\n        return grid_search.best_params_\n    except Exception as e:\n        print(f\"    Error during GridSearchCV: {e}\")\n        # Return default C if grid search fails? Or raise error?\n        print(\"    Hyperparameter tuning failed.\")\n        return None\n\n\ndef perform_cross_training(X_train, y_train, X_test, y_test, train_label, test_label, kernel='linear', gamma='scale', degree=3):\n    if X_train is None or y_train is None or X_test is None or y_test is None:\n        print(\"Error: Missing input data for cross-training.\")\n        return None\n\n    print(f\"\\n--- Cross-Training: Train on {train_label}, Test on {test_label} (Kernel: {kernel}) ---\")\n    start_time_xt = time.time()\n\n    C_range_xt = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n    gamma_range_xt = ['scale', 'auto', 0.001, 0.01, 0.1] # Example if tuning gamma for RBF/Poly\n    degree_range_xt = [2, 3, 4]         # Example if tuning degree for Poly\n\n    tune_gamma = kernel in ['rbf', 'poly', 'sigmoid']\n    tune_degree = kernel == 'poly'\n\n    best_params = tune_hyperparameters(\n        X_train, y_train, kernel, C_range_xt,\n        gamma_range=gamma_range_xt if tune_gamma else None,\n        degree_range=degree_range_xt if tune_degree else None\n    )\n\n    if best_params is None:\n        print(\"Cross-training aborted due to hyperparameter tuning failure.\")\n        return None\n\n    best_C = best_params.get('C') # Should always be found if tuning didn't fail\n    best_gamma = best_params.get('gamma', gamma) # Use passed gamma if not tuned\n    best_degree = best_params.get('degree', degree) # Use passed degree if not tuned\n    print(f\"    Using parameters for final model: C={best_C}, gamma={best_gamma}, degree={best_degree}\")\n\n    print(\"    Training final model on entire training set...\")\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    final_svm = SVC(kernel=kernel, C=best_C, gamma=best_gamma, degree=best_degree,\n                    random_state=42, probability=False, max_iter=20000)\n    final_svm.fit(X_train_scaled, y_train)\n\n    print(\"    Testing model on test set...\")\n    X_test_scaled = scaler.transform(X_test) # Use scaler fit on training data\n    accuracy = final_svm.score(X_test_scaled, y_test)\n    decision_scores = final_svm.decision_function(X_test_scaled)\n\n    end_time_xt = time.time()\n    print(f\"Cross-Training completed. Time: {end_time_xt - start_time_xt:.1f}s\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n    results = {\n        'accuracy': accuracy,\n        'true_labels': y_test,\n        'decision_scores': decision_scores,\n        'best_params': best_params,\n        'kernel': kernel\n    }\n    return results\n\n\n\ndef plot_svm_weights_stem(weights, title=\"SVM Channel Weights (Fold 1)\", top_n=6):\n    \"\"\"Generates a stem plot of SVM weights (actual or approximated), highlighting top N.\"\"\"\n    if weights is None:\n        print(f\"Info: Skipping stem plot '{title}' (weights not available).\")\n        return\n    if len(weights) == 0:\n        print(f\"Error: Empty weights provided for stem plot '{title}'.\")\n        return\n\n    n_channels = len(weights)\n    if n_channels != 204:\n         print(f\"Warning: Expected 204 weights for stem plot, got {n_channels}. Adjusting plot limits.\")\n\n    channel_indices = np.arange(1, n_channels + 1)\n    actual_top_n = min(top_n, n_channels)\n    if actual_top_n &lt; top_n:\n        print(f\"Warning: Requested top {top_n} channels, but only {n_channels} available. Plotting top {actual_top_n}.\")\n\n    # Handle cases with very few channels correctly\n    if actual_top_n &gt; 0:\n        dominant_indices = np.argsort(np.abs(weights))[-actual_top_n:]\n    else:\n        dominant_indices = []\n\n\n    plt.figure(figsize=(12, 6))\n    markerline, stemlines, baseline = plt.stem(channel_indices, weights, linefmt='grey', markerfmt='o', basefmt='r-', label='_nolegend_')\n    plt.setp(markerline, markersize=4, markerfacecolor='grey', markeredgecolor='black')\n    for idx in dominant_indices:\n        plt.stem(channel_indices[idx], weights[idx], linefmt='b-', markerfmt='bo', basefmt=' ') # Re-plot dominant stems to be blue\n        vertical_offset = 0.05 * np.max(np.abs(weights)) * np.sign(weights[idx]) if np.max(np.abs(weights)) &gt; 0 else 0.05 * np.sign(weights[idx])\n        # If weight is zero, place text slightly above\n        if weights[idx] == 0:\n            vertical_offset = 0.05\n        plt.text(channel_indices[idx], weights[idx] + vertical_offset, f'{weights[idx]:.2f}',\n                 ha='center', va='bottom' if weights[idx] &gt;= 0 else 'top', color='blue', fontsize=9)\n\n    if actual_top_n &gt; 0:\n        dominant_proxy = plt.Line2D([0], [0], linestyle='none', c='b', marker='o', markersize=5, label=f'Top {actual_top_n} Abs. Magnitude Channels')\n        plt.legend(handles=[dominant_proxy])\n    else:\n        plt.legend()\n\n    plt.xlabel(\"Channel Index (1-204)\"); plt.ylabel(\"SVM Weight / Approx. Weight\"); plt.title(title)\n    plt.xlim(0, max(n_channels, 1) + 1) # Ensure xlim is at least (0, 2)\n    plt.grid(True, axis='y', linestyle=':'); plt.tight_layout(); plt.show()\n\n\ndef plot_weights_on_brain(weights, sensor_x, sensor_y, title=\"SVM Weight Magnitude on Brain Surface (Fold 1)\", grid_resolution=100):\n    if weights is None:\n        print(f\"Info: Skipping brain plot '{title}' (weights not available).\")\n        return\n    if len(weights) != 204:\n        print(f\"Error: Expected 204 weights for brain plot '{title}'. Got {len(weights)}.\")\n        return\n    if sensor_x is None or sensor_y is None or len(sensor_x) != 102 or len(sensor_y) != 102:\n        print(f\"Error: Sensor locations missing or incorrect for brain plot '{title}'.\")\n        return\n\n    try:\n        electrode_magnitudes = np.sqrt(weights[0::2]**2 + weights[1::2]**2)\n    except IndexError:\n         print(f\"Error: Could not pair weights for magnitude calculation in brain plot '{title}'. Check weight vector length.\")\n         return\n\n    if len(electrode_magnitudes) != 102:\n        print(f\"Error: Calculated electrode magnitude array size incorrect ({len(electrode_magnitudes)}). Expected 102 for plot '{title}'.\"); return\n\n    xi = np.linspace(sensor_x.min()-0.5, sensor_x.max()+0.5, grid_resolution)\n    yi = np.linspace(sensor_y.min()-0.5, sensor_y.max()+0.5, grid_resolution)\n    xi, yi = np.meshgrid(xi, yi)\n\n    zi = griddata((sensor_x, sensor_y), electrode_magnitudes, (xi, yi), method='cubic') # 'linear', 'nearest', 'cubic'\n\n    plt.figure(figsize=(7, 6))\n    contour = plt.contourf(xi, yi, zi, levels=15, cmap=plt.cm.viridis) # Adjust levels and cmap as needed\n    plt.colorbar(contour, label='SVM Weight Magnitude (Approx. for Non-Linear)')\n    # Overlay sensor locations\n    # plt.scatter(sensor_x, sensor_y, c='red', s=10, label='Sensor Locations') # Optional: Show sensors\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.axis('off') # Hide axes for a cleaner look\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_overall_roc(true_labels, decision_scores, data_label=\"Data\", title=\"Overall ROC Curve\"):\n    if true_labels is None or decision_scores is None:\n        print(f\"Error: Missing true_labels or decision_scores for ROC plot '{title}'.\")\n        return\n    if len(true_labels) != len(decision_scores):\n         print(f\"Error: Mismatched lengths for true_labels ({len(true_labels)}) and decision_scores ({len(decision_scores)}) for ROC plot '{title}'.\")\n         return\n    if len(true_labels) == 0:\n        print(f\"Error: Empty data provided for ROC plot '{title}'.\")\n        return\n\n    fpr, tpr, thresholds = roc_curve(true_labels, decision_scores)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC ({data_label}, AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance (AUC = 0.50)')\n    plt.xlim([-0.02, 1.0]); plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(title)\n    plt.legend(loc=\"lower right\"); plt.grid(True, linestyle=':'); plt.tight_layout(); plt.show()\n\n\ndef plot_individual_and_overall_roc(outer_fold_true_labels, outer_fold_scores,\n                                    all_true_labels, all_decision_scores,\n                                    data_label=\"Data\", title=\"Individual Fold and Overall ROC Curves\"):\n    if not isinstance(outer_fold_true_labels, list) or not isinstance(outer_fold_scores, list) or \\\n       len(outer_fold_true_labels) != len(outer_fold_scores):\n        print(f\"Error: Invalid or mismatched per-fold data for Indiv. ROC plot '{title}'. Expected lists of arrays.\")\n        return\n    if all_true_labels is None or all_decision_scores is None :\n         print(f\"Error: Missing aggregated data (all_true_labels or all_decision_scores) for Indiv. ROC plot '{title}'.\")\n         return\n    if len(all_true_labels) == 0:\n         print(f\"Error: Empty aggregated data for Indiv. ROC plot '{title}'.\")\n         return\n\n\n    plt.figure(figsize=(9, 7))\n    n_folds = len(outer_fold_true_labels)\n    fold_aucs = []\n    for i in range(n_folds):\n        # Check if fold data is valid\n        if len(outer_fold_true_labels[i]) &gt; 0 and len(outer_fold_scores[i]) &gt; 0 and \\\n           len(np.unique(outer_fold_true_labels[i])) &gt; 1: # Need at least two classes for ROC\n            try:\n                fpr, tpr, _ = roc_curve(outer_fold_true_labels[i], outer_fold_scores[i])\n                fold_auc = auc(fpr, tpr)\n                fold_aucs.append(fold_auc)\n                plt.plot(fpr, tpr, lw=1, alpha=0.4, label=f'Fold {i+1} (AUC = {fold_auc:.2f})')\n            except Exception as e:\n                print(f\"Warning: Could not plot ROC for fold {i+1}. Error: {e}\")\n        else:\n             print(f\"Warning: Skipping ROC for fold {i+1} due to insufficient or invalid data.\")\n\n    # Calculate and plot overall ROC only if aggregated data is valid\n    if len(all_true_labels) &gt; 0 and len(all_decision_scores) &gt; 0 and \\\n       len(np.unique(all_true_labels)) &gt; 1:\n            try:\n                fpr_all, tpr_all, _ = roc_curve(all_true_labels, all_decision_scores)\n                roc_auc_all = auc(fpr_all, tpr_all)\n                plt.plot(fpr_all, tpr_all, color='b', lw=2.5, alpha=0.9,\n                         label=f'Overall ROC ({data_label}, AUC = {roc_auc_all:.2f})')\n            except Exception as e:\n                print(f\"Warning: Could not plot Overall ROC. Error: {e}\")\n    else:\n         print(\"Warning: Skipping Overall ROC due to insufficient or invalid aggregated data.\")\n\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance (AUC = 0.50)')\n    plt.xlim([-0.02, 1.0]); plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(title)\n    if fold_aucs:\n        mean_fold_auc = np.mean(fold_aucs)\n        std_fold_auc = np.std(fold_aucs)\n        plt.legend(loc=\"lower right\", fontsize='small', title=f\"Mean Fold AUC: {mean_fold_auc:.2f} ± {std_fold_auc:.2f}\")\n    else:\n        plt.legend(loc=\"lower right\", fontsize='small')\n    plt.grid(True, linestyle=':')\n    plt.tight_layout(); plt.show()\n\nif __name__ == \"__main__\":\n    print(\"\\n=== Starting BCI Analysis ===\")\n\n    BASE_PATH = '../assets/'\n    OVERT_FILE_1 = BASE_PATH + 'feaSubEOvert_1.csv'\n    OVERT_FILE_2 = BASE_PATH + 'feaSubEOvert_2.csv'\n    IMG_FILE_1 = BASE_PATH + 'feaSubEImg_1.csv'\n    IMG_FILE_2 = BASE_PATH + 'feaSubEImg_2.csv'\n    SENSOR_FILE = BASE_PATH + 'BCIsensor_xy.csv'\n\n    RUN_LINEAR_KERNEL = True      # Run linear kernel experiments\n    RUN_RBF_KERNEL = True         # Run RBF kernel experiments\n    RUN_POLY_KERNEL = True        # Run Polynomial kernel experiments\n    RUN_CROSS_TRAIN_LINEAR = True # Run cross-training (only implemented for linear here)\n\n    DEFAULT_GAMMA = 'scale'\n    DEFAULT_DEGREE = 3\n    DEFAULT_C_RANGE = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\n    sensor_x, sensor_y = load_sensor_locations(SENSOR_FILE)\n\n    X_overt, y_overt = load_and_prepare_data(OVERT_FILE_1, OVERT_FILE_2)\n    X_img, y_img = load_and_prepare_data(IMG_FILE_1, IMG_FILE_2)\n\n    def run_plots(results, data_type, kernel_type, sensor_x, sensor_y):\n\n        weights = results.get('fold1_coefficients', None)\n        weight_note = \"(Approximated)\" if kernel_type != 'linear' and weights is not None else \"\"\n\n        plot_svm_weights_stem(weights,\n                              title=f\"{data_type} ({kernel_type}): SVM Weights {weight_note} (Fold 1)\")\n        if sensor_x is not None and sensor_y is not None:\n             plot_weights_on_brain(weights, sensor_x, sensor_y,\n                                   title=f\"{data_type} ({kernel_type}): Weight Magnitude {weight_note} (Fold 1)\")\n        else:\n             print(f\"Skipping brain plot for {data_type} ({kernel_type}) due to missing sensor locations.\")\n\n        plot_individual_and_overall_roc(results.get('outer_fold_true_labels', []), results.get('outer_fold_scores', []),\n                                        results.get('all_true_labels', None), results.get('all_decision_scores', None),\n                                        data_label=f\"{data_type} ({kernel_type})\",\n                                        title=f\"{data_type} ({kernel_type}): Individual & Overall ROC\")\n\n    if RUN_LINEAR_KERNEL:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Overt (Linear Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None:\n            overt_results_lin = perform_two_level_cv(X_overt, y_overt, data_label=\"Overt\", kernel='linear', C_range=DEFAULT_C_RANGE)\n            run_plots(overt_results_lin, \"Overt\", \"Linear\", sensor_x, sensor_y)\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Imagined (Linear Kernel)\\n\" + \"=\"*30)\n        if X_img is not None:\n             img_results_lin = perform_two_level_cv(X_img, y_img, data_label=\"Imagined\", kernel='linear', C_range=DEFAULT_C_RANGE)\n             run_plots(img_results_lin, \"Imagined\", \"Linear\", sensor_x, sensor_y)\n\n\n    if RUN_CROSS_TRAIN_LINEAR:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Cross-Train Overt -&gt; Imagined (Linear Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None and X_img is not None:\n            xt_ov_img_lin = perform_cross_training(X_overt, y_overt, X_img, y_img, \"Overt\", \"Imagined\", kernel='linear')\n            if xt_ov_img_lin:\n                 plot_overall_roc(xt_ov_img_lin['true_labels'], xt_ov_img_lin['decision_scores'],\n                                  data_label=\"Train Overt, Test Imagined (Linear)\",\n                                  title=\"Cross-Train ROC: Overt -&gt; Imagined (Linear)\")\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Cross-Train Imagined -&gt; Overt (Linear Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None and X_img is not None:\n             xt_img_ov_lin = perform_cross_training(X_img, y_img, X_overt, y_overt, \"Imagined\", \"Overt\", kernel='linear')\n             if xt_img_ov_lin:\n                  plot_overall_roc(xt_img_ov_lin['true_labels'], xt_img_ov_lin['decision_scores'],\n                                   data_label=\"Train Imagined, Test Overt (Linear)\",\n                                   title=\"Cross-Train ROC: Imagined -&gt; Overt (Linear)\")\n\n    if RUN_RBF_KERNEL:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Overt (RBF Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None:\n            overt_results_rbf = perform_two_level_cv(X_overt, y_overt, data_label=\"Overt\", kernel='rbf', gamma=DEFAULT_GAMMA, C_range=DEFAULT_C_RANGE)\n            run_plots(overt_results_rbf, \"Overt\", \"RBF\", sensor_x, sensor_y)\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Imagined (RBF Kernel)\\n\" + \"=\"*30)\n        if X_img is not None:\n            img_results_rbf = perform_two_level_cv(X_img, y_img, data_label=\"Imagined\", kernel='rbf', gamma=DEFAULT_GAMMA, C_range=DEFAULT_C_RANGE)\n            run_plots(img_results_rbf, \"Imagined\", \"RBF\", sensor_x, sensor_y)\n\n    if RUN_POLY_KERNEL:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Overt (Polynomial Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None:\n            overt_results_poly = perform_two_level_cv(X_overt, y_overt, data_label=\"Overt\", kernel='poly',\n                                                     gamma=DEFAULT_GAMMA, degree=DEFAULT_DEGREE, C_range=DEFAULT_C_RANGE)\n            run_plots(overt_results_poly, \"Overt\", \"Polynomial\", sensor_x, sensor_y)\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Imagined (Polynomial Kernel)\\n\" + \"=\"*30)\n        if X_img is not None:\n            img_results_poly = perform_two_level_cv(X_img, y_img, data_label=\"Imagined\", kernel='poly',\n                                                   gamma=DEFAULT_GAMMA, degree=DEFAULT_DEGREE, C_range=DEFAULT_C_RANGE)\n            run_plots(img_results_poly, \"Imagined\", \"Polynomial\", sensor_x, sensor_y)\n\n\n=== Starting BCI Analysis ===\nLoading sensor locations from: ../assets/BCIsensor_xy.csv\nSensor locations loaded. Shape: (102, 2)\nLoading data: ../assets/feaSubEOvert_1.csv (Label 0), ../assets/feaSubEOvert_2.csv (Label 1)\nData loaded. X shape: (240, 204), y shape: (240,)\nLoading data: ../assets/feaSubEImg_1.csv (Label 0), ../assets/feaSubEImg_2.csv (Label 1)\nData loaded. X shape: (240, 204), y shape: (240,)\n\n==============================\n Scenario: Same-Train Overt (Linear Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Overt Data (Kernel: linear) ---\n  Outer Fold 1/6\n    Stored actual coefficients for linear kernel (Fold 1).\n  Outer Fold 1 completed. Accuracy: 0.9500. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9750. Optimal C: 1. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.9750. Optimal C: 1. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9000. Optimal C: 0.001. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.9750. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.9500. Optimal C: 0.1. Time: 0.0s\n\nFinished 6-fold CV for Overt data (Kernel: linear).\nTotal CV Time: 0.3s\nOptimal C found per fold: [0.1, 1, 1, 0.001, 0.1, 0.1]\nAccuracy per fold: ['0.9500', '0.9750', '0.9750', '0.9000', '0.9750', '0.9500']\nAverage Accuracy: 0.9542 +/- 0.0267\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Imagined (Linear Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Imagined Data (Kernel: linear) ---\n  Outer Fold 1/6\n    Stored actual coefficients for linear kernel (Fold 1).\n  Outer Fold 1 completed. Accuracy: 0.8500. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9500. Optimal C: 0.01. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.8000. Optimal C: 0.01. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9250. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.8500. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.9500. Optimal C: 0.1. Time: 0.1s\n\nFinished 6-fold CV for Imagined data (Kernel: linear).\nTotal CV Time: 0.5s\nOptimal C found per fold: [0.1, 0.01, 0.01, 0.1, 0.1, 0.1]\nAccuracy per fold: ['0.8500', '0.9500', '0.8000', '0.9250', '0.8500', '0.9500']\nAverage Accuracy: 0.8875 +/- 0.0573\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Cross-Train Overt -&gt; Imagined (Linear Kernel)\n==============================\n\n--- Cross-Training: Train on Overt, Test on Imagined (Kernel: linear) ---\n    Tuning hyperparameters (Kernel: linear) using 5-fold CV...\n    Best parameters found: {'C': 0.1} (Best Score: 0.9583)\n    Using parameters for final model: C=0.1, gamma=scale, degree=3\n    Training final model on entire training set...\n    Testing model on test set...\nCross-Training completed. Time: 1.3s\nTest Accuracy: 0.8917\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Cross-Train Imagined -&gt; Overt (Linear Kernel)\n==============================\n\n--- Cross-Training: Train on Imagined, Test on Overt (Kernel: linear) ---\n    Tuning hyperparameters (Kernel: linear) using 5-fold CV...\n    Best parameters found: {'C': 1} (Best Score: 0.9042)\n    Using parameters for final model: C=1, gamma=scale, degree=3\n    Training final model on entire training set...\n    Testing model on test set...\nCross-Training completed. Time: 0.1s\nTest Accuracy: 0.9125\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Overt (RBF Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Overt Data (Kernel: rbf) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for rbf kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 0.9750. Optimal C: 1. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9250. Optimal C: 10. Time: 0.2s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.9250. Optimal C: 10. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9250. Optimal C: 10. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.9750. Optimal C: 10. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.8750. Optimal C: 10. Time: 0.1s\n\nFinished 6-fold CV for Overt data (Kernel: rbf).\nTotal CV Time: 0.8s\nOptimal C found per fold: [1, 10, 10, 10, 10, 10]\nAccuracy per fold: ['0.9750', '0.9250', '0.9250', '0.9250', '0.9750', '0.8750']\nAverage Accuracy: 0.9333 +/- 0.0344\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Imagined (RBF Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Imagined Data (Kernel: rbf) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for rbf kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 0.8500. Optimal C: 10. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9250. Optimal C: 1. Time: 0.2s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.8000. Optimal C: 1. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.8500. Optimal C: 1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.8750. Optimal C: 10. Time: 0.1s\n\nFinished 6-fold CV for Imagined data (Kernel: rbf).\nTotal CV Time: 0.8s\nOptimal C found per fold: [10, 1, 1, 1, 10, 10]\nAccuracy per fold: ['0.8500', '0.9250', '0.8000', '0.8500', '0.8250', '0.8750']\nAverage Accuracy: 0.8542 +/- 0.0393\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Overt (Polynomial Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Overt Data (Kernel: poly) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for poly kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 1.0000. Optimal C: 10. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9500. Optimal C: 1. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.9500. Optimal C: 100. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9250. Optimal C: 1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.9500. Optimal C: 1. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.9250. Optimal C: 1. Time: 0.1s\n\nFinished 6-fold CV for Overt data (Kernel: poly).\nTotal CV Time: 0.6s\nOptimal C found per fold: [10, 1, 100, 1, 1, 1]\nAccuracy per fold: ['1.0000', '0.9500', '0.9500', '0.9250', '0.9500', '0.9250']\nAverage Accuracy: 0.9500 +/- 0.0250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Imagined (Polynomial Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Imagined Data (Kernel: poly) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for poly kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.8750. Optimal C: 10. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.7750. Optimal C: 10. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.7750. Optimal C: 1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n\nFinished 6-fold CV for Imagined data (Kernel: poly).\nTotal CV Time: 0.6s\nOptimal C found per fold: [10, 10, 10, 1, 10, 10]\nAccuracy per fold: ['0.8250', '0.8750', '0.7750', '0.7750', '0.8250', '0.8250']\nAverage Accuracy: 0.8167 +/- 0.0344"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "",
    "text": "For decades now, researchers have been exploring brain-computer interfaces (BCIs) as a potential way for individuals with paralysis or other severe motor disabilities to interact with the world again. It’s an incredible field, but it comes with significant hurdles – everything from reliably picking up brain signals to accurately figuring out what those signals actually mean in terms of user intent.\n\nProject Goals: What we’re focusing on in this project is one specific piece of that puzzle: how can we classify electrical brain activity, measured non-invasively using electroencephalography (EEG), to understand someone’s intended movement? To do this, we employed a machine learning tool called a Support Vector Machine, or SVM, exploring different configurations (kernels) to see what works best.\nApproach Overview: Our main goal here was to see if we could determine, just by looking at EEG data, whether a person was intending to move their left hand or their right hand, even if they weren’t physically moving at all. The BCI system enabling this has two parts in our setup: the EEG system that captures the raw brain signals, and the SVM algorithm that interprets those signals and makes the left-vs-right classification.\nReport Scope: By the end of this discussion, we’ll have a clear picture of how well different SVM variants perform when applied to this EEG data for decoding movement intentions. We’ll look at the overall accuracy and ROC curves, compare linear, RBF, and polynomial kernels, and also look into some specific factors within the model and the data that influenced how well they worked.\n\n\n\nWhy is this so important? Well, advancements in BCIs could genuinely change lives for a vast number of people. For some perspective, the 2013 US Paralysis Prevalence & Health Disparities Survey found that nearly 5.4 million people were living with paralysis (Armour et al., 2016). Back then, that was almost 1.7% of the entire US population, and that number has likely only grown. Paralysis often brings profound challenges – the same survey noted that only about 15.5% of these individuals were employed, and over 30% were smokers. Given these severe impacts, technologies that can offer some relief or restore function are incredibly valuable for improving quality of life. BCIs stand out as one of the most promising paths forward to potentially restore movement capabilities and significantly enhance well-being for those affected.\n\n\n\n\n\nLet’s start with the basics. Our brain is constantly sending out electrical signals that orchestrate everything our body does. But this communication network relies on many biological components, and if any part breaks down, the connection can be lost. For people experiencing this, a Brain-Computer Interface (BCI) offers an alternative route, or a way for the brain’s commands to bypass the damaged pathways and still control external devices or even their own limbs (Nicolas-Alonso & Gomez-Gil, 2012). Ultimately, a BCI acts as a translator, converting brain activity directly into control signals. To make this happen, we need two key things: a way to ‘listen’ to the brain’s signals, and a way to make sense of them. In this project, we’re focusing on non-invasive BCIs that use Electroencephalography (EEG) to pick up those signals.\nEEG works by placing sensors on the scalp to detect the tiny electrical fields generated by brain activity. While it has the major advantage of being non-invasive (no surgery required!), the signals we get face some challenges. They’re incredibly complex, often buried in noise (from muscle twitches, eye blinks, even electrical interference from nearby devices), and they’re weakened as they pass through the skull and scalp. Plus, to get a good spatial picture of brain activity, we use many electrodes simultaneously, which results in very high-dimensional data – meaning each snapshot of brain activity has lots of different measurements to consider. Trying to reliably pull out a specific intention, like “move left” versus “move right,” from this noisy, high-dimensional stream requires robust analysis tools. That’s where machine learning comes into play.\n\n\n\nOkay, so how do we actually interpret these complex brain signals? That’s where Support Vector Machines, or SVMs, come in. SVMs are a type of supervised machine learning model that are good at classification tasks – essentially, sorting data into predefined categories. You train an SVM by showing it examples that are already labeled (like EEG snippets labeled as “intended left” or “intended right”). The SVM learns the patterns distinguishing these categories. Then, when you give it a new, unlabeled piece of data (a new EEG segment), it uses what it learned to predict which category that new data point belongs to. In our case, the SVM’s job is to look at the features extracted from an EEG signal and decide: “left intention” or “right intention”?\nSo, how does an SVM actually do this? Conceptually, it tries to find the “best” possible boundary to separate the data points belonging to different classes. Imagine you have a scatter plot with red dots and green dots. If the red dots are mostly in one area and the green dots in another, an SVM tries to draw a line (or, in higher dimensions, a plane or hyperplane) that separates them.\n\n\n\n\n\n\nFigure 1: Figure 1: SVM\n\n\n\nSee the plot on the right (in the imagined figure)? That blue line is the decision boundary found by the SVM. What makes SVMs special is how they find this boundary. They don’t just pick any line that separates the groups; they specifically look for the line that creates the largest possible “buffer zone” or margin between itself and the closest points from each class. These closest points, the ones right up against the edge of this buffer zone, are called the “support vectors” – they’re critical because they dictate exactly where the boundary and margin end up. This principle of maximizing the margin often helps the SVM generalize better, meaning it performs well not just on the data it was trained on, but also on new, unseen data. While the math behind it involves optimization, the core idea is finding this widest possible separation. We implement SVMs in our project using sklearn (“SVC,” n.d.).\nSVMs turn out to be perfect for the kind of EEG data we’re dealing with. EEG data, with signals from many electrodes, is naturally high-dimensional (204 dimensions in our case!). SVMs are known to handle high-dimensional spaces effectively, potentially better than some other algorithms that can suffer from the “curse of dimensionality.” They can also work well even when the number of training examples isn’t vastly larger than the number of features, which is relevant here. Plus, the focus on maximizing the margin can make the resulting classifier somewhat robust to noise, which is always a concern with EEG.\nAnd SVMs aren’t just for BCIs! They’re used in all sorts of areas, like classifying text (think spam email filters) (Mammone et al., 2009), recognizing faces in images (Guo et al., 2000), and even predicting things like seismic events (Hearst et al., 1998). So, it’s a versatile technology with broad applications, making its study valuable in many contexts."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "",
    "text": "Why is this so important? Well, advancements in BCIs could genuinely change lives for a vast number of people. For some perspective, the 2013 US Paralysis Prevalence & Health Disparities Survey found that nearly 5.4 million people were living with paralysis (Armour et al., 2016). Back then, that was almost 1.7% of the entire US population, and that number has likely only grown. Paralysis often brings profound challenges – the same survey noted that only about 15.5% of these individuals were employed, and over 30% were smokers. Given these severe impacts, technologies that can offer some relief or restore function are incredibly valuable for improving quality of life. BCIs stand out as one of the most promising paths forward to potentially restore movement capabilities and significantly enhance well-being for those affected."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "",
    "text": "Let’s start with the basics. Our brain is constantly sending out electrical signals that orchestrate everything our body does. But this communication network relies on many biological components, and if any part breaks down, the connection can be lost. For people experiencing this, a Brain-Computer Interface (BCI) offers an alternative route, or a way for the brain’s commands to bypass the damaged pathways and still control external devices or even their own limbs (Nicolas-Alonso & Gomez-Gil, 2012). Ultimately, a BCI acts as a translator, converting brain activity directly into control signals. To make this happen, we need two key things: a way to ‘listen’ to the brain’s signals, and a way to make sense of them. In this project, we’re focusing on non-invasive BCIs that use Electroencephalography (EEG) to pick up those signals.\nEEG works by placing sensors on the scalp to detect the tiny electrical fields generated by brain activity. While it has the major advantage of being non-invasive (no surgery required!), the signals we get face some challenges. They’re incredibly complex, often buried in noise (from muscle twitches, eye blinks, even electrical interference from nearby devices), and they’re weakened as they pass through the skull and scalp. Plus, to get a good spatial picture of brain activity, we use many electrodes simultaneously, which results in very high-dimensional data – meaning each snapshot of brain activity has lots of different measurements to consider. Trying to reliably pull out a specific intention, like “move left” versus “move right,” from this noisy, high-dimensional stream requires robust analysis tools. That’s where machine learning comes into play.\n\n\n\nOkay, so how do we actually interpret these complex brain signals? That’s where Support Vector Machines, or SVMs, come in. SVMs are a type of supervised machine learning model that are good at classification tasks – essentially, sorting data into predefined categories. You train an SVM by showing it examples that are already labeled (like EEG snippets labeled as “intended left” or “intended right”). The SVM learns the patterns distinguishing these categories. Then, when you give it a new, unlabeled piece of data (a new EEG segment), it uses what it learned to predict which category that new data point belongs to. In our case, the SVM’s job is to look at the features extracted from an EEG signal and decide: “left intention” or “right intention”?\nSo, how does an SVM actually do this? Conceptually, it tries to find the “best” possible boundary to separate the data points belonging to different classes. Imagine you have a scatter plot with red dots and green dots. If the red dots are mostly in one area and the green dots in another, an SVM tries to draw a line (or, in higher dimensions, a plane or hyperplane) that separates them.\n\n\n\n\n\n\nFigure 1: Figure 1: SVM\n\n\n\nSee the plot on the right (in the imagined figure)? That blue line is the decision boundary found by the SVM. What makes SVMs special is how they find this boundary. They don’t just pick any line that separates the groups; they specifically look for the line that creates the largest possible “buffer zone” or margin between itself and the closest points from each class. These closest points, the ones right up against the edge of this buffer zone, are called the “support vectors” – they’re critical because they dictate exactly where the boundary and margin end up. This principle of maximizing the margin often helps the SVM generalize better, meaning it performs well not just on the data it was trained on, but also on new, unseen data. While the math behind it involves optimization, the core idea is finding this widest possible separation. We implement SVMs in our project using sklearn (“SVC,” n.d.).\nSVMs turn out to be perfect for the kind of EEG data we’re dealing with. EEG data, with signals from many electrodes, is naturally high-dimensional (204 dimensions in our case!). SVMs are known to handle high-dimensional spaces effectively, potentially better than some other algorithms that can suffer from the “curse of dimensionality.” They can also work well even when the number of training examples isn’t vastly larger than the number of features, which is relevant here. Plus, the focus on maximizing the margin can make the resulting classifier somewhat robust to noise, which is always a concern with EEG.\nAnd SVMs aren’t just for BCIs! They’re used in all sorts of areas, like classifying text (think spam email filters) (Mammone et al., 2009), recognizing faces in images (Guo et al., 2000), and even predicting things like seismic events (Hearst et al., 1998). So, it’s a versatile technology with broad applications, making its study valuable in many contexts."
  },
  {
    "objectID": "index.html#data-acquisition",
    "href": "index.html#data-acquisition",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nThe foundation for everything is the EEG data. This data came from a single human subject.\n\nEEG Setup: The recording setup used 102 electrodes placed across the scalp – you can see the layout in the plot below (each blue dot is an electrode). Each electrode provided two pieces of information related to the local electrical field gradient (one for the x-direction, one for the y-direction). So, for every moment in time or trial we recorded, we ended up with 204 distinct data channels or features.\n\n\n\n\n\n\n\nFigure 2: Figure 2: Electrode Positioning\n\n\n\n\nExperimental Conditions: The data was collected under two different conditions:\n\nOvert Movement: In this case, the subject physically moved their left or right arm. We’d expect the brain signals during actual movement to be relatively strong and potentially easier to classify.\nImagined Movement: Here, the subject simply thought about moving their left or right arm, but stayed still. These signals are usually much fainter and harder to detect, but they’re crucial for BCIs intended for people who can’t physically move.\n\nData Structure: For both the Overt and Imagined conditions, we have a dataset containing 120 trials labeled “movement 1” and 120 trials labeled “movement 2”. Now, for this specific dataset, we don’t actually know which label corresponds to “left” and which to “right,” but for classification purposes, that doesn’t matter – we just need to distinguish between the two types. This gives us 240 trials in total for each condition (Overt and Imagined). Each single trial is represented by that 204-dimensional feature vector we talked about (the readings from all 204 data channels). Our task is essentially a binary classification problem: given a 204-dimensional vector, decide if it belongs to “movement 1” or “movement 2”."
  },
  {
    "objectID": "index.html#classification-with-support-vector-machines-svms",
    "href": "index.html#classification-with-support-vector-machines-svms",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.2 Classification with Support Vector Machines (SVMs)",
    "text": "2.2 Classification with Support Vector Machines (SVMs)\nNow, before we jump into the results, let’s look more precisely what the SVM is doing under the hood. Remember, its core job is to find that optimal boundary (a hyperplane in our 204-dimensional space) that best separates the EEG patterns corresponding to “Movement 1” from those corresponding to “Movement 2”.\n\n2.2.1 Mathematical Formulation\nIf we have \\(N\\) training trials, each with a feature vector \\(x_i\\) (our 204 EEG measurements) and a known class label \\(y_i\\) (which we’ll represent as either -1 for Movement 1 or +1 for Movement 2), a linear SVM tries to find a weight vector \\(w\\) and a bias term \\(b\\). These define the separating hyperplane with the equation \\(w^T x + b = 0\\).\nBut what if the data isn’t perfectly separable with a straight line (or flat plane)? What if some “Movement 1” points are mixed in with “Movement 2” points? That’s where the soft-margin SVM comes in handy. Instead of insisting on perfect separation, it tries to find a balance: maximize the margin (the buffer zone) while also minimizing the number of points that end up on the wrong side of the boundary or inside the margin. This is done by solving an optimization problem (Hearst et al., 1998):\n\\[\n\\min_{w, b, \\xi} \\frac{1}{2} w^T w + C \\sum_{i=1}^{N} \\xi_i\n\\]\nsubject to the constraints:\n\\[\ny_i (w^T x_i + b) \\ge 1 - \\xi_i, \\quad \\text{for } i = 1, \\dots, N\n\\] \\[\n\\xi_i \\ge 0, \\quad \\text{for } i = 1, \\dots, N\n\\]\nBreaking down these terms:\n\n\\(w\\): The weight vector. It’s perpendicular to the separating boundary. Minimizing its squared length (\\(\\frac{1}{2} w^T w\\)) is equivalent to maximizing the margin width.\n\\(b\\): The bias term. It shifts the boundary position without changing its orientation.\n\\(x_i\\): The 204-dimensional feature vector for the \\(i\\)-th trial.\n\\(y_i\\): The class label (+1 or -1) for the \\(i\\)-th trial.\n\\(\\xi_i\\): These are the slack variables. Think of them as measuring how much a data point \\(i\\) “violates” the margin. If \\(\\xi_i = 0\\), the point is correctly classified and outside the margin. If it’s between 0 and 1, it’s correctly classified but inside the margin. If \\(\\xi_i &gt; 1\\), the point is actually misclassified.\n\\(C\\): This is the regularization parameter. (It’s equivalent to \\(1/\\alpha\\) in our class notes, and is what sklearn uses.) \\(C\\) controls the trade-off:\n\nA large \\(C\\) puts a heavy penalty on misclassifications (large \\(\\xi_i\\)). The SVM will try very hard to classify all training points correctly, possibly leading to a narrower margin and potentially “overfitting” the training data (meaning it might not generalize well to new data).\nA small \\(C\\) is more tolerant of misclassifications. It allows for a potentially wider margin, even if some training points end up on the wrong side or within it. This might lead to better generalization but risks “underfitting” if the margin becomes too wide and ignores the underlying structure. Finding the right value for \\(C\\) (and potentially other parameters like \\(\\gamma\\) or \\(d\\) for non-linear kernels) is key to getting good performance, and we use a process called cross-validation (which we’ll discuss next) to do this.\n\n\nWhy this works (Convex Optimization):\nThis mathematical setup is what’s known as a convex optimization problem. This matters because for convex problems, we’re guaranteed that any solution we find that looks like the best locally is the globally best solution. There’s no risk of getting stuck in a suboptimal valley. The objective function (the part we’re minimizing) and the constraints (the rules we must follow) are all convex (they curve upwards like a bowl, mathematically speaking), which makes the whole problem convex (Hearst et al., 1998). This mathematical property is a big reason why SVMs are so reliable and widely used – we know we can find the single best boundary according to our criteria (at least for the linear case and standard formulations).\n\n\n2.2.2 Handling Non-Linearity: The Kernel Trick\nThe formulation we just looked at finds a linear boundary (a flat plane). But what if the real distinction between “Movement 1” and “Movement 2” in our EEG data follows a more complex, curved pattern? SVMs have a way to handle this known as the “kernel trick.”\nThe basic idea is to imagine mapping our original 204-dimensional data into an even higher-dimensional space (Weiße et al., 2006). In this new, much more complex space, the data might magically become linearly separable again. The “trick” is that we don’t actually have to compute the coordinates in this potentially massive new space. Instead, we use kernel functions \\(K(x_i, x_j)\\). These functions directly calculate what the dot product would be between the mapped vectors \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\) in that high-dimensional space (\\(K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\\)). Since the math behind the SVM solution relies heavily on these dot products, we can just swap out the simple dot product \\(x_i^T x_j\\) with our chosen kernel function \\(K(x_i, x_j)\\). This lets us effectively find a non-linear boundary in the original space without the excess computation of actually working in the super-high-dimensional mapped space.\nCommon Kernels: In this project, we explored three common kernel options:\n\nLinear Kernel: \\(K(x_i, x_j) = x_i^T x_j\\). This is our baseline, assuming a linear separation is sufficient.\nPolynomial Kernel: \\(K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d\\). This allows for curved boundaries defined by polynomials. It introduces new hyperparameters like the degree \\(d\\), a scaling factor \\(\\gamma\\), and a coefficient \\(r\\) that need tuning.\nRadial Basis Function (RBF) Kernel (or Gaussian Kernel): \\(K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)\\). This is a very popular kernel capable of creating highly complex decision boundaries. It uses a hyperparameter \\(\\gamma\\) that controls the “reach” or influence of each training point. Both \\(C\\) and \\(\\gamma\\) need to be tuned (Han et al., 2012)."
  },
  {
    "objectID": "index.html#model-evaluation-two-level-cross-validation",
    "href": "index.html#model-evaluation-two-level-cross-validation",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.3 Model Evaluation: Two-Level Cross-Validation",
    "text": "2.3 Model Evaluation: Two-Level Cross-Validation\nNow we have our SVM model variants (linear, RBF, polynomial). But how do we know which one is actually best for our problem? And how do we choose the optimal values for their hyperparameters (like \\(C\\) for all, \\(\\gamma\\) for RBF/Poly, \\(d\\) for Poly)? We need a reliable way to estimate how well each classifier configuration will perform on new, unseen data.\nAs discussed before, just tuning hyperparameters and evaluating on the same cross-validation splits can lead to inflated performance estimates. To avoid this and get a truly unbiased evaluation while also performing hyperparameter tuning, we rely on two-level cross-validation (Cross-Validation of Component Models, n.d.).\nThe Process:\n\nOuter Loop (Performance Estimation): Divides the data into \\(k_{outer}\\) folds (here, 6). Each fold serves once as the final, held-out test set.\nInner Loop (Hyperparameter Tuning): Operates only on the \\(k_{outer}-1\\) folds designated as the outer training set for that round. It performs its own cross-validation (using \\(k_{inner}=5\\) folds) to find the best hyperparameters (e.g., best combination of \\(C\\) and \\(\\gamma\\) for RBF, or \\(C\\), \\(\\gamma\\), \\(d\\), \\(r\\) for Poly) without seeing the outer test fold.\nTraining and Testing: Once the inner loop selects the best hyperparameters for that outer fold, a new SVM model is trained with those parameters on the entire outer training set. This model is then evaluated once on the held-out outer test set.\n\nThis entire process is repeated for all \\(k_{outer}\\) outer folds, and the final performance metrics (accuracy, AUC) are averaged across the results from these outer test sets. This rigorous approach ensures that our hyperparameter tuning doesn’t bias our final performance evaluation, giving us a fair comparison between the different kernel types and a reliable estimate of their true generalization ability. We applied this entire two-level CV procedure separately for the linear, RBF, and polynomial SVMs."
  },
  {
    "objectID": "index.html#classification-performance",
    "href": "index.html#classification-performance",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.1 Classification Performance",
    "text": "3.1 Classification Performance\n\n3.1.1 Same-Train Scenarios (Overt and Imagined)\nFirst, let’s see how the SVM performed when trained and tested on the same type of data.\n\nROC Curves: Figures 1 and 2 show the ROC curves. Each plot shows the performance for the 6 individual outer folds (the lighter lines) and the average ROC curve across all folds (the solid blue line).\n\n\n\n\n\n\nFigure 3: Figure 1: Individual Fold and Overall ROC Curves for Same-Train Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 4: Figure 2: Individual Fold and Overall ROC Curves for Same-Train Imagined (Linear SVM)\n\n\n\nInterpretation: Looking at these plots, especially the average curves and their AUC values, it’s pretty clear that the classifier performed better on the Overt dataset (Figure 1) than on the Imagined dataset (Figure 2). This makes intuitive sense. The brain signals generated during actual physical movement are generally stronger and more distinct, giving the classifier a clearer pattern to learn from. Imagined movement signals are fainter and potentially more variable, making the classification task inherently harder.\nCross-Validation Consistency Evaluation: It’s also interesting to look at the spread of the individual fold curves (the lighter lines). For the Overt data (Figure 1), the performance seems reasonably consistent across the different folds. However, for the Imagined data (Figure 2), there’s noticeably more variation between the folds. Some folds achieve quite good performance, while others lag behind. This suggests that classifying imagined movements might be more sensitive to the specific trials chosen for training and testing in any given fold. It really underscores why averaging across multiple folds using our two-level cross-validation approach is crucial for getting a reliable overall picture of performance.\nAccuracy: For a more direct quantitative comparison, let’s look at the accuracy – the simple proportion of trials classified correctly (using a standard decision threshold of 0). Tables 1 and 2 show the accuracy achieved on the outer test set for each of the 6 folds, along with the optimal regularization parameter (\\(C\\)) that was selected by the inner loop for that specific fold.\nTable 1: Per-Fold Accuracy Results for Same-Train Overt (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\nTime (s)\n\n\n\n\n1/6\n0.9500\n0.1\n0.1\n\n\n2/6\n0.9750\n1\n0.1\n\n\n3/6\n0.9750\n1\n0.1\n\n\n4/6\n0.9000\n0.001\n0.1\n\n\n5/6\n0.9750\n0.1\n0.1\n\n\n6/6\n0.9500\n0.1\n0.0\n\n\nAvg\n0.9542\n\n\n\n\nStd Dev\n0.0264\n\n\n\n\n\nTable 2: Per-Fold Accuracy Results for Same-Train Imagined (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\nTime (s)\n\n\n\n\n1/6\n0.8500\n0.1\n0.1\n\n\n2/6\n0.9500\n0.01\n0.1\n\n\n3/6\n0.8000\n0.01\n0.1\n\n\n4/6\n0.9250\n0.1\n0.1\n\n\n5/6\n0.8500\n0.1\n0.1\n\n\n6/6\n0.9500\n0.1\n0.1\n\n\nAvg\n0.8875\n\n\n\n\nStd Dev\n0.0580\n\n\n\n\n\nInterpretation: The average accuracy numbers confirm what we saw in the ROC curves: the classifier hit an average accuracy of about 95.4% on the Overt data, compared to about 88.8% on the Imagined data. These tables also clearly show the variability we discussed. Look at the Imagined data (Table 2) – accuracy ranges from a low of 80% in Fold 3 to a high of 95% in Folds 2 and 6. That’s a significant 15% swing! The standard deviation of the accuracy scores is also much higher for Imagined (0.0580) than for Overt (0.0264), numerically backing up our observation that performance was less consistent for the imagined movement task. This again points to the inherent difficulty of decoding these subtler brain signals.\n\n\n\n3.1.2 Cross-Train Scenarios\nNow, what happens if we train the classifier on one type of data but test it on the other? This is interesting because it tells us how well the patterns learned during, say, actual movement generalize to imagined movement, and vice-versa. For this analysis, we first found the single best hyperparameter \\(C\\) by running a simple 5-fold cross-validation within the entire training dataset (e.g., all 240 Overt trials). Then, we trained one final SVM using that best \\(C\\) on all the training data and tested it on all the data from the other condition (e.g., all 240 Imagined trials). Figures 3 and 4 show the ROC curves for these two cross-training situations.\n\n\n\n\n\n\nFigure 5: Figure 3: ROC Curve for Cross-Training: Train Imagined -&gt; Test Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 6: Figure 4: ROC Curve for Cross-Training: Train Overt -&gt; Test Imagined (Linear SVM)\n\n\n\nComparison: When we compare these two scenarios, we see an interesting pattern. One might initially expect that training on the cleaner, stronger Overt data would lead to a model that generalizes better when tested on the noisier Imagined data. Conversely, training on the noisy Imagined data might result in a model that struggles with the different characteristics of Overt data. However, our results actually showed the opposite, if only slightly! * Training on Imagined data and testing on Overt data (Figure 3) yielded an accuracy of about 91.3% (AUC = 0.97). * Training on Overt data and testing on Imagined data (Figure 4) yielded an accuracy of about 89.2% (AUC = 0.96). So, somewhat counterintuitively, the model trained on imagined movement performed slightly better when predicting overt movement intentions than the other way around. Importantly, both cross-training accuracies fall between the same-train accuracies we saw earlier (which were ~95.4% for Overt and ~88.8% for Imagined). This suggests there’s definitely some shared underlying pattern between overt and imagined movement signals that the SVM can pick up on, allowing for some degree of generalization. However, the fact that neither cross-trained model reached the performance of the same-train Overt model indicates that there are still distinct differences between the two signal types that make perfect transfer difficult.\n\n\n3.1.3 Regularization Parameter Comparison\nLet’s briefly look back at the optimal \\(C\\) values selected by the inner cross-validation loops in our same-train scenarios (Tables 1 and 2). These values reflect the best trade-off found between maximizing the margin and minimizing classification errors on the training data for each fold.\nObservation: * Optimal \\(C\\) values chosen for Overt folds: [0.1, 1, 1, 0.001, 0.1, 0.1] * Optimal \\(C\\) values chosen for Imagined folds: [0.1, 0.01, 0.01, 0.1, 0.1, 0.1] Interpretation: While \\(C=0.1\\) was the most frequent choice for both datasets, there are slight differences. The Overt data saw \\(C=1\\) selected twice and even \\(C=0.001\\) once, covering a wider range. The Imagined data twice selected \\(C=0.01\\). There isn’t a dramatic difference, but the slightly more frequent appearance of smaller \\(C\\) values (0.01) for the Imagined data might hint that a slightly wider margin (more tolerance for errors) was sometimes preferred, perhaps due to the data being noisier or less clearly separable. Conversely, the selection of \\(C=1\\) for some Overt folds could suggest that the data in those specific folds was separable enough to allow for a tighter fit to the training points without compromising the margin too much. However, with \\(C=0.1\\) being dominant in both, the overall preference seems fairly similar across the two conditions in this specific experiment."
  },
  {
    "objectID": "index.html#feature-channel-importance-svm-weights",
    "href": "index.html#feature-channel-importance-svm-weights",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.2 Feature (Channel) Importance (SVM Weights)",
    "text": "3.2 Feature (Channel) Importance (SVM Weights)\nOne of the nice things about linear SVMs is that the learned weight vector \\(w\\) gives us direct insight into which features (in our case, which EEG channels) were most important for making the classification decision. A larger absolute weight \\(|w_j|\\) for channel \\(j\\) means that channel had a bigger influence on separating Movement 1 from Movement 2. The sign of the weight tells us the direction of influence (positive weights push towards one class, negative towards the other).\n\nSpatial Visualization: To get a feel for where on the head the important signals were coming from, we can visualize the magnitude of the weights. For each of the 102 electrode locations, we calculated the combined magnitude of the weights for its corresponding x and y gradient channels (\\(\\sqrt{w_{x}^2 + w_{y}^2}\\)). Figures 5 and 7 plot these magnitudes on a top-down view of the scalp for a representative fold (Fold 1). Brighter colors indicate higher importance.\n\n\n\n\n\n\nFigure 7: Figure 5: SVM Weight Magnitude on Brain Surface (Overt Data, Fold 1, Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 8: Figure 7: SVM Weight Magnitude on Brain Surface (Imagined Data, Fold 1, Linear SVM)\n\n\n\nInterpretation: Looking at these heatmaps, we can see that the general areas highlighted as important are quite similar between the Overt (Fig 5) and Imagined (Fig 7) conditions. The most influential channels seem to be located over central and parietal regions of the scalp. This aligns well with our understanding of neuroscience, as these areas, particularly the motor cortex located centrally, are known to be heavily involved in planning and executing movements. It’s reassuring that the SVM seems to be focusing on brain regions relevant to the task. The patterns aren’t identical, which might reflect subtle differences in neural activity between actual and imagined movement, but the overall spatial focus is broadly consistent.\nStem Plot and Dominant Channels: For a more detailed view, Figures 6 and 8 show stem plots of the signed weight for every single one of the 204 channels. We’ve also highlighted the 6 channels with the largest absolute weights in each case.\n\n\n\n\n\n\nFigure 9: Figure 6: Signed SVM Weights per Channel (Overt Data, Fold 1, Linear SVM), Top 6 Highlighted\n\n\n\n\n\n\n\n\n\nFigure 10: Figure 8: Signed SVM Weights per Channel (Imagined Data, Fold 1, Linear SVM), Top 6 Highlighted\n\n\n\nInterpretation: These plots show that while many channels have weights very close to zero (meaning they contribute little to the decision), a subset of channels carries significant weight. Let’s list the top 6 for Fold 1: * Top 6 Overt (Fold 1): Channel 140 (+0.24), Channel 136 (-0.22), Channel 120 (+0.2), Channel 144 (-0.2), Channel 158 (+0.2), Channel 128 (-0.19) * Top 6 Imagined (Fold 1): Channel 151 (+0.41), Channel 136 (-0.36), Channel 140 (+0.35), Channel 153 (+0.35), Channel 154 (+0.31), Channel 128 (-0.3) Comparing these lists, we see some interesting overlaps and differences. Channels 128, 136, and 140 appear in the top 6 for both Overt and Imagined data, and notably, they have the same sign (direction of influence) in both lists (128 and 136 are negative, 140 is positive). This strongly suggests these channels are capturing activity patterns crucial for distinguishing the two movement intentions regardless of whether the movement is executed or imagined. The other top channels differ, potentially highlighting signals that are more specific to either the overt execution or the internal simulation of movement. The generally larger magnitude of weights in the Imagined list might seem counterintuitive, but could reflect the model needing to rely more heavily on fewer, perhaps more subtle, distinguishing features in the weaker imagined signal data. Overall, this weight analysis confirms that the SVM is learning neurophysiologically plausible patterns."
  },
  {
    "objectID": "index.html#classification-performance-1",
    "href": "index.html#classification-performance-1",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "4.1 Classification Performance",
    "text": "4.1 Classification Performance\n\n4.1.1 Same-Train Scenarios (Overt and Imagined)\nWe first evaluate the classifier’s performance using two-level cross-validation within each dataset independently.\n\nROC Curves: The Receiver Operating Characteristic (ROC) curves visualize the trade-off between the true positive rate (correctly identifying one class) and the false positive rate (incorrectly identifying the other class) across different decision thresholds. Figure 1 shows the ROC curves for the Overt dataset, and Figure 2 shows them for the Imagined dataset. Each plot displays the ROC curve for each of the 6 outer cross-validation folds (lighter lines) and the average ROC curve across all folds (darker blue line).\n\n\n\n\n\n\nFigure 1: Figure 1: Individual Fold and Overall ROC Curves for Same-Train Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 2: Figure 2: Individual Fold and Overall ROC Curves for Same-Train Imagined (Linear SVM)\n\n\n\nInterpretation: As we can see from the plots and the AUC (area under curve), the classifier generally performs better on the overt dataset (Avg AUC in Fig. 1) compared to the Imagined dataset (Avg AUC in Fig. 2). This aligns with our expectation that the stronger neural signals produced during actual limb movement provide a clearer basis for classification compared to the fainter signals associated with imagined movement. When signals are stronger, the classifier has more distinct information to work with, leading to better separation between classes.\nCross-Validation Consistency Evaluation: Looking at the individual fold ROC curves show variability in performance across different subsets of the data. While the overt data (Fig. 1) shows relatively consistent performance across folds, the imagined data (Fig. 2) shows more pronounced variance between folds. This suggests that classification on the imagined data might be more sensitive to the specific trials included in the training/testing splits. This shows the importance of averaging performance across multiple folds, as done in two-level CV, to get a more reliable estimate of the classifier’s true performance.\nAccuracy: We measure accuracy here as the proportion of correctly classified trials assuming a decision threshold of 0 on the SVM’s output score, which provides an easy quantitative way to compare performance. Tables 1 and 2 show the accuracy achieved on the test set of each outer fold, along with the optimal regularization parameter \\(C\\) selected by the inner cross-validation loop for that fold.\nTable 1: Per-Fold Accuracy Results for Same-Train Overt (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\nTime (s)\n\n\n\n\n1/6\n0.9500\n0.1\n0.1\n\n\n2/6\n0.9750\n1\n0.1\n\n\n3/6\n0.9750\n1\n0.1\n\n\n4/6\n0.9000\n0.001\n0.1\n\n\n5/6\n0.9750\n0.1\n0.1\n\n\n6/6\n0.9500\n0.1\n0.0\n\n\nAvg\n0.9542\n\n\n\n\nStd Dev\n0.0264\n\n\n\n\n\nTable 2: Per-Fold Accuracy Results for Same-Train Imagined (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\nTime (s)\n\n\n\n\n1/6\n0.8500\n0.1\n0.1\n\n\n2/6\n0.9500\n0.01\n0.1\n\n\n3/6\n0.8000\n0.01\n0.1\n\n\n4/6\n0.9250\n0.1\n0.1\n\n\n5/6\n0.8500\n0.1\n0.1\n\n\n6/6\n0.9500\n0.1\n0.1\n\n\nAvg\n0.8875\n\n\n\n\nStd Dev\n0.0580\n\n\n\n\n\nInterpretation: The average accuracy results quantitatively confirm the trend observed in the ROC curves: the classifier achieves higher average accuracy on the overt data (95.4%) compared to the imagined data (88.8%). The tables also numerically illustrate the fold-to-fold variability discussed earlier, especially for the imagined data where the accuracy ranges from 80% to 95% across folds – a 15% difference. The standard deviation of accuracy across folds is also higher for the imagined data (0.0580) than for the overt data (0.0264), further supporting the observation of greater performance variability. This reinforces the conclusion that classifying imagined movement from EEG is inherently more challenging due to weaker or less distinct signal patterns.\n\n\n\n4.1.2 Cross-Train Scenarios\nWe now investigate how well a classifier trained on one type of data (overt or imagined) generalizes to the other type. For these scenarios, we first determine the best hyperparameter \\(C\\) using 5-fold cross-validation within the entire training dataset (e.g., all 240 Overt trials). Then, we train a single linear SVM using this best \\(C\\) on the entire training dataset and evaluate its performance on the entire dataset of the other type (e.g., all 240 Imagined trials). The resulting ROC curves are shown in Figures 3 and 4.\n\n\n\n\n\n\nFigure 3: Figure 3: ROC Curve for Cross-Training: Train Imagined -&gt; Test Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 4: Figure 4: ROC Curve for Cross-Training: Train Overt -&gt; Test Imagined (Linear SVM)\n\n\n\nComparison: Comparing the two cross-training scenarios reveals differences in generalization performance. ** Generally, I would expect that training on the cleaner, stronger overt data results in a model that performs better on the imagined data than vice versa. Training on the imagined data may cause our model to become noisy and unable to perform as well on the cleaner overt data. However, in practice, we did not observe this - in fact, the model performed slightly better when trained on imagined and tested on overt. In addition, performance seemed to lie in between the performance of same-train, which was surprising. I would have assumed that, due to the methods of data collection being equal, same-train would produce a higher test accuracy simply due to the fact that all the data was gathered in the same way. However, when training on overt and testing on imagined, we noticed an accuracy of 0.8917, while training on imagined and testing on overt yielded an accuracy of 0.9125. Both of these values are lower than the average accuracy for same-train overt, but higher than same-train imagined. This suggests that the inclusion of any overt data improves performance, and may be a good sign for transferability between the two kinds of data.\n\n\n4.1.3 Regularization Parameter Comparison\nThe optimal regularization parameter \\(C\\) selected during the inner cross-validation loops (shown in Tables 1 and 2) reflects the balance struck between maximizing the margin and minimizing training errors for each fold. Comparing the typical values of \\(C_{best}\\) across the two datasets can offer insights[cite: 166].\nObservation: Examining Tables 1 and 2, we observe the range and frequency of optimal \\(C\\) values chosen for each dataset. * Overt Data Optimal Cs: [0.1, 1, 1, 0.001, 0.1, 0.1] * Imagined Data Optimal Cs: [0.1, 0.01, 0.01, 0.1, 0.1, 0.1] Interpretation: (User should refine this interpretation based on the observed values). In this case, both datasets predominantly favor \\(C=0.1\\). However, the Overt data also selected \\(C=1\\) twice and \\(C=0.001\\) once, while the Imagined data selected \\(C=0.01\\) twice. There isn’t a dramatically clear trend, but perhaps the slightly more frequent selection of smaller \\(C\\) values (0.01) for the Imagined data could suggest a slightly higher tolerance for misclassification was needed, potentially due to noisier or less separable data requiring a wider margin, though \\(C=0.1\\) remains dominant for both. A larger \\(C\\) (like \\(C=1\\) seen in Overt) suggests the data for those folds might have been more separable, allowing for a tighter fit to the training data without significantly sacrificing the margin."
  },
  {
    "objectID": "index.html#feature-channel-importance-svm-weights-1",
    "href": "index.html#feature-channel-importance-svm-weights-1",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "4.2 Feature (Channel) Importance (SVM Weights)",
    "text": "4.2 Feature (Channel) Importance (SVM Weights)\nFor linear SVMs, the learned weight vector \\(w\\) provides insights into the importance of each feature (EEG channel) for the classification decision. The magnitude of the weight \\(w_j\\) associated with feature \\(j\\) indicates its influence: features with larger absolute weights contribute more significantly to determining the position of the separating hyperplane and thus the classification outcome. Positive weights push the decision towards one class, while negative weights push it towards the other.\nWe analyze the weights obtained from the model trained on the first outer fold (\\(Fold\\ \\#1\\)) as a representative example for both the Overt and Imagined datasets.\n\nSpatial Visualization: Figures 5 and 7 visualize the magnitude of the SVM weights mapped onto the scalp locations corresponding to the 102 electrodes. The magnitude at each electrode location is calculated as the Euclidean norm of the weights for the x and y gradient channels associated with that electrode (\\(\\sqrt{w_{x}^2 + w_{y}^2}\\)). This helps identify brain regions whose associated channels were deemed important by the classifier[cite: 159, 160].\n\n\n\n\n\n\nFigure 5: Figure 5: SVM Weight Magnitude on Brain Surface (Overt Data, Fold 1, Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 6: Figure 7: SVM Weight Magnitude on Brain Surface (Imagined Data, Fold 1, Linear SVM)\n\n\n\nInterpretation: Looking at the magnitude of the weights as represented by a colormap, Observe the spatial distribution of high-magnitude weights in Figures 5 and 7. Are there specific areas (e.g., over motor cortex regions, central, parietal areas) that consistently show higher importance? Compare the patterns between the Overt (Fig. 5) and Imagined (Fig. 7) conditions. Are the important regions similar or different? Differences could reflect distinct neural processes involved in actual versus imagined movement.\nStem Plot and Dominant Channels: Figures 6 and 8 provide stem plots showing the signed weight for each of the 204 channels for Fold 1[cite: 160, 161]. The channels with the 6 largest absolute weights (top 6 dominant channels) are highlighted[cite: 160, 161].\n\n\n\n\n\n\nFigure 7: Figure 6: Signed SVM Weights per Channel (Overt Data, Fold 1, Linear SVM), Top 6 Highlighted\n\n\n\n\n\n\n\n\n\nFigure 8: Figure 8: Signed SVM Weights per Channel (Imagined Data, Fold 1, Linear SVM), Top 6 Highlighted\n\n\n\nInterpretation: The stem plots show the distribution and sign of weights across all channels. * (User should list the top 6 channels and their signed weights from their* specific plots)  Top 6 Overt (Fold 1): Channel [Index]: [Weight], Channel [Index]: [Weight], …, Channel [Index]: [Weight] * Top 6 Imagined (Fold 1): Channel [Index]: [Weight], Channel [Index]: [Weight], …, Channel [Index]: [Weight] * Compare the overall distribution of weights (e.g., are many weights near zero, or are there many influential channels?) between Overt (Fig. 6) and Imagined (Fig. 8). Compare the identity and weights of the top 6 channels – are they similar or different between the two conditions? This provides a more granular view of which specific measurements were most discriminative."
  },
  {
    "objectID": "index.html#formulating-the-classification-problem",
    "href": "index.html#formulating-the-classification-problem",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.4 Formulating the Classification Problem",
    "text": "2.4 Formulating the Classification Problem\nExplicitly defining the machine learning problem we’re solving here:\n\nInput Features: For each trial (either an overt or imagined movement), the input to our classifier is a single vector \\(x \\in \\mathbb{R}^{204}\\). This vector represents the measurements from the 102 electrodes, with each electrode contributing two gradient values (x and y direction), resulting in 204 features per trial. These features are assumed to capture the spatial and electrical characteristics of the brain activity during that trial.\nTarget Classes: The output we want to predict is a binary class label, \\(y \\in \\{\\text{\"Movement 1\"}, \\text{\"Movement 2\"}\\}\\). As mentioned, the dataset labels these generically, corresponding to the intention to move either the left or right hand (mapping unknown). Our goal is to distinguish between these two distinct intentions based on the EEG features.\nObjective: The objective is to learn a classification function \\(f: \\mathbb{R}^{204} \\rightarrow \\{\\text{\"Movement 1\"}, \\text{\"Movement 2\"}\\}\\) using a Support Vector Machine. We aim to find the function \\(f\\) (defined by the SVM’s parameters, including the kernel type and its associated hyperparameters like \\(C\\), \\(\\gamma\\), \\(d\\)) that minimizes classification errors on unseen data. The two-level cross-validation procedure is our method for estimating this generalization performance and for selecting the optimal hyperparameters for each kernel type based on the training data."
  },
  {
    "objectID": "index.html#linear-kernel-classification-performance",
    "href": "index.html#linear-kernel-classification-performance",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.1 Linear Kernel Classification Performance",
    "text": "3.1 Linear Kernel Classification Performance\nWe’ll start with the baseline linear SVM.\n\n3.1.1 Same-Train Scenarios (Linear SVM)\n\nROC Curves: Figures 1 and 2 show the ROC curves for the linear SVM when trained and tested on Overt and Imagined data, respectively.\n\n\n\n\n\n\nFigure 3: Figure 1: Individual Fold and Overall ROC Curves for Same-Train Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 4: Figure 2: Individual Fold and Overall ROC Curves for Same-Train Imagined (Linear SVM)\n\n\n\nInterpretation: As previously discussed, the linear SVM performed better on Overt data (Avg AUC in Fig. 1) than Imagined data (Avg AUC in Fig. 2), likely due to stronger signals. Performance was reasonably consistent across folds for Overt, but more variable for Imagined.\nAccuracy: Tables 1 and 2 show the fold-by-fold accuracy and optimal \\(C\\) values.\nTable 1: Per-Fold Accuracy Results for Same-Train Overt (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.9500\n0.1\n\n\n2/6\n0.9750\n1\n\n\n3/6\n0.9750\n1\n\n\n4/6\n0.9000\n0.001\n\n\n5/6\n0.9750\n0.1\n\n\n6/6\n0.9500\n0.1\n\n\nAvg\n0.9542\n\n\n\nStd Dev\n0.0264\n\n\n\n\nTable 2: Per-Fold Accuracy Results for Same-Train Imagined (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.8500\n0.1\n\n\n2/6\n0.9500\n0.01\n\n\n3/6\n0.8000\n0.01\n\n\n4/6\n0.9250\n0.1\n\n\n5/6\n0.8500\n0.1\n\n\n6/6\n0.9500\n0.1\n\n\nAvg\n0.8875\n\n\n\nStd Dev\n0.0580\n\n\n\n\nInterpretation: Average accuracy was ~95.4% for Overt and ~88.8% for Imagined, confirming the ROC trend. Variability across folds was higher for Imagined data.\n\n\n\n3.1.2 Cross-Train Scenarios (Linear SVM)\n\nROC Curves: Figures 3 and 4 show the ROC curves for the cross-training scenarios using the linear SVM.\n\n\n\n\n\n\nFigure 5: Figure 3: ROC Curve for Cross-Training: Train Imagined -&gt; Test Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 6: Figure 4: ROC Curve for Cross-Training: Train Overt -&gt; Test Imagined (Linear SVM)\n\n\n\nComparison: As noted before, training on Imagined and testing on Overt (Fig 3, ~91.3% Acc, AUC 0.97) performed slightly better than training on Overt and testing on Imagined (Fig 4, ~89.2% Acc, AUC 0.96). Both fell between the same-train performances, suggesting some shared patterns but also distinct differences between the conditions.\n\n\n\n3.1.3 Feature Importance (Linear SVM Weights)\n\nSpatial and Stem Plots: Figures 5-8 visualize the weights for the linear SVM.\n   \nInterpretation: The weights highlighted channels primarily over sensorimotor areas, consistent with neuroscience. The primary motor cortex lies in the middle of the brain, which is the most heavily weighted part. There was significant overlap in the most important channels (e.g., 128, 136, 140) between Overt and Imagined conditions, supporting the idea of shared neural substrates."
  },
  {
    "objectID": "index.html#rbf-kernel-classification-performance",
    "href": "index.html#rbf-kernel-classification-performance",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.2 RBF Kernel Classification Performance",
    "text": "3.2 RBF Kernel Classification Performance\nNow let’s consider the performance when using the Radial Basis Function (RBF) kernel, which allows for more complex, non-linear decision boundaries. The hyperparameters \\(C\\) and \\(\\gamma\\) were tuned simultaneously in the inner loop of the two-level CV.\n\n3.2.1 Same-Train Scenarios (RBF SVM)\n\nROC Curves: Let’s assume Figures 9 and 10 show the ROC curves for the RBF SVM on Overt and Imagined data, respectively.\n \nInterpretation: The average AUC for the Overt data (Figure 9) is approximately 0.99, suggesting excellent performance, slightly higher than the linear kernel’s average. For the Imagined data (Figure 10), the average AUC is around 0.94, which is also higher than the linear kernel’s average (~0.92 based on visual inspection of Fig 2), indicating the RBF kernel might be capturing useful non-linearities, especially in the more challenging Imagined condition. Variability between folds still seems present, particularly for Imagined data.\nAccuracy: Similarly, assume Tables 3 and 4 present the accuracy results and the optimal \\((C, \\gamma)\\) pairs chosen for each fold.\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.9750\n1\n\n\n2/6\n0.9250\n10\n\n\n3/6\n0.9250\n10\n\n\n4/6\n0.9250\n10\n\n\n5/6\n0.9750\n10\n\n\n6/6\n0.8750\n10\n\n\nAvg\n0.9333\n\n\n\nStd Dev\n0.0354\n\n\n\n\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.8500\n10\n\n\n2/6\n0.9250\n1\n\n\n3/6\n0.8000\n1\n\n\n4/6\n0.8500\n1\n\n\n5/6\n0.8250\n10\n\n\n6/6\n0.8750\n10\n\n\nAvg\n0.8542\n\n\n\nStd Dev\n0.0417\n\n\n\n\nInterpretation: Comparing the average accuracies, the RBF kernel achieved ~93.3% on Overt data (Table 3) and ~85.4% on Imagined data (Table 4). Interestingly, these average accuracies are slightly lower than those achieved by the linear kernel (95.4% Overt, 88.8% Imagined). This contrasts with the AUC results and suggests that while the RBF kernel might offer better separation overall (higher AUC), the optimal decision threshold for maximizing accuracy might be different, or the added complexity might slightly hurt performance at the standard threshold, perhaps due to overfitting on some folds despite cross-validation. The optimal \\(C\\) values tend to be higher (1 or 10) than often seen for the linear kernel, potentially indicating the need for a tighter fit when using the flexible RBF kernel.\n\n\n\n3.2.2 Feature Importance (RBF SVM)\n \n\nWeight Interpretation: Visualizing feature importance for non-linear kernels like RBF isn’t as straightforward as plotting the weights (\\(w\\)) in the linear case. With a linear SVM, the weights directly correspond to the importance of a feature, but the same association doesn’t exist with different kernels. Therefore, we look at the performance metrics and also use approximations to map weights back to support vectors. However, these heatmaps still show a general distribution of “weights” in the same locations as the linear SVM."
  },
  {
    "objectID": "index.html#polynomial-kernel-classification-performance",
    "href": "index.html#polynomial-kernel-classification-performance",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.3 Polynomial Kernel Classification Performance",
    "text": "3.3 Polynomial Kernel Classification Performance\nFinally, let’s consider the Polynomial kernel. This kernel also allows for non-linear boundaries, with complexity controlled by the polynomial degree \\(d\\) (along with hyperparameters \\(C\\), \\(\\gamma\\), and \\(r\\), all tuned via the inner CV loop).\n\n3.3.1 Same-Train Scenarios (Polynomial SVM)\n\nROC Curves: Assume Figures 13 and 14 show the ROC curves for the Polynomial SVM on Overt and Imagined data.\n \nInterpretation: The average AUC for the Overt data (Figure 13) is approximately 0.98, slightly below the RBF kernel but comparable to the linear kernel. However, for the Imagined data (Figure 14), the average AUC drops significantly to around 0.89, which is much worse than both the linear (~0.92) and RBF (~0.94) kernels. This suggests the polynomial kernel might be struggling or potentially overfitting on the more difficult Imagined dataset.\nAccuracy: Assume Tables 5 and 6 show the accuracy results and the optimal hyperparameter sets (\\(C, \\gamma, d, r\\)) for each fold.\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n1.0000\n10\n\n\n2/6\n0.9500\n1\n\n\n3/6\n0.9500\n100\n\n\n4/6\n0.9250\n1\n\n\n5/6\n0.9500\n1\n\n\n6/6\n0.9250\n1\n\n\nAvg\n0.9508\n\n\n\nStd Dev\n0.0252\n\n\n\n\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.8250\n10\n\n\n2/6\n0.8750\n10\n\n\n3/6\n0.7750\n10\n\n\n4/6\n0.7750\n1\n\n\n5/6\n0.8250\n10\n\n\n6/6\n0.8250\n10\n\n\nAvg\n0.8167\n\n\n\nStd Dev\n0.0366\n\n\n\n\nInterpretation: The average accuracy for the Polynomial kernel is ~95.1% on Overt data (Table 5), which is very close to the linear kernel’s performance. However, on the Imagined data (Table 6), the average accuracy drops to ~81.7%, noticeably lower than both the linear (~88.8%) and RBF (~85.4%) kernels. This confirms the trend seen in the AUCs – the polynomial kernel seems less effective for the imagined movement data in this experiment. Optimal \\(C\\) values varied widely, including a large value of 100 in one Overt fold.\n \nInterpretation: Similar as all the other heatmaps, we see the weights concentrated in the motor cortices."
  }
]