[
  {
    "objectID": "notebooks/Untitled.html",
    "href": "notebooks/Untitled.html",
    "title": "BCI Movement Decoding",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\n# Suppress ConvergenceWarning from SVC\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\ndef load_and_prepare_data(file1, file2, label1=0, label2=1):\n    \"\"\"Loads two CSV files, assigns labels, and combines them.\"\"\"\n    df1 = pd.read_csv(file1, header=None)\n    df2 = pd.read_csv(file2, header=None)\n\n    # Assign labels\n    y1 = np.full(df1.shape[0], label1)\n    y2 = np.full(df2.shape[0], label2)\n\n    # Combine features and labels\n    X = pd.concat([df1, df2], ignore_index=True).values\n    y = np.concatenate([y1, y2])\n\n    return X, y\n\ndef perform_two_level_cv(X, y, outer_k=6, inner_k=5, C_range=None):\n    \"\"\"Performs two-level cross-validation for SVM hyperparameter tuning.\"\"\"\n    if C_range is None:\n        # Define the search space for the regularization parameter C\n        C_range = np.logspace(-3, 3, 7) # [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\n    outer_cv = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=42)\n    inner_cv = StratifiedKFold(n_splits=inner_k, shuffle=True, random_state=42)\n\n    outer_fold_accuracies = []\n    optimal_Cs = []\n\n    print(f\"\\nStarting {outer_k}-fold outer cross-validation...\")\n\n    # Outer loop for model evaluation\n    for i, (train_outer_idx, test_outer_idx) in enumerate(outer_cv.split(X, y)):\n        print(f\"  Outer Fold {i+1}/{outer_k}\")\n        X_train_outer, X_test_outer = X[train_outer_idx], X[test_outer_idx]\n        y_train_outer, y_test_outer = y[train_outer_idx], y[test_outer_idx]\n\n        # --- Data Scaling (Outer Fold) ---\n        scaler = StandardScaler()\n        X_train_outer_scaled = scaler.fit_transform(X_train_outer)\n        X_test_outer_scaled = scaler.transform(X_test_outer) # Use same scaler fit on training data\n\n        inner_loop_results = {} # {C: [fold1_acc, fold2_acc, ...]}\n\n        # Inner loop for hyperparameter tuning (finding best C)\n        print(f\"    Starting {inner_k}-fold inner cross-validation for hyperparameter tuning...\")\n        for C_val in C_range:\n            inner_fold_accuracies = []\n            for j, (train_inner_idx, val_inner_idx) in enumerate(inner_cv.split(X_train_outer_scaled, y_train_outer)):\n                # print(f\"      Inner Fold {j+1}/{inner_k} for C={C_val}\") # Uncomment for very detailed logging\n                X_train_inner, X_val_inner = X_train_outer_scaled[train_inner_idx], X_train_outer_scaled[val_inner_idx]\n                y_train_inner, y_val_inner = y_train_outer[train_inner_idx], y_train_outer[val_inner_idx]\n\n                # Train SVM with current C on inner training fold\n                svm_inner = SVC(kernel='linear', C=C_val, random_state=42, probability=False) # Probability=False can speed up\n                svm_inner.fit(X_train_inner, y_train_inner)\n\n                # Evaluate on inner validation fold\n                y_pred_inner = svm_inner.predict(X_val_inner)\n                accuracy = accuracy_score(y_val_inner, y_pred_inner)\n                inner_fold_accuracies.append(accuracy)\n\n            # Store average accuracy for this C value across inner folds\n            inner_loop_results[C_val] = np.mean(inner_fold_accuracies)\n            # print(f\"      Average Inner Accuracy for C={C_val:.3f}: {inner_loop_results[C_val]:.4f}\") # Uncomment for detail\n\n        # Find the best C based on inner loop average accuracy\n        best_C = max(inner_loop_results, key=inner_loop_results.get)\n        optimal_Cs.append(best_C)\n        print(f\"    Optimal C found for Outer Fold {i+1}: {best_C} (Avg Inner Acc: {inner_loop_results[best_C]:.4f})\")\n\n        # Train the final SVM for this outer fold using the best C on the *entire* outer training set\n        print(f\"    Training final SVM for Outer Fold {i+1} with C={best_C}...\")\n        final_svm = SVC(kernel='linear', C=best_C, random_state=42)\n        final_svm.fit(X_train_outer_scaled, y_train_outer)\n\n        # Evaluate the final SVM on the outer test set\n        y_pred_outer = final_svm.predict(X_test_outer_scaled)\n        outer_accuracy = accuracy_score(y_test_outer, y_pred_outer)\n        outer_fold_accuracies.append(outer_accuracy)\n        print(f\"  Outer Fold {i+1}/{outer_k} Accuracy: {outer_accuracy:.4f}\")\n\n    average_accuracy = np.mean(outer_fold_accuracies)\n    std_dev_accuracy = np.std(outer_fold_accuracies)\n\n    print(f\"\\nFinished cross-validation.\")\n    print(f\"Optimal C for each outer fold: {optimal_Cs}\")\n    print(f\"Accuracy for each outer fold: {[f'{acc:.4f}' for acc in outer_fold_accuracies]}\")\n    print(f\"Average Accuracy across all outer folds: {average_accuracy:.4f}\")\n    print(f\"Standard Deviation of Accuracy across outer folds: {std_dev_accuracy:.4f}\")\n\n    return average_accuracy, std_dev_accuracy, optimal_Cs, outer_fold_accuracies\n\n\n# --- Main Execution ---\n\n# Define file paths (assuming they are in the current directory or accessible)\nimg_file1 = '../../feaSubEImg_1.csv'\nimg_file2 = '../../feaSubEImg_2.csv'\novert_file1 = '../../feaSubEOvert_1.csv'\novert_file2 = '../../feaSubEOvert_2.csv'\n\n# Load data\nprint(\"Loading Imagined Movement Data...\")\nX_img, y_img = load_and_prepare_data(img_file1, img_file2)\nprint(f\"Imagined Data Shape: X={X_img.shape}, y={y_img.shape}\")\nprint(f\"Imagined Data Labels: {np.unique(y_img, return_counts=True)}\")\n\n\nprint(\"\\nLoading Overt Movement Data...\")\nX_overt, y_overt = load_and_prepare_data(overt_file1, overt_file2)\nprint(f\"Overt Data Shape: X={X_overt.shape}, y={y_overt.shape}\")\nprint(f\"Overt Data Labels: {np.unique(y_overt, return_counts=True)}\")\n\n\n# --- Run Two-Level CV for Imagined Data ---\nprint(\"\\n--- Starting Analysis for Imagined Movement Data ---\")\nimg_avg_acc, img_std_acc, img_opt_C, img_fold_accs = perform_two_level_cv(X_img, y_img)\n\n\n# --- Run Two-Level CV for Overt Data ---\nprint(\"\\n--- Starting Analysis for Overt Movement Data ---\")\novert_avg_acc, overt_std_acc, overt_opt_C, overt_fold_accs = perform_two_level_cv(X_overt, y_overt)\n\n\n# --- Final Summary ---\nprint(\"\\n\\n--- Results Summary ---\")\nprint(f\"Imagined Movement Data:\")\nprint(f\"  - Average Accuracy: {img_avg_acc:.4f}\")\nprint(f\"  - Std Dev Accuracy: {img_std_acc:.4f}\")\nprint(f\"  - Optimal C per fold: {img_opt_C}\")\nprint(f\"  - Fold Accuracies: {[f'{acc:.4f}' for acc in img_fold_accs]}\")\n\nprint(f\"\\nOvert Movement Data:\")\nprint(f\"  - Average Accuracy: {overt_avg_acc:.4f}\")\nprint(f\"  - Std Dev Accuracy: {overt_std_acc:.4f}\")\nprint(f\"  - Optimal C per fold: {overt_opt_C}\")\nprint(f\"  - Fold Accuracies: {[f'{acc:.4f}' for acc in overt_fold_accs]}\")\n\nLoading Imagined Movement Data...\nImagined Data Shape: X=(408, 120), y=(408,)\nImagined Data Labels: (array([0, 1]), array([204, 204]))\n\nLoading Overt Movement Data...\nOvert Data Shape: X=(408, 120), y=(408,)\nOvert Data Labels: (array([0, 1]), array([204, 204]))\n\n--- Starting Analysis for Imagined Movement Data ---\n\nStarting 6-fold outer cross-validation...\n  Outer Fold 1/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 1: 0.1 (Avg Inner Acc: 0.7735)\n    Training final SVM for Outer Fold 1 with C=0.1...\n  Outer Fold 1/6 Accuracy: 0.7059\n  Outer Fold 2/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 2: 0.1 (Avg Inner Acc: 0.7294)\n    Training final SVM for Outer Fold 2 with C=0.1...\n  Outer Fold 2/6 Accuracy: 0.7206\n  Outer Fold 3/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 3: 0.1 (Avg Inner Acc: 0.7382)\n    Training final SVM for Outer Fold 3 with C=0.1...\n  Outer Fold 3/6 Accuracy: 0.7206\n  Outer Fold 4/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 4: 0.01 (Avg Inner Acc: 0.7147)\n    Training final SVM for Outer Fold 4 with C=0.01...\n  Outer Fold 4/6 Accuracy: 0.7647\n  Outer Fold 5/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 5: 0.1 (Avg Inner Acc: 0.7441)\n    Training final SVM for Outer Fold 5 with C=0.1...\n  Outer Fold 5/6 Accuracy: 0.7794\n  Outer Fold 6/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 6: 0.1 (Avg Inner Acc: 0.7235)\n    Training final SVM for Outer Fold 6 with C=0.1...\n  Outer Fold 6/6 Accuracy: 0.7647\n\nFinished cross-validation.\nOptimal C for each outer fold: [0.1, 0.1, 0.1, 0.01, 0.1, 0.1]\nAccuracy for each outer fold: ['0.7059', '0.7206', '0.7206', '0.7647', '0.7794', '0.7647']\nAverage Accuracy across all outer folds: 0.7426\nStandard Deviation of Accuracy across outer folds: 0.0278\n\n--- Starting Analysis for Overt Movement Data ---\n\nStarting 6-fold outer cross-validation...\n  Outer Fold 1/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 1: 0.1 (Avg Inner Acc: 0.7412)\n    Training final SVM for Outer Fold 1 with C=0.1...\n  Outer Fold 1/6 Accuracy: 0.6912\n  Outer Fold 2/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 2: 0.01 (Avg Inner Acc: 0.7324)\n    Training final SVM for Outer Fold 2 with C=0.01...\n  Outer Fold 2/6 Accuracy: 0.6765\n  Outer Fold 3/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 3: 0.1 (Avg Inner Acc: 0.7471)\n    Training final SVM for Outer Fold 3 with C=0.1...\n  Outer Fold 3/6 Accuracy: 0.7206\n  Outer Fold 4/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 4: 0.01 (Avg Inner Acc: 0.6971)\n    Training final SVM for Outer Fold 4 with C=0.01...\n  Outer Fold 4/6 Accuracy: 0.8088\n  Outer Fold 5/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 5: 0.1 (Avg Inner Acc: 0.7265)\n    Training final SVM for Outer Fold 5 with C=0.1...\n  Outer Fold 5/6 Accuracy: 0.7941\n  Outer Fold 6/6\n    Starting 5-fold inner cross-validation for hyperparameter tuning...\n    Optimal C found for Outer Fold 6: 1.0 (Avg Inner Acc: 0.6853)\n    Training final SVM for Outer Fold 6 with C=1.0...\n  Outer Fold 6/6 Accuracy: 0.6765\n\nFinished cross-validation.\nOptimal C for each outer fold: [0.1, 0.01, 0.1, 0.01, 0.1, 1.0]\nAccuracy for each outer fold: ['0.6912', '0.6765', '0.7206', '0.8088', '0.7941', '0.6765']\nAverage Accuracy across all outer folds: 0.7279\nStandard Deviation of Accuracy across outer folds: 0.0542\n\n\n--- Results Summary ---\nImagined Movement Data:\n  - Average Accuracy: 0.7426\n  - Std Dev Accuracy: 0.0278\n  - Optimal C per fold: [0.1, 0.1, 0.1, 0.01, 0.1, 0.1]\n  - Fold Accuracies: ['0.7059', '0.7206', '0.7206', '0.7647', '0.7794', '0.7647']\n\nOvert Movement Data:\n  - Average Accuracy: 0.7279\n  - Std Dev Accuracy: 0.0542\n  - Optimal C per fold: [0.1, 0.01, 0.1, 0.01, 0.1, 1.0]\n  - Fold Accuracies: ['0.6912', '0.6765', '0.7206', '0.8088', '0.7941', '0.6765']\n\n\n\n# bci_movement_decoding.py\n\"\"\"\nMini‑Project #2 — Brain‑Computer Interface Movement Decoding\n===========================================================\n\nThis script reproduces **all** data‑generation tasks described in the\nproject hand‑out and report guidance.  Running it once will\n\n* load the provided CSV files\n* build the Imagined and Overt data sets (204 features × 240 trials each)\n* perform two‑level cross‑validation (6 outer folds ✕ 5 inner folds) for a\n  **linear SVM** baseline on each data set, choosing α ∈ {0.01, 1, 100, 10000}\n* draw the required visualisations (spatial weight maps, stem plots, per‑fold\n  and aggregate ROC curves)\n* list the six dominant channels for fold #1 of each data set\n* evaluate the two cross‑training scenarios (train overt → test imagined and\n  vice‑versa) with hyper‑parameter selection on the training domain only\n* save every figure under `outputs/` and cache raw numpy arrays for further\n  analysis if desired.\n\nThe code is completely self‑contained **except** for the standard scientific\nPython stack (NumPy, SciPy, scikit‑learn, Matplotlib, Pandas) and requires\nno manual tweaking once the data files are present in the working directory.\n\"\"\"\nfrom __future__ import annotations\nimport os\nimport itertools\nfrom pathlib import Path\nfrom typing import Tuple, List, Dict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import griddata\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\n# -----------------------------------------------------------------------------\n# Config\n# -----------------------------------------------------------------------------\nDATA_DIR = Path('.')          # directory containing the CSVs\nOUTPUT_DIR = Path('outputs')  # where all artefacts are written\nOUTPUT_DIR.mkdir(exist_ok=True)\n\nALPHAS = np.array([1e-2, 1, 1e2, 1e4])  # regularisation grid (α)\nINNER_FOLDS = 5\nOUTER_FOLDS = 6\nRNG = 42\n\n# -----------------------------------------------------------------------------\n# Data loading helpers\n# -----------------------------------------------------------------------------\n\ndef _load_feature_csv(fname: str) -&gt; np.ndarray:\n    \"\"\"Return (trials, features) float64 array from a 204×120 CSV.\"\"\"\n    m = pd.read_csv(DATA_DIR / fname, header=None).values  # shape 204×120\n    return m.T  # 120 trials × 204 features\n\ndef load_datasets() -&gt; Dict[str, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Return dict: {'imagined': (X, y), 'overt': (X, y)}.\"\"\"\n    img1 = _load_feature_csv('../../feaSubEImg_1.csv')\n    img2 = _load_feature_csv('../../feaSubEImg_2.csv')\n    ovt1 = _load_feature_csv('../../feaSubEOvert_1.csv')\n    ovt2 = _load_feature_csv('../../feaSubEOvert_2.csv')\n\n    def _make_ds(cls1, cls2):\n        X = np.vstack([cls1, cls2])         # 240 × 204\n        y = np.hstack([np.zeros(cls1.shape[0]), np.ones(cls2.shape[0])])\n        return X, y\n\n    return {\n        'imagined': _make_ds(img1, img2),\n        'overt':    _make_ds(ovt1, ovt2),\n    }\n\n# -----------------------------------------------------------------------------\n# Visualisation helpers\n# -----------------------------------------------------------------------------\n\ndef plot_weights_brain(w: np.ndarray, xy: np.ndarray, title: str, fname: str):\n    \"\"\"Interpolate |w| onto a regular grid and produce a pcolor heat‑map.\"\"\"\n    # Each electrode has two consecutive channels (Ex, Ey) at same coords.\n    coords = np.repeat(xy, 2, axis=0)  # (204, 2)\n    x, y = coords[:, 0], coords[:, 1]\n    mag = np.abs(w)\n    # regular grid covering the point cloud bounds\n    grid_x, grid_y = np.mgrid[x.min():x.max():200j, y.min():y.max():200j]\n    grid_z = griddata(coords, mag, (grid_x, grid_y), method='cubic', fill_value=np.nan)\n\n    plt.figure(figsize=(5, 5))\n    pc = plt.pcolor(grid_x, grid_y, grid_z, shading='auto')\n    plt.scatter(x, y, c=mag, s=20, edgecolors='k', cmap='viridis')\n    plt.axis('equal'); plt.axis('off'); plt.title(title)\n    plt.colorbar(pc, label='|weight|')\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n\ndef stem_weights(w: np.ndarray, title: str, fname: str, top_k: int = 6):\n    plt.figure(figsize=(8, 4))\n    markerline, stemlines, baseline = plt.stem(range(len(w)), w)\n    plt.setp(markerline, markersize=3)\n    # highlight top‑k channels\n    top_idx = np.argsort(np.abs(w))[-top_k:]\n    plt.scatter(top_idx, w[top_idx], s=60, facecolors='r', edgecolors='k', zorder=3, label='top‑k')\n    plt.axhline(0, color='k', linewidth=0.8)\n    plt.title(title); plt.xlabel('Channel index'); plt.ylabel('Signed weight')\n    plt.legend(); plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n    # also dump a text list for convenience\n    with open(OUTPUT_DIR / (fname.replace('.png', '_top_channels.txt')), 'w') as fp:\n        fp.write('\\n'.join(f'{i}: {w[i]:+.4f}' for i in top_idx[::-1]))\n\n\ndef plot_roc(fprs: List[np.ndarray], tprs: List[np.ndarray], aucs: List[float],\n             mean_fpr: np.ndarray, mean_tpr: np.ndarray, mean_auc: float,\n             title: str, fname: str):\n    plt.figure(figsize=(6, 6))\n    for i, (fpr, tpr, a) in enumerate(zip(fprs, tprs, aucs), start=1):\n        plt.plot(fpr, tpr, lw=1, alpha=0.7, label=f'fold {i} (AUC {a:.2f})')\n    plt.plot(mean_fpr, mean_tpr, 'k', lw=2.5, label=f'aggregate (AUC {mean_auc:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--', lw=0.8)\n    plt.xlabel('False positive rate'); plt.ylabel('True positive rate')\n    plt.title(title); plt.legend(); plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n# -----------------------------------------------------------------------------\n# Core model / CV routine\n# -----------------------------------------------------------------------------\n\ndef _select_c(X: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Return best C chosen by INNER_FOLDS CV (argmax mean accuracy).\"\"\"\n    skf_inner = StratifiedKFold(n_splits=INNER_FOLDS, shuffle=True, random_state=RNG)\n    best_c, best_acc = None, -np.inf\n    for alpha in ALPHAS:\n        C = 1.0 / alpha\n        accs = []\n        for train_idx, val_idx in skf_inner.split(X, y):\n            Xtr, Xval = X[train_idx], X[val_idx]\n            ytr, yval = y[train_idx], y[val_idx]\n            scaler = StandardScaler().fit(Xtr)\n            Xtr_s = scaler.transform(Xtr)\n            Xval_s = scaler.transform(Xval)\n            clf = SVC(kernel='linear', C=C, random_state=RNG)\n            clf.fit(Xtr_s, ytr)\n            yhat = np.sign(clf.decision_function(Xval_s))  # threshold 0 decision\n            accs.append(accuracy_score(yval, (yhat &gt; 0).astype(int)))\n        mean_acc = np.mean(accs)\n        if mean_acc &gt; best_acc:\n            best_acc, best_c = mean_acc, C\n    return best_c\n\n\ndef two_level_cv(X: np.ndarray, y: np.ndarray, label: str, coords: np.ndarray):\n    \"\"\"Perform 6×5 two‑level CV; write all artefacts to disk.\"\"\"\n    skf_outer = StratifiedKFold(n_splits=OUTER_FOLDS, shuffle=True, random_state=RNG)\n\n    fprs, tprs, aucs = [], [], []\n    all_scores, all_labels = [], []\n    weights_fold1 = None\n\n    for fold, (train_idx, test_idx) in enumerate(skf_outer.split(X, y), start=1):\n        Xtr, Xte = X[train_idx], X[test_idx]\n        ytr, yte = y[train_idx], y[test_idx]\n\n        best_c = _select_c(Xtr, ytr)\n        scaler = StandardScaler().fit(Xtr)\n        Xtr_s = scaler.transform(Xtr)\n        Xte_s = scaler.transform(Xte)\n        clf = SVC(kernel='linear', C=best_c, random_state=RNG)\n        clf.fit(Xtr_s, ytr)\n\n        # store weights from fold 1 for visualisation\n        if fold == 1:\n            weights_fold1 = clf.coef_.ravel().copy()\n\n        scores = clf.decision_function(Xte_s)\n        fpr, tpr, _ = roc_curve(yte, scores)\n        fprs.append(fpr); tprs.append(tpr)\n        aucs.append(auc(fpr, tpr))\n        all_scores.append(scores); all_labels.append(yte)\n\n        # per‑fold accuracy\n        acc = accuracy_score(yte, (scores &gt; 0).astype(int))\n        print(f\"[{label}] Fold {fold}: C = {best_c:.3g}, AUC = {aucs[-1]:.3f}, ACC = {acc:.3f}\")\n\n    # aggregate ROC\n    all_scores = np.hstack(all_scores)\n    all_labels = np.hstack(all_labels)\n    mean_fpr, mean_tpr, _ = roc_curve(all_labels, all_scores)\n    mean_auc = auc(mean_fpr, mean_tpr)\n\n    # save ROC figure\n    plot_roc(fprs, tprs, aucs, mean_fpr, mean_tpr, mean_auc,\n             title=f'{label.capitalize()} data — 6‑fold CV ROC',\n             fname=f'roc_{label}.png')\n\n    # save weight visualisations for fold 1\n    plot_weights_brain(weights_fold1, coords,\n                       title=f'{label.capitalize()} fold 1 — |weights| map',\n                       fname=f'weights_map_{label}.png')\n    stem_weights(weights_fold1,\n                 title=f'{label.capitalize()} fold 1 — signed channel weights',\n                 fname=f'weights_stem_{label}.png')\n\n    # dump aggregate arrays for future use\n    np.savez(OUTPUT_DIR / f'roc_raw_{label}.npz',\n         fprs=np.array(fprs, dtype=object),\n         tprs=np.array(tprs, dtype=object),\n         aucs=np.array(aucs),\n         agg_fpr=mean_fpr,\n         agg_tpr=mean_tpr,\n         agg_auc=mean_auc)\n\n    np.save(OUTPUT_DIR / f'weights_fold1_{label}.npy', weights_fold1)\n\n    print(f\"[{label}] Aggregate AUC = {mean_auc:.3f}\\n\")\n\n# -----------------------------------------------------------------------------\n# Cross‑train evaluation\n# -----------------------------------------------------------------------------\n\ndef cross_train_eval(train: Tuple[np.ndarray, np.ndarray],\n                     test: Tuple[np.ndarray, np.ndarray],\n                     train_name: str, test_name: str):\n    Xtr, ytr = train\n    Xte, yte = test\n\n    best_c = _select_c(Xtr, ytr)\n    scaler = StandardScaler().fit(Xtr)\n    clf = SVC(kernel='linear', C=best_c, random_state=RNG)\n    clf.fit(scaler.transform(Xtr), ytr)\n\n    scores = clf.decision_function(scaler.transform(Xte))\n    fpr, tpr, _ = roc_curve(yte, scores)\n    auc_val = auc(fpr, tpr)\n    acc = accuracy_score(yte, (scores &gt; 0).astype(int))\n\n    title = f'Train {train_name} → Test {test_name} (AUC {auc_val:.2f})'\n    plt.figure(figsize=(5, 5))\n    plt.plot(fpr, tpr, lw=2, label=f'AUC = {auc_val:.2f}')\n    plt.plot([0, 1], [0, 1], 'k--', lw=0.8)\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(title); plt.legend(); plt.tight_layout()\n    fname = f'roc_cross_{train_name}_to_{test_name}.png'\n    plt.savefig(OUTPUT_DIR / fname, dpi=300)\n    plt.close()\n\n    print(f'[Cross‑train] {title}, ACC = {acc:.3f}, C = {best_c:.3g}\\n')\n\n# -----------------------------------------------------------------------------\n# Main entry‑point\n# -----------------------------------------------------------------------------\n\ndef main():\n    print('Loading data …')\n    datasets = load_datasets()\n    coords = pd.read_csv(DATA_DIR / '../../BCIsensor_xy.csv', header=None).values  # 102×2\n\n    for label, (X, y) in datasets.items():\n        print(f'Running two‑level CV on {label} data')\n        two_level_cv(X, y, label, coords)\n\n    # cross‑train scenarios\n    print('Running cross‑training scenarios')\n    cross_train_eval(datasets['overt'], datasets['imagined'], 'overt', 'imagined')\n    cross_train_eval(datasets['imagined'], datasets['overt'], 'imagined', 'overt')\n\n    print('All tasks complete. Figures and arrays available under outputs/.')\n\n\nif __name__ == '__main__':\n    main()\n\nLoading data …\nRunning two‑level CV on imagined data\n[imagined] Fold 1: C = 0.01, AUC = 0.960, ACC = 0.850\n[imagined] Fold 2: C = 100, AUC = 1.000, ACC = 0.950\n[imagined] Fold 3: C = 0.01, AUC = 0.908, ACC = 0.800\n[imagined] Fold 4: C = 0.01, AUC = 0.992, ACC = 0.925\n[imagined] Fold 5: C = 100, AUC = 0.963, ACC = 0.825\n[imagined] Fold 6: C = 0.01, AUC = 0.978, ACC = 0.900\n\n\n/var/folders/hv/8cbg61nd45x1vstcpkvdygcw0000gr/T/ipykernel_63180/607345600.py:96: MatplotlibDeprecationWarning: Getting the array from a PolyQuadMesh will return the full array in the future (uncompressed). To get this behavior now set the PolyQuadMesh with a 2D array .set_array(data2d).\n  plt.colorbar(pc, label='|weight|')\n\n\n[imagined] Aggregate AUC = 0.959\n\nRunning two‑level CV on overt data\n[overt] Fold 1: C = 100, AUC = 1.000, ACC = 0.975\n[overt] Fold 2: C = 100, AUC = 1.000, ACC = 0.975\n[overt] Fold 3: C = 100, AUC = 1.000, ACC = 0.975\n[overt] Fold 4: C = 100, AUC = 0.997, ACC = 0.975\n[overt] Fold 5: C = 100, AUC = 1.000, ACC = 0.950\n[overt] Fold 6: C = 100, AUC = 0.990, ACC = 0.950\n\n\n/var/folders/hv/8cbg61nd45x1vstcpkvdygcw0000gr/T/ipykernel_63180/607345600.py:96: MatplotlibDeprecationWarning: Getting the array from a PolyQuadMesh will return the full array in the future (uncompressed). To get this behavior now set the PolyQuadMesh with a 2D array .set_array(data2d).\n  plt.colorbar(pc, label='|weight|')\n\n\n[overt] Aggregate AUC = 0.998\n\nRunning cross‑training scenarios\n[Cross‑train] Train overt → Test imagined (AUC 0.96), ACC = 0.892, C = 100\n\n[Cross‑train] Train imagined → Test overt (AUC 0.98), ACC = 0.912, C = 100\n\nAll tasks complete. Figures and arrays available under outputs/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "",
    "text": "Welcome! Have you ever thought about how your brain tells your arm to move? It’s a complex process involving electrical signals. Now, imagine trying to capture those signals from outside the head using sensors and teaching a computer to understand the intention to move, even if the movement doesn’t happen. That’s the core challenge of this project.\nWe’re exploring the fascinating world of Brain-Computer Interfaces (BCIs), specifically focusing on decoding movement intentions from electroencephalography (EEG) data. Why is this important? BCIs hold immense potential for individuals with limited or no neuromuscular control, offering them new ways to interact with the world, control prosthetic limbs, or communicate (Emotiv, Accessed 2025-04-20; Looned et al., 2017; Ramadan & Vasilakos, 2010).\nOur toolbox includes signal processing techniques to clean up the noisy EEG data and machine learning, specifically Support Vector Machines (SVMs), to classify whether the brain signals correspond to an intended “left” or “right” movement. We’ll be looking at data from both actual (overt) movements and imagined movements, as both are relevant for real-world BCI applications[cite: 155]. Let’s dive in!\n\n\nSo, how does a BCI work? At its heart, it’s a bridge between the brain and an external device (Emotiv, Accessed 2025-04-20). Our brains are constantly buzzing with electrical activity generated by neurons firing (Emotiv, Accessed 2025-04-20). EEG uses sensors placed on the scalp (non-invasively!) to pick up these tiny electrical signals (contributors, Accessed 2025-04-20; Emotiv, Accessed 2025-04-20; Texas at Austin, 2025). Think of it like eavesdropping on the brain’s conversations.\nThe challenge, however, is that these signals are complex, often noisy, and recorded from the scalp, which is quite a distance from the neurons themselves (Texas at Austin, 2025). Furthermore, we’re dealing with a lot of data – signals from many electrodes recorded over time, resulting in high-dimensional datasets[cite: 152]. This is where machine learning comes to the rescue.\nWe need algorithms capable of sifting through this high-dimensional data to find the subtle patterns that indicate a specific intention, like moving the left hand versus the right hand. This project focuses on using SVMs, a type of classifier known for its effectiveness in handling high-dimensional data, even with relatively few training examples – a common scenario in BCI research (GeeksforGeeks, 2025; Hearst et al., 1998; Vidhya, 2021). The ultimate goal is to create a system that translates these detected brain patterns into reliable control signals for computers, wheelchairs, or prosthetic devices (Emotiv, Accessed 2025-04-20; Looned et al., 2017; Ramadan & Vasilakos, 2010)."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "",
    "text": "So, how does a BCI work? At its heart, it’s a bridge between the brain and an external device (Emotiv, Accessed 2025-04-20). Our brains are constantly buzzing with electrical activity generated by neurons firing (Emotiv, Accessed 2025-04-20). EEG uses sensors placed on the scalp (non-invasively!) to pick up these tiny electrical signals (contributors, Accessed 2025-04-20; Emotiv, Accessed 2025-04-20; Texas at Austin, 2025). Think of it like eavesdropping on the brain’s conversations.\nThe challenge, however, is that these signals are complex, often noisy, and recorded from the scalp, which is quite a distance from the neurons themselves (Texas at Austin, 2025). Furthermore, we’re dealing with a lot of data – signals from many electrodes recorded over time, resulting in high-dimensional datasets[cite: 152]. This is where machine learning comes to the rescue.\nWe need algorithms capable of sifting through this high-dimensional data to find the subtle patterns that indicate a specific intention, like moving the left hand versus the right hand. This project focuses on using SVMs, a type of classifier known for its effectiveness in handling high-dimensional data, even with relatively few training examples – a common scenario in BCI research (GeeksforGeeks, 2025; Hearst et al., 1998; Vidhya, 2021). The ultimate goal is to create a system that translates these detected brain patterns into reliable control signals for computers, wheelchairs, or prosthetic devices (Emotiv, Accessed 2025-04-20; Looned et al., 2017; Ramadan & Vasilakos, 2010)."
  },
  {
    "objectID": "index.html#data-acquisition",
    "href": "index.html#data-acquisition",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nFirst, we need the raw material: brain signals. The data for this project comes from EEG recordings of a human subject[cite: 3]. EEG measures electrical activity using electrodes placed on the scalp (Emotiv, Accessed 2025-04-20). In our case, 102 electrodes were used, each providing two channels of information related to the electrical field gradient (one for the x-direction, one for the y-direction), giving us a total of 204 data channels per time point or trial[cite: 168].\nCrucially, the data was collected under two different conditions[cite: 3, 155]: 1. Overt Movement: The subject physically moved their left or right arm. We expect these signals to be stronger and potentially easier to classify[cite: 156]. 2. Imagined Movement: The subject imagined moving their left or right arm but remained still. These signals are typically weaker but are highly relevant for BCIs designed for individuals who cannot physically move[cite: 156].\nFor each condition, we have 120 trials for “movement 1” (which corresponds to either left or right, the mapping is unknown [cite: 4]) and 120 trials for “movement 2” (the opposite direction), giving us 240 trials per condition[cite: 176, 191]. Each trial is represented by a 204-dimensional vector[cite: 176].\n[Visualization: Electrode locations on the scalp (similar to image from (171?))]"
  },
  {
    "objectID": "index.html#signal-processing",
    "href": "index.html#signal-processing",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "2.2 Signal Processing",
    "text": "2.2 Signal Processing\nRaw EEG data can be messy. Before feeding it to our classifier, some pre-processing is typically needed, although the specifics aren’t the focus of this report. Generally, this involves techniques to reduce noise and artifacts (like blinks or muscle movements) and potentially extract features that are more informative for classification. For this project, we’re using the provided feature vectors directly."
  },
  {
    "objectID": "index.html#classification-with-support-vector-machines-svms",
    "href": "index.html#classification-with-support-vector-machines-svms",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "2.3 Classification with Support Vector Machines (SVMs)",
    "text": "2.3 Classification with Support Vector Machines (SVMs)\nThis is where the machine learning magic happens! We’re using Support Vector Machines (SVMs) as our classifier. Why SVMs?\n\nGood with High Dimensions: As mentioned, EEG data is high-dimensional (204 features!), and SVMs are known to handle this well, avoiding the “curse of dimensionality” better than some other methods (GeeksforGeeks, 2025; Hearst et al., 1998; Vidhya, 2021).\nFinding the Best Boundary: The core idea of SVM is to find the “best” dividing line (or hyperplane in higher dimensions) that separates the data points belonging to different classes (e.g., “left movement” vs. “right movement”) (GeeksforGeeks, 2025; QuantStart, Accessed 2025-04-20; Santhanam, 2019). “Best” usually means the hyperplane that has the maximum possible margin or gap between itself and the closest data points (called support vectors) from each class (Banerjee, Accessed 2025-04-20; GeeksforGeeks, 2025; Santhanam, 2019; Vidhya, 2021). This maximization of the margin often leads to better generalization to new, unseen data (Jakkula, Accessed 2025-04-20).\n\n[Visualization: Simple 2D example of an SVM hyperplane, margin, and support vectors (similar to image from (179?) or descriptions in (QuantStart, Accessed 2025-04-20))]\nOur SVM Setup: * Linear Kernel: We’ll start with a linear SVM as our baseline[cite: 16]. This assumes the data can be separated by a straight line (or flat plane). We might explore other, non-linear kernels later[cite: 16, 179]. * Decision Statistic: Instead of just outputting a hard decision (“left” or “right”), we need our SVM implementation to output a continuous decision statistic (often related to the distance from the separating hyperplane)[cite: 11, 143]. This is crucial for generating Receiver Operating Characteristic (ROC) curves, which help us evaluate performance across different decision thresholds. * Regularization: SVMs involve an optimization process that often includes a regularization parameter (let’s call it \\(\\alpha\\))[cite: 180]. This parameter helps prevent overfitting by controlling the trade-off between maximizing the margin and minimizing classification errors on the training data. Finding the right \\(\\alpha\\) is key[cite: 181]. * Toolboxes: We’re allowed to use existing SVM libraries in Python or Matlab, but we need to understand how they work and cite them properly[cite: 6, 8]. For example, using Scikit-learn’s SVC implementation (“SVC,” n.d.)."
  },
  {
    "objectID": "index.html#model-evaluation-two-level-cross-validation",
    "href": "index.html#model-evaluation-two-level-cross-validation",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "2.4 Model Evaluation: Two-Level Cross-Validation",
    "text": "2.4 Model Evaluation: Two-Level Cross-Validation\nHow do we know how well our SVM classifier performs and how do we choose the best regularization parameter (\\(\\alpha\\))? We use a technique called two-level cross-validation[cite: 15]. This might sound complex, but it’s a robust way to get reliable performance estimates and tune parameters simultaneously.\nHere’s the gist: 1. Outer Level (1st Level): We split the entire dataset (e.g., all 240 Overt trials) into several folds (6 folds in this project [cite: 191]). We iterate 6 times. In each iteration, we use 5 folds (200 trials) for training and 1 fold (40 trials) for testing[cite: 192]. This gives us 6 different performance estimates. 2. Inner Level (2nd Level): Before training the SVM in the outer loop, we need to find the best \\(\\alpha\\) for that specific training set (the 200 trials). To do this, we perform another cross-validation within those 200 trials[cite: 187]. We use 5-fold cross-validation here[cite: 193]. We split the 200 trials into 5 inner folds (160 for inner training, 40 for inner testing)[cite: 193]. We try different values of \\(\\alpha\\) (e.g., 0.01, 1, 100, 10000 [cite: 193]), train an SVM on the 160 trials, test on the 40, and repeat for all 5 inner folds. The \\(\\alpha\\) that gives the best average performance (e.g., accuracy [cite: 192]) across these inner folds is chosen. 3. Back to Outer Level: Now, using the best \\(\\alpha\\) found in the inner loop, we train the final SVM for this outer fold on all 200 training trials and test it on the held-out 40 trials[cite: 189]. 4. Repeat: We repeat steps 2 and 3 for all 6 outer folds.\nThis process ensures that our parameter tuning (\\(\\alpha\\) selection) doesn’t “see” the final test data for that fold, giving us a less biased estimate of how well the model will generalize. We evaluate performance using ROC curves and accuracy[cite: 18, 110].\n[Visualization: Diagram illustrating two-level cross-validation (similar to image from (186?))]"
  },
  {
    "objectID": "index.html#classification-performance",
    "href": "index.html#classification-performance",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "3.1 Classification Performance",
    "text": "3.1 Classification Performance\nFor the “Same-Train” scenarios, we’ll look at the cross-validated performance[cite: 18]. We expect the SVM classifier to distinguish between the two movement types reasonably well. The original document mentions promising results with SVMs for similar tasks (Hearst et al., 1998). We’ll present performance using:\n\nROC Curves: These plots show the trade-off between correctly identifying one class (true positive rate) and incorrectly identifying the other class (false positive rate) across different decision thresholds. We’ll show the ROC curve for each cross-validation fold and the average ROC curve[cite: 110, 115, 117].\nAccuracy: The overall percentage of trials classified correctly, typically calculated at a specific threshold (e.g., \\(\\beta=0\\)). We’ll report accuracy for each fold and the average cross-validated accuracy[cite: 110, 116, 118].\n\n[Visualization: Combined plot showing 6 individual-fold ROC curves and the average ROC curve for the Same-Train Overt scenario] [Visualization: Combined plot showing 6 individual-fold ROC curves and the average ROC curve for the Same-Train Imagined scenario]\nFor the “Cross-Train” scenarios, we’ll train the SVM on the entire training dataset (using the best \\(\\alpha\\) found via cross-validation on that training set) and then evaluate it on the other dataset type[cite: 19]. We’ll present ROC curves and accuracy for these cases as well.\n[Visualization: Plot comparing ROC curves for Overt-to-Imagined and Imagined-to-Overt cross-training scenarios]\nWe anticipate differences in performance: * Overt data might yield better results than Imagined data due to stronger signals[cite: 156]. * Cross-training performance might be lower than same-training performance, indicating differences between overt and imagined brain activity patterns."
  },
  {
    "objectID": "index.html#feature-channel-importance",
    "href": "index.html#feature-channel-importance",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "3.2 Feature (Channel) Importance",
    "text": "3.2 Feature (Channel) Importance\nSVMs don’t just classify; they also provide insights into which features (EEG channels, in our case) are most important for making the classification decision. The SVM calculates a weight for each channel[cite: 184]. Channels with larger magnitude weights (positive or negative) contribute more significantly to the decision boundary[cite: 184].\nWe’ll analyze these weights: * Spatial Distribution: We can visualize the magnitude of these weights across the scalp to see which brain regions seem most informative for distinguishing left vs. right movements[cite: 80, 112, 113]. This might align with known brain areas involved in motor control. * Top Channels: We’ll identify the specific channels with the largest weights (e.g., top 6)[cite: 112, 113].\n[Visualization: Brain surface plot showing SVM channel weight magnitudes for an example fold (Overt data) (similar to image from (183?))] [Visualization: Stem plot showing signed SVM weights for all 204 channels for an example fold (Overt data), highlighting the top 6] [Visualization: Brain surface plot showing SVM channel weight magnitudes for an example fold (Imagined data)] [Visualization: Stem plot showing signed SVM weights for all 204 channels for an example fold (Imagined data), highlighting the top 6]\nComparing the weight patterns between Overt and Imagined conditions might also reveal interesting differences in brain activity."
  },
  {
    "objectID": "index.html#comparison-with-previous-work",
    "href": "index.html#comparison-with-previous-work",
    "title": "Brain-Computer Interface Movement Decoding",
    "section": "4.1 Comparison with Previous Work",
    "text": "4.1 Comparison with Previous Work\nOur work builds upon a rich history of BCI research (Emotiv, Accessed 2025-04-20; Ramadan & Vasilakos, 2010). Many studies have explored EEG-based movement decoding using various algorithms. SVMs have been a popular choice due to their theoretical grounding and practical performance (Hearst et al., 1998; Jakkula, Accessed 2025-04-20). We aim to contribute by systematically evaluating linear SVMs with rigorous two-level cross-validation across overt and imagined movement conditions, providing detailed performance metrics and channel importance analysis. Our results should be comparable to, and hopefully advance upon, similar studies in the field."
  }
]