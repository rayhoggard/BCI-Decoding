[
  {
    "objectID": "notebooks/Untitled.html",
    "href": "notebooks/Untitled.html",
    "title": "BCI Movement Decoding",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv(\"../assets/BCIsensor_xy.csv\", header=None)\nx = df.iloc[:, 0]\ny = df.iloc[:, 1]\nplt.plot(x, y)\nplt.scatter(x, y, color=\"blue\", s=20, zorder=3)\nfor i, (xi, yi) in enumerate(zip(x, y)):\n        plt.text(xi, yi, str(i + 1), fontsize=8, ha='right', va='bottom')\nplt.axis(\"equal\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import griddata\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, make_scorer\nimport time\nimport warnings\n\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n\ndef load_and_prepare_data(file1, file2, label1=0, label2=1):\n    print(f\"Loading data: {file1} (Label {label1}), {file2} (Label {label2})\")\n    try:\n        df1 = pd.read_csv(file1, header=None).T\n        df2 = pd.read_csv(file2, header=None).T\n        if df1.shape[1] != 204 or df2.shape[1] != 204:\n             print(f\"Warning: Expected 204 features. Got {df1.shape[1]} & {df2.shape[1]}.\")\n        y1 = np.full(df1.shape[0], label1)\n        y2 = np.full(df2.shape[0], label2)\n        X = pd.concat([df1, df2], ignore_index=True).values\n        y = np.concatenate([y1, y2])\n        print(f\"Data loaded. X shape: {X.shape}, y shape: {y.shape}\")\n        return X, y\n    except FileNotFoundError as e:\n        print(f\"Error: File not found - {e}.\")\n        return None, None\n    except Exception as e:\n        print(f\"Error loading data files: {e}\")\n        return None, None\n\ndef load_sensor_locations(filename=\"BCIsensor_xy.csv\"):\n    print(f\"Loading sensor locations from: {filename}\")\n    try:\n        locations = pd.read_csv(filename, header=None, names=['x', 'y']).values\n        if locations.shape[0] != 102 or locations.shape[1] != 2:\n            print(f\"Warning: Expected 102x2 shape, got {locations.shape}.\")\n        print(f\"Sensor locations loaded. Shape: {locations.shape}\")\n        return locations[:, 0], locations[:, 1]\n    except FileNotFoundError:\n        print(f\"Error: Sensor location file '{filename}' not found. Please ensure it's in the correct directory (e.g., '../assets/').\")\n        return None, None\n    except Exception as e:\n        print(f\"Error loading sensor locations: {e}\")\n        return None, None\n\ndef approximate_svm_weights(svm_model):\n    try:\n        weights = svm_model.dual_coef_ @ svm_model.support_vectors_\n        return weights.flatten()\n    except Exception as e:\n        print(f\"Error approximating SVM weights: {e}\")\n        return None\n\n\ndef perform_two_level_cv(X, y, data_label=\"Data\", kernel='linear', gamma='scale', degree=3, C_range=None, outer_k=6, inner_k=5):\n    if C_range is None:\n      C_range = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] # Default range\n\n    outer_cv = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=42)\n    inner_cv = StratifiedKFold(n_splits=inner_k, shuffle=True, random_state=42)\n\n    outer_fold_accuracies, optimal_Cs = [], []\n    all_true_labels, all_decision_scores = [], []\n    outer_fold_true_labels, outer_fold_scores = [], []\n    fold1_coefficients = None\n\n    print(f\"\\n--- Starting {outer_k}-Fold Outer CV for {data_label} Data (Kernel: {kernel}) ---\")\n    start_time_cv = time.time()\n\n    for i, (train_outer_idx, test_outer_idx) in enumerate(outer_cv.split(X, y)):\n        fold_start_time = time.time()\n        print(f\"  Outer Fold {i+1}/{outer_k}\")\n        X_train_outer, X_test_outer = X[train_outer_idx], X[test_outer_idx]\n        y_train_outer, y_test_outer = y[train_outer_idx], y[test_outer_idx]\n\n        scaler = StandardScaler()\n        X_train_outer_scaled = scaler.fit_transform(X_train_outer)\n        X_test_outer_scaled = scaler.transform(X_test_outer)\n\n        best_inner_acc = -1\n        best_C_for_fold = C_range[0]\n        for C_val in C_range:\n            inner_fold_accuracies = []\n            for j, (train_inner_idx, val_inner_idx) in enumerate(inner_cv.split(X_train_outer_scaled, y_train_outer)):\n                X_train_inner, X_val_inner = X_train_outer_scaled[train_inner_idx], X_train_outer_scaled[val_inner_idx]\n                y_train_inner, y_val_inner = y_train_outer[train_inner_idx], y_train_outer[val_inner_idx]\n                svm_inner = SVC(kernel=kernel, C=C_val, gamma=gamma, degree=degree,\n                                random_state=42, probability=False, max_iter=10000)\n                svm_inner.fit(X_train_inner, y_train_inner)\n                accuracy = svm_inner.score(X_val_inner, y_val_inner)\n                inner_fold_accuracies.append(accuracy)\n            avg_inner_acc = np.mean(inner_fold_accuracies)\n            if avg_inner_acc &gt; best_inner_acc:\n                best_inner_acc = avg_inner_acc\n                best_C_for_fold = C_val\n\n        optimal_Cs.append(best_C_for_fold)\n\n        final_svm = SVC(kernel=kernel, C=best_C_for_fold, gamma=gamma, degree=degree,\n                        random_state=42, probability=False, max_iter=20000)\n        final_svm.fit(X_train_outer_scaled, y_train_outer)\n\n        outer_accuracy = final_svm.score(X_test_outer_scaled, y_test_outer)\n        outer_fold_accuracies.append(outer_accuracy)\n        decision_scores = final_svm.decision_function(X_test_outer_scaled)\n        all_true_labels.extend(y_test_outer)\n        all_decision_scores.extend(decision_scores)\n        outer_fold_true_labels.append(y_test_outer)\n        outer_fold_scores.append(decision_scores)\n\n        if i == 0:\n            if kernel == 'linear':\n                fold1_coefficients = final_svm.coef_.flatten()\n                print(\"    Stored actual coefficients for linear kernel (Fold 1).\")\n            else:\n                print(f\"    Attempting to approximate coefficients for {kernel} kernel (Fold 1)...\")\n                fold1_coefficients = approximate_svm_weights(final_svm)\n                if fold1_coefficients is not None:\n                    print(\"    Successfully approximated and stored coefficients.\")\n                else:\n                     print(\"    Approximation failed or not possible.\")\n\n        fold_end_time = time.time()\n        print(f\"  Outer Fold {i+1} completed. Accuracy: {outer_accuracy:.4f}. Optimal C: {best_C_for_fold}. Time: {fold_end_time - fold_start_time:.1f}s\")\n\n    end_time_cv = time.time()\n    average_accuracy = np.mean(outer_fold_accuracies)\n    std_dev_accuracy = np.std(outer_fold_accuracies)\n\n    print(f\"\\nFinished {outer_k}-fold CV for {data_label} data (Kernel: {kernel}).\")\n    print(f\"Total CV Time: {end_time_cv - start_time_cv:.1f}s\")\n    print(f\"Optimal C found per fold: {optimal_Cs}\")\n    print(f\"Accuracy per fold: {[f'{acc:.4f}' for acc in outer_fold_accuracies]}\")\n    print(f\"Average Accuracy: {average_accuracy:.4f} +/- {std_dev_accuracy:.4f}\")\n\n    results = {\n        'avg_accuracy': average_accuracy, 'std_accuracy': std_dev_accuracy,\n        'optimal_Cs': optimal_Cs, 'fold_accuracies': outer_fold_accuracies,\n        'all_true_labels': np.array(all_true_labels),\n        'all_decision_scores': np.array(all_decision_scores),\n        'fold1_coefficients': fold1_coefficients, # Contains actual (linear) or approximated weights\n        'outer_fold_true_labels': outer_fold_true_labels,\n        'outer_fold_scores': outer_fold_scores,\n        'kernel': kernel # Store kernel used\n    }\n    return results\n\n\ndef tune_hyperparameters(X_train, y_train, kernel, C_range, gamma_range=None, degree_range=None, n_splits=5):\n    print(f\"    Tuning hyperparameters (Kernel: {kernel}) using {n_splits}-fold CV...\")\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train) # Scale data once before tuning\n\n    param_grid = {'C': C_range}\n    if kernel in ['rbf', 'poly', 'sigmoid'] and gamma_range:\n        param_grid['gamma'] = gamma_range\n    if kernel == 'poly' and degree_range:\n        param_grid['degree'] = degree_range\n\n    scorer = make_scorer(accuracy_score)\n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    grid_search = GridSearchCV(SVC(kernel=kernel, random_state=42, probability=False, max_iter=10000),\n                               param_grid, scoring=scorer, cv=cv, n_jobs=-1) # Use all available CPU cores\n    try:\n        grid_search.fit(X_train_scaled, y_train)\n        print(f\"    Best parameters found: {grid_search.best_params_} (Best Score: {grid_search.best_score_:.4f})\")\n        return grid_search.best_params_\n    except Exception as e:\n        print(f\"    Error during GridSearchCV: {e}\")\n        # Return default C if grid search fails? Or raise error?\n        print(\"    Hyperparameter tuning failed.\")\n        return None\n\n\ndef perform_cross_training(X_train, y_train, X_test, y_test, train_label, test_label, kernel='linear', gamma='scale', degree=3):\n    if X_train is None or y_train is None or X_test is None or y_test is None:\n        print(\"Error: Missing input data for cross-training.\")\n        return None\n\n    print(f\"\\n--- Cross-Training: Train on {train_label}, Test on {test_label} (Kernel: {kernel}) ---\")\n    start_time_xt = time.time()\n\n    C_range_xt = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n    gamma_range_xt = ['scale', 'auto', 0.001, 0.01, 0.1] # Example if tuning gamma for RBF/Poly\n    degree_range_xt = [2, 3, 4]         # Example if tuning degree for Poly\n\n    tune_gamma = kernel in ['rbf', 'poly', 'sigmoid']\n    tune_degree = kernel == 'poly'\n\n    best_params = tune_hyperparameters(\n        X_train, y_train, kernel, C_range_xt,\n        gamma_range=gamma_range_xt if tune_gamma else None,\n        degree_range=degree_range_xt if tune_degree else None\n    )\n\n    if best_params is None:\n        print(\"Cross-training aborted due to hyperparameter tuning failure.\")\n        return None\n\n    best_C = best_params.get('C') # Should always be found if tuning didn't fail\n    best_gamma = best_params.get('gamma', gamma) # Use passed gamma if not tuned\n    best_degree = best_params.get('degree', degree) # Use passed degree if not tuned\n    print(f\"    Using parameters for final model: C={best_C}, gamma={best_gamma}, degree={best_degree}\")\n\n    print(\"    Training final model on entire training set...\")\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    final_svm = SVC(kernel=kernel, C=best_C, gamma=best_gamma, degree=best_degree,\n                    random_state=42, probability=False, max_iter=20000)\n    final_svm.fit(X_train_scaled, y_train)\n\n    print(\"    Testing model on test set...\")\n    X_test_scaled = scaler.transform(X_test) # Use scaler fit on training data\n    accuracy = final_svm.score(X_test_scaled, y_test)\n    decision_scores = final_svm.decision_function(X_test_scaled)\n\n    end_time_xt = time.time()\n    print(f\"Cross-Training completed. Time: {end_time_xt - start_time_xt:.1f}s\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n    results = {\n        'accuracy': accuracy,\n        'true_labels': y_test,\n        'decision_scores': decision_scores,\n        'best_params': best_params,\n        'kernel': kernel\n    }\n    return results\n\n\n\ndef plot_svm_weights_stem(weights, title=\"SVM Channel Weights (Fold 1)\", top_n=6):\n    \"\"\"Generates a stem plot of SVM weights (actual or approximated), highlighting top N.\"\"\"\n    if weights is None:\n        print(f\"Info: Skipping stem plot '{title}' (weights not available).\")\n        return\n    if len(weights) == 0:\n        print(f\"Error: Empty weights provided for stem plot '{title}'.\")\n        return\n\n    n_channels = len(weights)\n    if n_channels != 204:\n         print(f\"Warning: Expected 204 weights for stem plot, got {n_channels}. Adjusting plot limits.\")\n\n    channel_indices = np.arange(1, n_channels + 1)\n    actual_top_n = min(top_n, n_channels)\n    if actual_top_n &lt; top_n:\n        print(f\"Warning: Requested top {top_n} channels, but only {n_channels} available. Plotting top {actual_top_n}.\")\n\n    # Handle cases with very few channels correctly\n    if actual_top_n &gt; 0:\n        dominant_indices = np.argsort(np.abs(weights))[-actual_top_n:]\n    else:\n        dominant_indices = []\n\n\n    plt.figure(figsize=(12, 6))\n    markerline, stemlines, baseline = plt.stem(channel_indices, weights, linefmt='grey', markerfmt='o', basefmt='r-', label='_nolegend_')\n    plt.setp(markerline, markersize=4, markerfacecolor='grey', markeredgecolor='black')\n    for idx in dominant_indices:\n        plt.stem(channel_indices[idx], weights[idx], linefmt='b-', markerfmt='bo', basefmt=' ') # Re-plot dominant stems to be blue\n        vertical_offset = 0.05 * np.max(np.abs(weights)) * np.sign(weights[idx]) if np.max(np.abs(weights)) &gt; 0 else 0.05 * np.sign(weights[idx])\n        # If weight is zero, place text slightly above\n        if weights[idx] == 0:\n            vertical_offset = 0.05\n        plt.text(channel_indices[idx], weights[idx] + vertical_offset, f'{weights[idx]:.2f}',\n                 ha='center', va='bottom' if weights[idx] &gt;= 0 else 'top', color='blue', fontsize=9)\n\n    if actual_top_n &gt; 0:\n        dominant_proxy = plt.Line2D([0], [0], linestyle='none', c='b', marker='o', markersize=5, label=f'Top {actual_top_n} Abs. Magnitude Channels')\n        plt.legend(handles=[dominant_proxy])\n    else:\n        plt.legend()\n\n    plt.xlabel(\"Channel Index (1-204)\"); plt.ylabel(\"SVM Weight / Approx. Weight\"); plt.title(title)\n    plt.xlim(0, max(n_channels, 1) + 1) # Ensure xlim is at least (0, 2)\n    plt.grid(True, axis='y', linestyle=':'); plt.tight_layout(); plt.show()\n\n\ndef plot_weights_on_brain(weights, sensor_x, sensor_y, title=\"SVM Weight Magnitude on Brain Surface (Fold 1)\", grid_resolution=100):\n    if weights is None:\n        print(f\"Info: Skipping brain plot '{title}' (weights not available).\")\n        return\n    if len(weights) != 204:\n        print(f\"Error: Expected 204 weights for brain plot '{title}'. Got {len(weights)}.\")\n        return\n    if sensor_x is None or sensor_y is None or len(sensor_x) != 102 or len(sensor_y) != 102:\n        print(f\"Error: Sensor locations missing or incorrect for brain plot '{title}'.\")\n        return\n\n    try:\n        electrode_magnitudes = np.sqrt(weights[0::2]**2 + weights[1::2]**2)\n    except IndexError:\n         print(f\"Error: Could not pair weights for magnitude calculation in brain plot '{title}'. Check weight vector length.\")\n         return\n\n    if len(electrode_magnitudes) != 102:\n        print(f\"Error: Calculated electrode magnitude array size incorrect ({len(electrode_magnitudes)}). Expected 102 for plot '{title}'.\"); return\n\n    xi = np.linspace(sensor_x.min()-0.5, sensor_x.max()+0.5, grid_resolution)\n    yi = np.linspace(sensor_y.min()-0.5, sensor_y.max()+0.5, grid_resolution)\n    xi, yi = np.meshgrid(xi, yi)\n\n    zi = griddata((sensor_x, sensor_y), electrode_magnitudes, (xi, yi), method='cubic') # 'linear', 'nearest', 'cubic'\n\n    plt.figure(figsize=(7, 6))\n    contour = plt.contourf(xi, yi, zi, levels=15, cmap=plt.cm.viridis) # Adjust levels and cmap as needed\n    plt.colorbar(contour, label='SVM Weight Magnitude (Approx. for Non-Linear)')\n    # Overlay sensor locations\n    # plt.scatter(sensor_x, sensor_y, c='red', s=10, label='Sensor Locations') # Optional: Show sensors\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.axis('off') # Hide axes for a cleaner look\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_overall_roc(true_labels, decision_scores, data_label=\"Data\", title=\"Overall ROC Curve\"):\n    if true_labels is None or decision_scores is None:\n        print(f\"Error: Missing true_labels or decision_scores for ROC plot '{title}'.\")\n        return\n    if len(true_labels) != len(decision_scores):\n         print(f\"Error: Mismatched lengths for true_labels ({len(true_labels)}) and decision_scores ({len(decision_scores)}) for ROC plot '{title}'.\")\n         return\n    if len(true_labels) == 0:\n        print(f\"Error: Empty data provided for ROC plot '{title}'.\")\n        return\n\n    fpr, tpr, thresholds = roc_curve(true_labels, decision_scores)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC ({data_label}, AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance (AUC = 0.50)')\n    plt.xlim([-0.02, 1.0]); plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(title)\n    plt.legend(loc=\"lower right\"); plt.grid(True, linestyle=':'); plt.tight_layout(); plt.show()\n\n\ndef plot_individual_and_overall_roc(outer_fold_true_labels, outer_fold_scores,\n                                    all_true_labels, all_decision_scores,\n                                    data_label=\"Data\", title=\"Individual Fold and Overall ROC Curves\"):\n    if not isinstance(outer_fold_true_labels, list) or not isinstance(outer_fold_scores, list) or \\\n       len(outer_fold_true_labels) != len(outer_fold_scores):\n        print(f\"Error: Invalid or mismatched per-fold data for Indiv. ROC plot '{title}'. Expected lists of arrays.\")\n        return\n    if all_true_labels is None or all_decision_scores is None :\n         print(f\"Error: Missing aggregated data (all_true_labels or all_decision_scores) for Indiv. ROC plot '{title}'.\")\n         return\n    if len(all_true_labels) == 0:\n         print(f\"Error: Empty aggregated data for Indiv. ROC plot '{title}'.\")\n         return\n\n\n    plt.figure(figsize=(9, 7))\n    n_folds = len(outer_fold_true_labels)\n    fold_aucs = []\n    for i in range(n_folds):\n        # Check if fold data is valid\n        if len(outer_fold_true_labels[i]) &gt; 0 and len(outer_fold_scores[i]) &gt; 0 and \\\n           len(np.unique(outer_fold_true_labels[i])) &gt; 1: # Need at least two classes for ROC\n            try:\n                fpr, tpr, _ = roc_curve(outer_fold_true_labels[i], outer_fold_scores[i])\n                fold_auc = auc(fpr, tpr)\n                fold_aucs.append(fold_auc)\n                plt.plot(fpr, tpr, lw=1, alpha=0.4, label=f'Fold {i+1} (AUC = {fold_auc:.2f})')\n            except Exception as e:\n                print(f\"Warning: Could not plot ROC for fold {i+1}. Error: {e}\")\n        else:\n             print(f\"Warning: Skipping ROC for fold {i+1} due to insufficient or invalid data.\")\n\n    # Calculate and plot overall ROC only if aggregated data is valid\n    if len(all_true_labels) &gt; 0 and len(all_decision_scores) &gt; 0 and \\\n       len(np.unique(all_true_labels)) &gt; 1:\n            try:\n                fpr_all, tpr_all, _ = roc_curve(all_true_labels, all_decision_scores)\n                roc_auc_all = auc(fpr_all, tpr_all)\n                plt.plot(fpr_all, tpr_all, color='b', lw=2.5, alpha=0.9,\n                         label=f'Overall ROC ({data_label}, AUC = {roc_auc_all:.2f})')\n            except Exception as e:\n                print(f\"Warning: Could not plot Overall ROC. Error: {e}\")\n    else:\n         print(\"Warning: Skipping Overall ROC due to insufficient or invalid aggregated data.\")\n\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance (AUC = 0.50)')\n    plt.xlim([-0.02, 1.0]); plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(title)\n    if fold_aucs:\n        mean_fold_auc = np.mean(fold_aucs)\n        std_fold_auc = np.std(fold_aucs)\n        plt.legend(loc=\"lower right\", fontsize='small', title=f\"Mean Fold AUC: {mean_fold_auc:.2f} ± {std_fold_auc:.2f}\")\n    else:\n        plt.legend(loc=\"lower right\", fontsize='small')\n    plt.grid(True, linestyle=':')\n    plt.tight_layout(); plt.show()\n\nif __name__ == \"__main__\":\n    print(\"\\n=== Starting BCI Analysis ===\")\n\n    BASE_PATH = '../assets/'\n    OVERT_FILE_1 = BASE_PATH + 'feaSubEOvert_1.csv'\n    OVERT_FILE_2 = BASE_PATH + 'feaSubEOvert_2.csv'\n    IMG_FILE_1 = BASE_PATH + 'feaSubEImg_1.csv'\n    IMG_FILE_2 = BASE_PATH + 'feaSubEImg_2.csv'\n    SENSOR_FILE = BASE_PATH + 'BCIsensor_xy.csv'\n\n    RUN_LINEAR_KERNEL = True      # Run linear kernel experiments\n    RUN_RBF_KERNEL = True         # Run RBF kernel experiments\n    RUN_POLY_KERNEL = True        # Run Polynomial kernel experiments\n    RUN_CROSS_TRAIN_LINEAR = True # Run cross-training (only implemented for linear here)\n\n    DEFAULT_GAMMA = 'scale'\n    DEFAULT_DEGREE = 3\n    DEFAULT_C_RANGE = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\n    sensor_x, sensor_y = load_sensor_locations(SENSOR_FILE)\n\n    X_overt, y_overt = load_and_prepare_data(OVERT_FILE_1, OVERT_FILE_2)\n    X_img, y_img = load_and_prepare_data(IMG_FILE_1, IMG_FILE_2)\n\n    def run_plots(results, data_type, kernel_type, sensor_x, sensor_y):\n\n        weights = results.get('fold1_coefficients', None)\n        weight_note = \"(Approximated)\" if kernel_type != 'linear' and weights is not None else \"\"\n\n        plot_svm_weights_stem(weights,\n                              title=f\"{data_type} ({kernel_type}): SVM Weights {weight_note} (Fold 1)\")\n        if sensor_x is not None and sensor_y is not None:\n             plot_weights_on_brain(weights, sensor_x, sensor_y,\n                                   title=f\"{data_type} ({kernel_type}): Weight Magnitude {weight_note} (Fold 1)\")\n        else:\n             print(f\"Skipping brain plot for {data_type} ({kernel_type}) due to missing sensor locations.\")\n\n        plot_individual_and_overall_roc(results.get('outer_fold_true_labels', []), results.get('outer_fold_scores', []),\n                                        results.get('all_true_labels', None), results.get('all_decision_scores', None),\n                                        data_label=f\"{data_type} ({kernel_type})\",\n                                        title=f\"{data_type} ({kernel_type}): Individual & Overall ROC\")\n\n    if RUN_LINEAR_KERNEL:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Overt (Linear Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None:\n            overt_results_lin = perform_two_level_cv(X_overt, y_overt, data_label=\"Overt\", kernel='linear', C_range=DEFAULT_C_RANGE)\n            run_plots(overt_results_lin, \"Overt\", \"Linear\", sensor_x, sensor_y)\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Imagined (Linear Kernel)\\n\" + \"=\"*30)\n        if X_img is not None:\n             img_results_lin = perform_two_level_cv(X_img, y_img, data_label=\"Imagined\", kernel='linear', C_range=DEFAULT_C_RANGE)\n             run_plots(img_results_lin, \"Imagined\", \"Linear\", sensor_x, sensor_y)\n\n\n    if RUN_CROSS_TRAIN_LINEAR:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Cross-Train Overt -&gt; Imagined (Linear Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None and X_img is not None:\n            xt_ov_img_lin = perform_cross_training(X_overt, y_overt, X_img, y_img, \"Overt\", \"Imagined\", kernel='linear')\n            if xt_ov_img_lin:\n                 plot_overall_roc(xt_ov_img_lin['true_labels'], xt_ov_img_lin['decision_scores'],\n                                  data_label=\"Train Overt, Test Imagined (Linear)\",\n                                  title=\"Cross-Train ROC: Overt -&gt; Imagined (Linear)\")\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Cross-Train Imagined -&gt; Overt (Linear Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None and X_img is not None:\n             xt_img_ov_lin = perform_cross_training(X_img, y_img, X_overt, y_overt, \"Imagined\", \"Overt\", kernel='linear')\n             if xt_img_ov_lin:\n                  plot_overall_roc(xt_img_ov_lin['true_labels'], xt_img_ov_lin['decision_scores'],\n                                   data_label=\"Train Imagined, Test Overt (Linear)\",\n                                   title=\"Cross-Train ROC: Imagined -&gt; Overt (Linear)\")\n\n    if RUN_RBF_KERNEL:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Overt (RBF Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None:\n            overt_results_rbf = perform_two_level_cv(X_overt, y_overt, data_label=\"Overt\", kernel='rbf', gamma=DEFAULT_GAMMA, C_range=DEFAULT_C_RANGE)\n            run_plots(overt_results_rbf, \"Overt\", \"RBF\", sensor_x, sensor_y)\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Imagined (RBF Kernel)\\n\" + \"=\"*30)\n        if X_img is not None:\n            img_results_rbf = perform_two_level_cv(X_img, y_img, data_label=\"Imagined\", kernel='rbf', gamma=DEFAULT_GAMMA, C_range=DEFAULT_C_RANGE)\n            run_plots(img_results_rbf, \"Imagined\", \"RBF\", sensor_x, sensor_y)\n\n    if RUN_POLY_KERNEL:\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Overt (Polynomial Kernel)\\n\" + \"=\"*30)\n        if X_overt is not None:\n            overt_results_poly = perform_two_level_cv(X_overt, y_overt, data_label=\"Overt\", kernel='poly',\n                                                     gamma=DEFAULT_GAMMA, degree=DEFAULT_DEGREE, C_range=DEFAULT_C_RANGE)\n            run_plots(overt_results_poly, \"Overt\", \"Polynomial\", sensor_x, sensor_y)\n\n        print(\"\\n\" + \"=\"*30 + \"\\n Scenario: Same-Train Imagined (Polynomial Kernel)\\n\" + \"=\"*30)\n        if X_img is not None:\n            img_results_poly = perform_two_level_cv(X_img, y_img, data_label=\"Imagined\", kernel='poly',\n                                                   gamma=DEFAULT_GAMMA, degree=DEFAULT_DEGREE, C_range=DEFAULT_C_RANGE)\n            run_plots(img_results_poly, \"Imagined\", \"Polynomial\", sensor_x, sensor_y)\n\n\n=== Starting BCI Analysis ===\nLoading sensor locations from: ../assets/BCIsensor_xy.csv\nSensor locations loaded. Shape: (102, 2)\nLoading data: ../assets/feaSubEOvert_1.csv (Label 0), ../assets/feaSubEOvert_2.csv (Label 1)\nData loaded. X shape: (240, 204), y shape: (240,)\nLoading data: ../assets/feaSubEImg_1.csv (Label 0), ../assets/feaSubEImg_2.csv (Label 1)\nData loaded. X shape: (240, 204), y shape: (240,)\n\n==============================\n Scenario: Same-Train Overt (Linear Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Overt Data (Kernel: linear) ---\n  Outer Fold 1/6\n    Stored actual coefficients for linear kernel (Fold 1).\n  Outer Fold 1 completed. Accuracy: 0.9500. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9750. Optimal C: 1. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.9750. Optimal C: 1. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9000. Optimal C: 0.001. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.9750. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.9500. Optimal C: 0.1. Time: 0.0s\n\nFinished 6-fold CV for Overt data (Kernel: linear).\nTotal CV Time: 0.3s\nOptimal C found per fold: [0.1, 1, 1, 0.001, 0.1, 0.1]\nAccuracy per fold: ['0.9500', '0.9750', '0.9750', '0.9000', '0.9750', '0.9500']\nAverage Accuracy: 0.9542 +/- 0.0267\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Imagined (Linear Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Imagined Data (Kernel: linear) ---\n  Outer Fold 1/6\n    Stored actual coefficients for linear kernel (Fold 1).\n  Outer Fold 1 completed. Accuracy: 0.8500. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9500. Optimal C: 0.01. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.8000. Optimal C: 0.01. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9250. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.8500. Optimal C: 0.1. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.9500. Optimal C: 0.1. Time: 0.1s\n\nFinished 6-fold CV for Imagined data (Kernel: linear).\nTotal CV Time: 0.5s\nOptimal C found per fold: [0.1, 0.01, 0.01, 0.1, 0.1, 0.1]\nAccuracy per fold: ['0.8500', '0.9500', '0.8000', '0.9250', '0.8500', '0.9500']\nAverage Accuracy: 0.8875 +/- 0.0573\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Cross-Train Overt -&gt; Imagined (Linear Kernel)\n==============================\n\n--- Cross-Training: Train on Overt, Test on Imagined (Kernel: linear) ---\n    Tuning hyperparameters (Kernel: linear) using 5-fold CV...\n    Best parameters found: {'C': 0.1} (Best Score: 0.9583)\n    Using parameters for final model: C=0.1, gamma=scale, degree=3\n    Training final model on entire training set...\n    Testing model on test set...\nCross-Training completed. Time: 1.3s\nTest Accuracy: 0.8917\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Cross-Train Imagined -&gt; Overt (Linear Kernel)\n==============================\n\n--- Cross-Training: Train on Imagined, Test on Overt (Kernel: linear) ---\n    Tuning hyperparameters (Kernel: linear) using 5-fold CV...\n    Best parameters found: {'C': 1} (Best Score: 0.9042)\n    Using parameters for final model: C=1, gamma=scale, degree=3\n    Training final model on entire training set...\n    Testing model on test set...\nCross-Training completed. Time: 0.1s\nTest Accuracy: 0.9125\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Overt (RBF Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Overt Data (Kernel: rbf) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for rbf kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 0.9750. Optimal C: 1. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9250. Optimal C: 10. Time: 0.2s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.9250. Optimal C: 10. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9250. Optimal C: 10. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.9750. Optimal C: 10. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.8750. Optimal C: 10. Time: 0.1s\n\nFinished 6-fold CV for Overt data (Kernel: rbf).\nTotal CV Time: 0.8s\nOptimal C found per fold: [1, 10, 10, 10, 10, 10]\nAccuracy per fold: ['0.9750', '0.9250', '0.9250', '0.9250', '0.9750', '0.8750']\nAverage Accuracy: 0.9333 +/- 0.0344\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Imagined (RBF Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Imagined Data (Kernel: rbf) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for rbf kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 0.8500. Optimal C: 10. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9250. Optimal C: 1. Time: 0.2s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.8000. Optimal C: 1. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.8500. Optimal C: 1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.8750. Optimal C: 10. Time: 0.1s\n\nFinished 6-fold CV for Imagined data (Kernel: rbf).\nTotal CV Time: 0.8s\nOptimal C found per fold: [10, 1, 1, 1, 10, 10]\nAccuracy per fold: ['0.8500', '0.9250', '0.8000', '0.8500', '0.8250', '0.8750']\nAverage Accuracy: 0.8542 +/- 0.0393\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Overt (Polynomial Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Overt Data (Kernel: poly) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for poly kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 1.0000. Optimal C: 10. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.9500. Optimal C: 1. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.9500. Optimal C: 100. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.9250. Optimal C: 1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.9500. Optimal C: 1. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.9250. Optimal C: 1. Time: 0.1s\n\nFinished 6-fold CV for Overt data (Kernel: poly).\nTotal CV Time: 0.6s\nOptimal C found per fold: [10, 1, 100, 1, 1, 1]\nAccuracy per fold: ['1.0000', '0.9500', '0.9500', '0.9250', '0.9500', '0.9250']\nAverage Accuracy: 0.9500 +/- 0.0250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==============================\n Scenario: Same-Train Imagined (Polynomial Kernel)\n==============================\n\n--- Starting 6-Fold Outer CV for Imagined Data (Kernel: poly) ---\n  Outer Fold 1/6\n    Attempting to approximate coefficients for poly kernel (Fold 1)...\n    Successfully approximated and stored coefficients.\n  Outer Fold 1 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n  Outer Fold 2/6\n  Outer Fold 2 completed. Accuracy: 0.8750. Optimal C: 10. Time: 0.1s\n  Outer Fold 3/6\n  Outer Fold 3 completed. Accuracy: 0.7750. Optimal C: 10. Time: 0.1s\n  Outer Fold 4/6\n  Outer Fold 4 completed. Accuracy: 0.7750. Optimal C: 1. Time: 0.1s\n  Outer Fold 5/6\n  Outer Fold 5 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n  Outer Fold 6/6\n  Outer Fold 6 completed. Accuracy: 0.8250. Optimal C: 10. Time: 0.1s\n\nFinished 6-fold CV for Imagined data (Kernel: poly).\nTotal CV Time: 0.6s\nOptimal C found per fold: [10, 10, 10, 1, 10, 10]\nAccuracy per fold: ['0.8250', '0.8750', '0.7750', '0.7750', '0.8250', '0.8250']\nAverage Accuracy: 0.8167 +/- 0.0344"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "",
    "text": "For decades now, researchers have been exploring brain-computer interfaces (BCIs) as a potential way for individuals with paralysis or other severe motor disabilities to interact with the world again. It’s an incredible field, but it comes with significant hurdles – everything from reliably picking up brain signals to accurately figuring out what those signals actually mean in terms of user intent.\n\nProject Goals: What we’re focusing on in this project is one specific piece of that puzzle: how can we classify electrical brain activity, measured non-invasively using electroencephalography (EEG), to understand someone’s intended movement? To do this, we employed a machine learning tool called a Support Vector Machine, or SVM, exploring different configurations (kernels) to see what works best.\nApproach Overview: Our main goal here was to see if we could determine, just by looking at EEG data, whether a person was intending to move their left hand or their right hand, even if they weren’t physically moving at all. The BCI system enabling this has two parts in our setup: the EEG system that captures the raw brain signals, and the SVM algorithm that interprets those signals and makes the left-vs-right classification.\nReport Scope: By the end of this discussion, we’ll have a clear picture of how well different SVM variants perform when applied to this EEG data for decoding movement intentions. We’ll look at the overall accuracy and ROC curves, compare linear, RBF, and polynomial kernels, and also look into some specific factors within the model and the data that influenced how well they worked.\n\n\n\nWhy is this so important? Well, advancements in BCIs could genuinely change lives for a vast number of people. For some perspective, the 2013 US Paralysis Prevalence & Health Disparities Survey found that nearly 5.4 million people were living with paralysis (Armour et al., 2016). Back then, that was almost 1.7% of the entire US population, and that number has likely only grown. Paralysis often brings profound challenges – the same survey noted that only about 15.5% of these individuals were employed, and over 30% were smokers. Given these severe impacts, technologies that can offer some relief or restore function are incredibly valuable for improving quality of life. BCIs stand out as one of the most promising paths forward to potentially restore movement capabilities and significantly enhance well-being for those affected.\n\n\n\n\n\nLet’s start with the basics. Our brain is constantly sending out electrical signals that orchestrate everything our body does. But this communication network relies on many biological components, and if any part breaks down, the connection can be lost. For people experiencing this, a Brain-Computer Interface (BCI) offers an alternative route, or a way for the brain’s commands to bypass the damaged pathways and still control external devices or even their own limbs (Nicolas-Alonso & Gomez-Gil, 2012). Ultimately, a BCI acts as a translator, converting brain activity directly into control signals. To make this happen, we need two key things: a way to ‘listen’ to the brain’s signals, and a way to make sense of them. In this project, we’re focusing on non-invasive BCIs that use Electroencephalography (EEG) to pick up those signals.\nEEG works by placing sensors on the scalp to detect the tiny electrical fields generated by brain activity. While it has the major advantage of being non-invasive (no surgery required!), the signals we get face some challenges. They’re incredibly complex, often buried in noise (from muscle twitches, eye blinks, even electrical interference from nearby devices), and they’re weakened as they pass through the skull and scalp. Plus, to get a good spatial picture of brain activity, we use many electrodes simultaneously, which results in very high-dimensional data – meaning each snapshot of brain activity has lots of different measurements to consider. Trying to reliably pull out a specific intention, like “move left” versus “move right,” from this noisy, high-dimensional stream requires robust analysis tools. That’s where machine learning comes into play.\n\n\n\nOkay, so how do we actually interpret these complex brain signals? That’s where Support Vector Machines, or SVMs, come in. SVMs are a type of supervised machine learning model that are good at classification tasks – essentially, sorting data into predefined categories. You train an SVM by showing it examples that are already labeled (like EEG snippets labeled as “intended left” or “intended right”). The SVM learns the patterns distinguishing these categories. Then, when you give it a new, unlabeled piece of data (a new EEG segment), it uses what it learned to predict which category that new data point belongs to. In our case, the SVM’s job is to look at the features extracted from an EEG signal and decide: “left intention” or “right intention”?\nSo, how does an SVM actually do this? Conceptually, it tries to find the “best” possible boundary to separate the data points belonging to different classes. Imagine you have a scatter plot with red dots and green dots. If the red dots are mostly in one area and the green dots in another, an SVM tries to draw a line (or, in higher dimensions, a plane or hyperplane) that separates them.\n\n\n\n\n\n\nFigure 1: SVM\n\n\n\nSee the plot on the right (in the imagined figure)? That blue line is the decision boundary found by the SVM. What makes SVMs special is how they find this boundary. They don’t just pick any line that separates the groups; they specifically look for the line that creates the largest possible “buffer zone” or margin between itself and the closest points from each class. These closest points, the ones right up against the edge of this buffer zone, are called the “support vectors” – they’re critical because they dictate exactly where the boundary and margin end up. This principle of maximizing the margin often helps the SVM generalize better, meaning it performs well not just on the data it was trained on, but also on new, unseen data. While the math behind it involves optimization, the core idea is finding this widest possible separation. We implement SVMs in our project using sklearn (“SVC,” n.d.).\nSVMs turn out to be perfect for the kind of EEG data we’re dealing with. EEG data, with signals from many electrodes, is naturally high-dimensional (204 dimensions in our case!). SVMs are known to handle high-dimensional spaces effectively, potentially better than some other algorithms that can suffer from the “curse of dimensionality.” They can also work well even when the number of training examples isn’t vastly larger than the number of features, which is relevant here. Plus, the focus on maximizing the margin can make the resulting classifier somewhat robust to noise, which is always a concern with EEG.\nAnd SVMs aren’t just for BCIs! They’re used in all sorts of areas, like classifying text (think spam email filters) (Mammone et al., 2009), recognizing faces in images (Guo et al., 2000), and even predicting things like seismic events (Hearst et al., 1998). So, it’s a versatile technology with broad applications, making its study valuable in many contexts."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "",
    "text": "Why is this so important? Well, advancements in BCIs could genuinely change lives for a vast number of people. For some perspective, the 2013 US Paralysis Prevalence & Health Disparities Survey found that nearly 5.4 million people were living with paralysis (Armour et al., 2016). Back then, that was almost 1.7% of the entire US population, and that number has likely only grown. Paralysis often brings profound challenges – the same survey noted that only about 15.5% of these individuals were employed, and over 30% were smokers. Given these severe impacts, technologies that can offer some relief or restore function are incredibly valuable for improving quality of life. BCIs stand out as one of the most promising paths forward to potentially restore movement capabilities and significantly enhance well-being for those affected."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "",
    "text": "Let’s start with the basics. Our brain is constantly sending out electrical signals that orchestrate everything our body does. But this communication network relies on many biological components, and if any part breaks down, the connection can be lost. For people experiencing this, a Brain-Computer Interface (BCI) offers an alternative route, or a way for the brain’s commands to bypass the damaged pathways and still control external devices or even their own limbs (Nicolas-Alonso & Gomez-Gil, 2012). Ultimately, a BCI acts as a translator, converting brain activity directly into control signals. To make this happen, we need two key things: a way to ‘listen’ to the brain’s signals, and a way to make sense of them. In this project, we’re focusing on non-invasive BCIs that use Electroencephalography (EEG) to pick up those signals.\nEEG works by placing sensors on the scalp to detect the tiny electrical fields generated by brain activity. While it has the major advantage of being non-invasive (no surgery required!), the signals we get face some challenges. They’re incredibly complex, often buried in noise (from muscle twitches, eye blinks, even electrical interference from nearby devices), and they’re weakened as they pass through the skull and scalp. Plus, to get a good spatial picture of brain activity, we use many electrodes simultaneously, which results in very high-dimensional data – meaning each snapshot of brain activity has lots of different measurements to consider. Trying to reliably pull out a specific intention, like “move left” versus “move right,” from this noisy, high-dimensional stream requires robust analysis tools. That’s where machine learning comes into play.\n\n\n\nOkay, so how do we actually interpret these complex brain signals? That’s where Support Vector Machines, or SVMs, come in. SVMs are a type of supervised machine learning model that are good at classification tasks – essentially, sorting data into predefined categories. You train an SVM by showing it examples that are already labeled (like EEG snippets labeled as “intended left” or “intended right”). The SVM learns the patterns distinguishing these categories. Then, when you give it a new, unlabeled piece of data (a new EEG segment), it uses what it learned to predict which category that new data point belongs to. In our case, the SVM’s job is to look at the features extracted from an EEG signal and decide: “left intention” or “right intention”?\nSo, how does an SVM actually do this? Conceptually, it tries to find the “best” possible boundary to separate the data points belonging to different classes. Imagine you have a scatter plot with red dots and green dots. If the red dots are mostly in one area and the green dots in another, an SVM tries to draw a line (or, in higher dimensions, a plane or hyperplane) that separates them.\n\n\n\n\n\n\nFigure 1: SVM\n\n\n\nSee the plot on the right (in the imagined figure)? That blue line is the decision boundary found by the SVM. What makes SVMs special is how they find this boundary. They don’t just pick any line that separates the groups; they specifically look for the line that creates the largest possible “buffer zone” or margin between itself and the closest points from each class. These closest points, the ones right up against the edge of this buffer zone, are called the “support vectors” – they’re critical because they dictate exactly where the boundary and margin end up. This principle of maximizing the margin often helps the SVM generalize better, meaning it performs well not just on the data it was trained on, but also on new, unseen data. While the math behind it involves optimization, the core idea is finding this widest possible separation. We implement SVMs in our project using sklearn (“SVC,” n.d.).\nSVMs turn out to be perfect for the kind of EEG data we’re dealing with. EEG data, with signals from many electrodes, is naturally high-dimensional (204 dimensions in our case!). SVMs are known to handle high-dimensional spaces effectively, potentially better than some other algorithms that can suffer from the “curse of dimensionality.” They can also work well even when the number of training examples isn’t vastly larger than the number of features, which is relevant here. Plus, the focus on maximizing the margin can make the resulting classifier somewhat robust to noise, which is always a concern with EEG.\nAnd SVMs aren’t just for BCIs! They’re used in all sorts of areas, like classifying text (think spam email filters) (Mammone et al., 2009), recognizing faces in images (Guo et al., 2000), and even predicting things like seismic events (Hearst et al., 1998). So, it’s a versatile technology with broad applications, making its study valuable in many contexts."
  },
  {
    "objectID": "index.html#data-acquisition",
    "href": "index.html#data-acquisition",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nThe foundation for everything is the EEG data. This data came from a single human subject.\n\nEEG Setup: The recording setup used 102 electrodes placed across the scalp – you can see the layout in the plot below (each blue dot is an electrode). Each electrode provided two pieces of information related to the local electrical field gradient (one for the x-direction, one for the y-direction). So, for every moment in time or trial we recorded, we ended up with 204 distinct data channels or features.\n\n\n\n\n\n\n\nFigure 2: Electrode Positioning\n\n\n\n\nExperimental Conditions: The data was collected under two different conditions:\n\nOvert Movement: In this case, the subject physically moved their left or right arm. We’d expect the brain signals during actual movement to be relatively strong and potentially easier to classify.\nImagined Movement: Here, the subject simply thought about moving their left or right arm, but stayed still. These signals are usually much fainter and harder to detect, but they’re crucial for BCIs intended for people who can’t physically move.\n\nData Structure: For both the Overt and Imagined conditions, we have a dataset containing 120 trials labeled “movement 1” and 120 trials labeled “movement 2”. Now, for this specific dataset, we don’t actually know which label corresponds to “left” and which to “right,” but for classification purposes, that doesn’t matter – we just need to distinguish between the two types. This gives us 240 trials in total for each condition (Overt and Imagined). Each single trial is represented by that 204-dimensional feature vector we talked about (the readings from all 204 data channels). Our task is essentially a binary classification problem: given a 204-dimensional vector, decide if it belongs to “movement 1” or “movement 2”."
  },
  {
    "objectID": "index.html#classification-with-support-vector-machines-svms",
    "href": "index.html#classification-with-support-vector-machines-svms",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.2 Classification with Support Vector Machines (SVMs)",
    "text": "2.2 Classification with Support Vector Machines (SVMs)\nNow, before we jump into the results, let’s look more precisely what the SVM is doing under the hood. Remember, its core job is to find that optimal boundary (a hyperplane in our 204-dimensional space) that best separates the EEG patterns corresponding to “Movement 1” from those corresponding to “Movement 2”.\n\n2.2.1 Mathematical Formulation\nIf we have \\(N\\) training trials, each with a feature vector \\(x_i\\) (our 204 EEG measurements) and a known class label \\(y_i\\) (which we’ll represent as either -1 for Movement 1 or +1 for Movement 2), a linear SVM tries to find a weight vector \\(w\\) and a bias term \\(b\\). These define the separating hyperplane with the equation \\(w^T x + b = 0\\).\nBut what if the data isn’t perfectly separable with a straight line (or flat plane)? What if some “Movement 1” points are mixed in with “Movement 2” points? That’s where the soft-margin SVM comes in handy. Instead of insisting on perfect separation, it tries to find a balance: maximize the margin (the buffer zone) while also minimizing the number of points that end up on the wrong side of the boundary or inside the margin. This is done by solving an optimization problem (Hearst et al., 1998):\n\\[\n\\min_{w, b, \\xi} \\frac{1}{2} w^T w + C \\sum_{i=1}^{N} \\xi_i\n\\]\nsubject to the constraints:\n\\[\ny_i (w^T x_i + b) \\ge 1 - \\xi_i, \\quad \\text{for } i = 1, \\dots, N\n\\] \\[\n\\xi_i \\ge 0, \\quad \\text{for } i = 1, \\dots, N\n\\]\nBreaking down these terms:\n\n\\(w\\): The weight vector. It’s perpendicular to the separating boundary. Minimizing its squared length (\\(\\frac{1}{2} w^T w\\)) is equivalent to maximizing the margin width.\n\\(b\\): The bias term. It shifts the boundary position without changing its orientation.\n\\(x_i\\): The 204-dimensional feature vector for the \\(i\\)-th trial.\n\\(y_i\\): The class label (+1 or -1) for the \\(i\\)-th trial.\n\\(\\xi_i\\): These are the slack variables. Think of them as measuring how much a data point \\(i\\) “violates” the margin. If \\(\\xi_i = 0\\), the point is correctly classified and outside the margin. If it’s between 0 and 1, it’s correctly classified but inside the margin. If \\(\\xi_i &gt; 1\\), the point is actually misclassified.\n\\(C\\): This is the regularization parameter. (It’s equivalent to \\(1/\\alpha\\) in our class notes, and is what sklearn uses.) \\(C\\) controls the trade-off:\n\nA large \\(C\\) puts a heavy penalty on misclassifications (large \\(\\xi_i\\)). The SVM will try very hard to classify all training points correctly, possibly leading to a narrower margin and potentially “overfitting” the training data (meaning it might not generalize well to new data).\nA small \\(C\\) is more tolerant of misclassifications. It allows for a potentially wider margin, even if some training points end up on the wrong side or within it. This might lead to better generalization but risks “underfitting” if the margin becomes too wide and ignores the underlying structure. Finding the right value for \\(C\\) (and potentially other parameters like \\(\\gamma\\) or \\(d\\) for non-linear kernels) is key to getting good performance, and we use a process called cross-validation (which we’ll discuss next) to do this.\n\n\nWhy this works (Convex Optimization):\nThis mathematical setup is what’s known as a convex optimization problem. This matters because for convex problems, we’re guaranteed that any solution we find that looks like the best locally is the globally best solution. There’s no risk of getting stuck in a suboptimal valley. The objective function (the part we’re minimizing) and the constraints (the rules we must follow) are all convex (they curve upwards like a bowl, mathematically speaking), which makes the whole problem convex (Hearst et al., 1998). This mathematical property is a big reason why SVMs are so reliable and widely used – we know we can find the single best boundary according to our criteria (at least for the linear case and standard formulations).\n\n\n2.2.2 Handling Non-Linearity: The Kernel Trick\nThe formulation we just looked at finds a linear boundary (a flat plane). But what if the real distinction between “Movement 1” and “Movement 2” in our EEG data follows a more complex, curved pattern? SVMs have a way to handle this known as the “kernel trick.”\nThe basic idea is to imagine mapping our original 204-dimensional data into an even higher-dimensional space (Weiße et al., 2006). In this new, much more complex space, the data might magically become linearly separable again. The “trick” is that we don’t actually have to compute the coordinates in this potentially massive new space. Instead, we use kernel functions \\(K(x_i, x_j)\\). These functions directly calculate what the dot product would be between the mapped vectors \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\) in that high-dimensional space (\\(K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\\)). Since the math behind the SVM solution relies heavily on these dot products, we can just swap out the simple dot product \\(x_i^T x_j\\) with our chosen kernel function \\(K(x_i, x_j)\\). This lets us effectively find a non-linear boundary in the original space without the excess computation of actually working in the super-high-dimensional mapped space.\nCommon Kernels: In this project, we explored three common kernel options:\n\nLinear Kernel: \\(K(x_i, x_j) = x_i^T x_j\\). This is our baseline, assuming a linear separation is sufficient.\nPolynomial Kernel: \\(K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d\\). This allows for curved boundaries defined by polynomials. It introduces new hyperparameters like the degree \\(d\\), a scaling factor \\(\\gamma\\), and a coefficient \\(r\\) that need tuning.\nRadial Basis Function (RBF) Kernel (or Gaussian Kernel): \\(K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)\\). This is a very popular kernel capable of creating highly complex decision boundaries. It uses a hyperparameter \\(\\gamma\\) that controls the “reach” or influence of each training point. Both \\(C\\) and \\(\\gamma\\) need to be tuned (Han et al., 2012)."
  },
  {
    "objectID": "index.html#model-evaluation-two-level-cross-validation",
    "href": "index.html#model-evaluation-two-level-cross-validation",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.3 Model Evaluation: Two-Level Cross-Validation",
    "text": "2.3 Model Evaluation: Two-Level Cross-Validation\nNow we have our SVM model variants (linear, RBF, polynomial). But how do we know which one is actually best for our problem? And how do we choose the optimal values for their hyperparameters (like \\(C\\) for all, \\(\\gamma\\) for RBF/Poly, \\(d\\) for Poly)? We need a reliable way to estimate how well each classifier configuration will perform on new, unseen data.\nAs discussed before, just tuning hyperparameters and evaluating on the same cross-validation splits can lead to inflated performance estimates. To avoid this and get a truly unbiased evaluation while also performing hyperparameter tuning, we rely on two-level cross-validation (Cross-Validation of Component Models, n.d.).\nThe Process:\n\nOuter Loop (Performance Estimation): Divides the data into \\(k_{outer}\\) folds (here, 6). Each fold serves once as the final, held-out test set.\nInner Loop (Hyperparameter Tuning): Operates only on the \\(k_{outer}-1\\) folds designated as the outer training set for that round. It performs its own cross-validation (using \\(k_{inner}=5\\) folds) to find the best hyperparameters (e.g., best combination of \\(C\\) and \\(\\gamma\\) for RBF, or \\(C\\), \\(\\gamma\\), \\(d\\), \\(r\\) for Poly) without seeing the outer test fold.\nTraining and Testing: Once the inner loop selects the best hyperparameters for that outer fold, a new SVM model is trained with those parameters on the entire outer training set. This model is then evaluated once on the held-out outer test set.\n\nThis entire process is repeated for all \\(k_{outer}\\) outer folds, and the final performance metrics (accuracy, AUC) are averaged across the results from these outer test sets. This rigorous approach ensures that our hyperparameter tuning doesn’t bias our final performance evaluation, giving us a fair comparison between the different kernel types and a reliable estimate of their true generalization ability. We applied this entire two-level CV procedure separately for the linear, RBF, and polynomial SVMs."
  },
  {
    "objectID": "index.html#formulating-the-classification-problem",
    "href": "index.html#formulating-the-classification-problem",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "2.4 Formulating the Classification Problem",
    "text": "2.4 Formulating the Classification Problem\nExplicitly defining the machine learning problem we’re solving here:\n\nInput Features: For each trial (either an overt or imagined movement), the input to our classifier is a single vector \\(x \\in \\mathbb{R}^{204}\\). This vector represents the measurements from the 102 electrodes, with each electrode contributing two gradient values (x and y direction), resulting in 204 features per trial. These features are assumed to capture the spatial and electrical characteristics of the brain activity during that trial.\nTarget Classes: The output we want to predict is a binary class label, \\(y \\in \\{\\text{\"Movement 1\"}, \\text{\"Movement 2\"}\\}\\). As mentioned, the dataset labels these generically, corresponding to the intention to move either the left or right hand (mapping unknown). Our goal is to distinguish between these two distinct intentions based on the EEG features.\nObjective: The objective is to learn a classification function \\(f: \\mathbb{R}^{204} \\rightarrow \\{\\text{\"Movement 1\"}, \\text{\"Movement 2\"}\\}\\) using a Support Vector Machine. We aim to find the function \\(f\\) (defined by the SVM’s parameters, including the kernel type and its associated hyperparameters like \\(C\\), \\(\\gamma\\), \\(d\\)) that minimizes classification errors on unseen data. The two-level cross-validation procedure is our method for estimating this generalization performance and for selecting the optimal hyperparameters for each kernel type based on the training data."
  },
  {
    "objectID": "index.html#linear-kernel-classification-performance",
    "href": "index.html#linear-kernel-classification-performance",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.1 Linear Kernel Classification Performance",
    "text": "3.1 Linear Kernel Classification Performance\nWe’ll start with the baseline linear SVM.\n\n3.1.1 Same-Train Scenarios (Linear SVM)\n\nROC Curves: Figures 1 and 2 show the ROC curves for the linear SVM when trained and tested on Overt and Imagined data, respectively.\n\n\n\n\n\n\nFigure 3: Individual Fold and Overall ROC Curves for Same-Train Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 4: Individual Fold and Overall ROC Curves for Same-Train Imagined (Linear SVM)\n\n\n\nInterpretation: As previously discussed, the linear SVM performed better on Overt data (Avg AUC in Fig. 1) than Imagined data (Avg AUC in Fig. 2), likely due to stronger signals. Performance was reasonably consistent across folds for Overt, but more variable for Imagined.\nAccuracy: Tables 1 and 2 show the fold-by-fold accuracy and optimal \\(C\\) values.\nTable 1: Per-Fold Accuracy Results for Same-Train Overt (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.9500\n0.1\n\n\n2/6\n0.9750\n1\n\n\n3/6\n0.9750\n1\n\n\n4/6\n0.9000\n0.001\n\n\n5/6\n0.9750\n0.1\n\n\n6/6\n0.9500\n0.1\n\n\nAvg\n0.9542\n\n\n\nStd Dev\n0.0264\n\n\n\n\nTable 2: Per-Fold Accuracy Results for Same-Train Imagined (Linear SVM)\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.8500\n0.1\n\n\n2/6\n0.9500\n0.01\n\n\n3/6\n0.8000\n0.01\n\n\n4/6\n0.9250\n0.1\n\n\n5/6\n0.8500\n0.1\n\n\n6/6\n0.9500\n0.1\n\n\nAvg\n0.8875\n\n\n\nStd Dev\n0.0580\n\n\n\n\nInterpretation: Average accuracy was ~95.4% for Overt and ~88.8% for Imagined, confirming the ROC trend. Variability across folds was higher for Imagined data.\n\n\n\n3.1.2 Cross-Train Scenarios (Linear SVM)\n\nROC Curves: Figures 3 and 4 show the ROC curves for the cross-training scenarios using the linear SVM.\n\n\n\n\n\n\nFigure 5: ROC Curve for Cross-Training: Train Imagined -&gt; Test Overt (Linear SVM)\n\n\n\n\n\n\n\n\n\nFigure 6: ROC Curve for Cross-Training: Train Overt -&gt; Test Imagined (Linear SVM)\n\n\n\nComparison: As noted before, training on Imagined and testing on Overt (Fig 3, ~91.3% Acc, AUC 0.97) performed slightly better than training on Overt and testing on Imagined (Fig 4, ~89.2% Acc, AUC 0.96). Both fell between the same-train performances, suggesting some shared patterns but also distinct differences between the conditions.\n\n\n\n3.1.3 Feature Importance (Linear SVM Weights)\n\nSpatial and Stem Plots: Figures 5-8 visualize the weights for the linear SVM.\n   \nInterpretation: The weights highlighted channels primarily over sensorimotor areas, consistent with neuroscience. The primary motor cortex lies in the middle of the brain, which is the most heavily weighted part. There was significant overlap in the most important channels (e.g., 128, 136, 140) between Overt and Imagined conditions, supporting the idea of shared neural substrates."
  },
  {
    "objectID": "index.html#rbf-kernel-classification-performance",
    "href": "index.html#rbf-kernel-classification-performance",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.2 RBF Kernel Classification Performance",
    "text": "3.2 RBF Kernel Classification Performance\nNow let’s consider the performance when using the Radial Basis Function (RBF) kernel, which allows for more complex, non-linear decision boundaries. The hyperparameters \\(C\\) and \\(\\gamma\\) were tuned simultaneously in the inner loop of the two-level CV.\n\n3.2.1 Same-Train Scenarios (RBF SVM)\n\nROC Curves: Let’s assume Figures 9 and 10 show the ROC curves for the RBF SVM on Overt and Imagined data, respectively.\n \nInterpretation: The average AUC for the Overt data (Figure 9) is approximately 0.99, suggesting excellent performance, slightly higher than the linear kernel’s average. For the Imagined data (Figure 10), the average AUC is around 0.94, which is also higher than the linear kernel’s average (~0.92 based on visual inspection of Fig 2), indicating the RBF kernel might be capturing useful non-linearities, especially in the more challenging Imagined condition. Variability between folds still seems present, particularly for Imagined data.\nAccuracy: Similarly, assume Tables 3 and 4 present the accuracy results and the optimal \\((C, \\gamma)\\) pairs chosen for each fold.\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.9750\n1\n\n\n2/6\n0.9250\n10\n\n\n3/6\n0.9250\n10\n\n\n4/6\n0.9250\n10\n\n\n5/6\n0.9750\n10\n\n\n6/6\n0.8750\n10\n\n\nAvg\n0.9333\n\n\n\nStd Dev\n0.0354\n\n\n\n\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.8500\n10\n\n\n2/6\n0.9250\n1\n\n\n3/6\n0.8000\n1\n\n\n4/6\n0.8500\n1\n\n\n5/6\n0.8250\n10\n\n\n6/6\n0.8750\n10\n\n\nAvg\n0.8542\n\n\n\nStd Dev\n0.0417\n\n\n\n\nInterpretation: Comparing the average accuracies, the RBF kernel achieved ~93.3% on Overt data (Table 3) and ~85.4% on Imagined data (Table 4). Interestingly, these average accuracies are slightly lower than those achieved by the linear kernel (95.4% Overt, 88.8% Imagined). This contrasts with the AUC results and suggests that while the RBF kernel might offer better separation overall (higher AUC), the optimal decision threshold for maximizing accuracy might be different, or the added complexity might slightly hurt performance at the standard threshold, perhaps due to overfitting on some folds despite cross-validation. The optimal \\(C\\) values tend to be higher (1 or 10) than often seen for the linear kernel, potentially indicating the need for a tighter fit when using the flexible RBF kernel.\n\n\n\n3.2.2 Feature Importance (RBF SVM)\n \n\nWeight Interpretation: Visualizing feature importance for non-linear kernels like RBF isn’t as straightforward as plotting the weights (\\(w\\)) in the linear case. With a linear SVM, the weights directly correspond to the importance of a feature, but the same association doesn’t exist with different kernels. Therefore, we look at the performance metrics and also use approximations to map weights back to support vectors. However, these heatmaps still show a general distribution of “weights” in the same locations as the linear SVM."
  },
  {
    "objectID": "index.html#polynomial-kernel-classification-performance",
    "href": "index.html#polynomial-kernel-classification-performance",
    "title": "Decoding Movement Intentions with Brain-Computer Interfaces",
    "section": "3.3 Polynomial Kernel Classification Performance",
    "text": "3.3 Polynomial Kernel Classification Performance\nFinally, let’s consider the Polynomial kernel. This kernel also allows for non-linear boundaries, with complexity controlled by the polynomial degree \\(d\\) (along with hyperparameters \\(C\\), \\(\\gamma\\), and \\(r\\), all tuned via the inner CV loop).\n\n3.3.1 Same-Train Scenarios (Polynomial SVM)\n\nROC Curves: Assume Figures 13 and 14 show the ROC curves for the Polynomial SVM on Overt and Imagined data.\n \nInterpretation: The average AUC for the Overt data (Figure 13) is approximately 0.98, slightly below the RBF kernel but comparable to the linear kernel. However, for the Imagined data (Figure 14), the average AUC drops significantly to around 0.89, which is much worse than both the linear (~0.92) and RBF (~0.94) kernels. This suggests the polynomial kernel might be struggling or potentially overfitting on the more difficult Imagined dataset.\nAccuracy: Assume Tables 5 and 6 show the accuracy results and the optimal hyperparameter sets (\\(C, \\gamma, d, r\\)) for each fold.\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n1.0000\n10\n\n\n2/6\n0.9500\n1\n\n\n3/6\n0.9500\n100\n\n\n4/6\n0.9250\n1\n\n\n5/6\n0.9500\n1\n\n\n6/6\n0.9250\n1\n\n\nAvg\n0.9508\n\n\n\nStd Dev\n0.0252\n\n\n\n\n\n\n\nFold\nAccuracy\nOptimal C\n\n\n\n\n1/6\n0.8250\n10\n\n\n2/6\n0.8750\n10\n\n\n3/6\n0.7750\n10\n\n\n4/6\n0.7750\n1\n\n\n5/6\n0.8250\n10\n\n\n6/6\n0.8250\n10\n\n\nAvg\n0.8167\n\n\n\nStd Dev\n0.0366\n\n\n\n\nInterpretation: The average accuracy for the Polynomial kernel is ~95.1% on Overt data (Table 5), which is very close to the linear kernel’s performance. However, on the Imagined data (Table 6), the average accuracy drops to ~81.7%, noticeably lower than both the linear (~88.8%) and RBF (~85.4%) kernels. This confirms the trend seen in the AUCs – the polynomial kernel seems less effective for the imagined movement data in this experiment. Optimal \\(C\\) values varied widely, including a large value of 100 in one Overt fold.\n \nInterpretation: Similar as all the other heatmaps, we see the weights concentrated in the motor cortices."
  }
]