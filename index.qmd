<!-- ---
title: "Brain-Computer Interface Movement Decoding"
author: "Ray Hoggard"
affiliation: "Duke University" 
format: html 
toc: true
toc-depth: 3 
abstract: |  # Use '|' for multi-line abstracts
  This paper explores methods for decoding movement intentions from brain signals using Brain-Computer Interfaces (BCIs). We investigate common signal processing techniques and machine learning models applied to electroencephalography (EEG) data. The goal is to develop a robust system capable of translating neural activity into control signals for assistive devices. Preliminary results show promising accuracy using a combination of feature extraction and support vector machines.
bibliography: references.bib
csl: apa.csl # APA style
---

# Introduction

Welcome to my project. Here's an equation: $f(x) = w^T x + b$.

## Background

More details about the background...

# Methods

Describe the methods used...

## Data Acquisition

Details on how data was acquired...

## Signal Processing

Details on signal processing steps...

Hello, this is stuff...

# Results

## Classification Performance

Our movement intention decoding approach achieved promising results across multiple subjects ([@hearst_support_1998]). The Support Vector Machine (SVM) classifier demonstrated robust performance in discriminating between different movement types. Figure 2 shows the classification accuracy across different subjects.

## Feature Importance

Analysis of feature importance revealed that temporal features in the alpha and beta frequency bands were most informative for movement decoding. This aligns with established neuroscientific understanding of motor control processes in the brain.

# Discussion

The results demonstrate the feasibility of decoding movement intentions from EEG signals using machine learning approaches. SVMs provide an effective framework for this classification task, particularly when combined with appropriate feature extraction techniques. However, several limitations should be considered:

1. Inter-subject variability remains a significant challenge
2. Real-time implementation requires further optimization
3. The current approach may not generalize well to novel movement types

## Comparison with Previous Work

Our approach builds upon previous BCI research while addressing several key limitations. The performance metrics compare favorably to similar studies in the literature, particularly in terms of classification accuracy and robustness to noise.

# Conclusion

This study demonstrates the potential of machine learning approaches for decoding movement intentions from brain signals. Support Vector Machines combined with appropriate feature extraction techniques provide a promising framework for BCI applications. Future work should focus on:

1. Improving real-time performance
2. Reducing calibration requirements
3. Exploring deep learning approaches for end-to-end movement decoding
4. Testing with individuals who have motor impairments

The findings have important implications for the development of assistive technologies and neural rehabilitation systems.

# References

::: {#refs}
::: -->

---
title: "Brain-Computer Interface Movement Decoding"
author: "Ray Hoggard"
affiliation: "Duke University" 
format: html 
toc: true
toc-depth: 3 
abstract: | 
  This paper presents a way to classify output from BCIs, distinguishing between left and right arm movements using brain signals
  recorded via EEG. We explore Support Vector Machines (SVMs), a machine learning tool that helps us classify these recorded signals,
  and correctly determine the person's intent. Results show that...
bibliography: references.bib
csl: apa.csl # APA style
---

# Introduction

Have you ever thought about how your brain tells your arm to move? It's a complex process involving electrical signals. Now, imagine trying to capture those signals from outside the head using sensors and teaching a computer to understand the *intention* to move, even if the movement doesn't happen. That's the core challenge of this project.

Brain-Computer Interfaces (BCIs) allow us to peer into someone's head and make guesses at what they're trying to do, even if nothing physically happens. In this project, we focus specifically on decoding movement intentions from electroencephalography (EEG) data. 

## Motivation

x number of people have no arms, growing #?, outcome data here, then add how much better things get once they have arms again. BCIs are an integral part of this, since they have arms now. Add a bunch of sources:...

## Background

So, how does a BCI work? At its heart, it's a bridge between the brain and an external device [@emotiv_bci_guide_nodate]. Our brains are constantly buzzing with electrical activity generated by neurons firing [@emotiv_bci_guide_nodate]. EEG uses sensors placed on the scalp (non-invasively!) to pick up these tiny electrical signals [@emotiv_bci_guide_nodate; @wikipedia_bci; @uts_bci_landscape_2025]. Think of it like eavesdropping on the brain's conversations.

The challenge, however, is that these signals are complex, often noisy, and recorded from the scalp, which is quite a distance from the neurons themselves [@uts_bci_landscape_2025]. Furthermore, we're dealing with a *lot* of data – signals from many electrodes recorded over time, resulting in high-dimensional datasets[cite: 152]. This is where machine learning comes to the rescue.

We need algorithms capable of sifting through this high-dimensional data to find the subtle patterns that indicate a specific intention, like moving the left hand versus the right hand. This project focuses on using SVMs, a type of classifier known for its effectiveness in handling high-dimensional data, even with relatively few training examples – a common scenario in BCI research [@hearst_support_1998; @geeksforgeeks_svm_2025; @analytics_vidhya_svm_2021]. The ultimate goal is to create a system that translates these detected brain patterns into reliable control signals for computers, wheelchairs, or prosthetic devices [@emotiv_bci_guide_nodate; @frontiers_bci_prosthesis_2017; @pubmed_eeg_bci_overview_2010].

# Methods

Alright, how do we actually go about teaching a computer to read minds (or at least, movement intentions)? Our approach involves a few key steps: acquiring the data, processing it, and then using a machine learning model to classify it.

## Data Acquisition

First, we need the raw material: brain signals. The data for this project comes from EEG recordings of a human subject[cite: 3]. EEG measures electrical activity using electrodes placed on the scalp [@emotiv_bci_guide_nodate]. In our case, 102 electrodes were used, each providing two channels of information related to the electrical field gradient (one for the x-direction, one for the y-direction), giving us a total of 204 data channels per time point or trial[cite: 168].

Crucially, the data was collected under two different conditions[cite: 3, 155]:
1.  **Overt Movement:** The subject physically moved their left or right arm. We expect these signals to be stronger and potentially easier to classify[cite: 156].
2.  **Imagined Movement:** The subject *imagined* moving their left or right arm but remained still. These signals are typically weaker but are highly relevant for BCIs designed for individuals who cannot physically move[cite: 156].

For each condition, we have 120 trials for "movement 1" (which corresponds to either left or right, the mapping is unknown [cite: 4]) and 120 trials for "movement 2" (the opposite direction), giving us 240 trials per condition[cite: 176, 191]. Each trial is represented by a 204-dimensional vector[cite: 176].

*[Visualization: Electrode locations on the scalp (similar to image from 171)]*

## Signal Processing

Raw EEG data can be messy. Before feeding it to our classifier, some pre-processing is typically needed, although the specifics aren't the focus of *this* report. Generally, this involves techniques to reduce noise and artifacts (like blinks or muscle movements) and potentially extract features that are more informative for classification. For this project, we're using the provided feature vectors directly.

## Classification with Support Vector Machines (SVMs)

This is where the machine learning magic happens! We're using Support Vector Machines (SVMs) as our classifier. Why SVMs?

1.  **Good with High Dimensions:** As mentioned, EEG data is high-dimensional (204 features!), and SVMs are known to handle this well, avoiding the "curse of dimensionality" better than some other methods [@hearst_support_1998; @analytics_vidhya_svm_2021; @geeksforgeeks_svm_2025].
2.  **Finding the Best Boundary:** The core idea of SVM is to find the "best" dividing line (or hyperplane in higher dimensions) that separates the data points belonging to different classes (e.g., "left movement" vs. "right movement") [@quantstart_svm_guide; @geeksforgeeks_svm_2025; @datacamp_svm_tutorial_2019]. "Best" usually means the hyperplane that has the maximum possible margin or gap between itself and the closest data points (called support vectors) from each class [@svm_classifier_tutorial_kaggle; @geeksforgeeks_svm_2025; @analytics_vidhya_svm_2021; @datacamp_svm_tutorial_2019]. This maximization of the margin often leads to better generalization to new, unseen data [@jakkula_svm_tutorial].

*[Visualization: Simple 2D example of an SVM hyperplane, margin, and support vectors (similar to image from 179 or descriptions in [@quantstart_svm_guide])]*

**Our SVM Setup:**
* **Linear Kernel:** We'll start with a linear SVM as our baseline[cite: 16]. This assumes the data can be separated by a straight line (or flat plane). We might explore other, non-linear kernels later[cite: 16, 179].
* **Decision Statistic:** Instead of just outputting a hard decision ("left" or "right"), we need our SVM implementation to output a continuous decision statistic (often related to the distance from the separating hyperplane)[cite: 11, 143]. This is crucial for generating Receiver Operating Characteristic (ROC) curves, which help us evaluate performance across different decision thresholds.
* **Regularization:** SVMs involve an optimization process that often includes a regularization parameter (let's call it $\alpha$)[cite: 180]. This parameter helps prevent overfitting by controlling the trade-off between maximizing the margin and minimizing classification errors on the training data. Finding the right $\alpha$ is key[cite: 181].
* **Toolboxes:** We're allowed to use existing SVM libraries in Python or Matlab, but we need to understand how they work and cite them properly[cite: 6, 8]. For example, using Scikit-learn's SVC implementation [@noauthor_svc_nodate].

## Model Evaluation: Two-Level Cross-Validation

How do we know how well our SVM classifier performs and how do we choose the best regularization parameter ($\alpha$)? We use a technique called **two-level cross-validation**[cite: 15]. This might sound complex, but it's a robust way to get reliable performance estimates and tune parameters simultaneously.

Here's the gist:
1.  **Outer Level (1st Level):** We split the entire dataset (e.g., all 240 Overt trials) into several folds (6 folds in this project [cite: 191]). We iterate 6 times. In each iteration, we use 5 folds (200 trials) for training and 1 fold (40 trials) for testing[cite: 192]. This gives us 6 different performance estimates.
2.  **Inner Level (2nd Level):** *Before* training the SVM in the outer loop, we need to find the best $\alpha$ for that specific training set (the 200 trials). To do this, we perform *another* cross-validation *within* those 200 trials[cite: 187]. We use 5-fold cross-validation here[cite: 193]. We split the 200 trials into 5 inner folds (160 for inner training, 40 for inner testing)[cite: 193]. We try different values of $\alpha$ (e.g., 0.01, 1, 100, 10000 [cite: 193]), train an SVM on the 160 trials, test on the 40, and repeat for all 5 inner folds. The $\alpha$ that gives the best average performance (e.g., accuracy [cite: 192]) across these inner folds is chosen.
3.  **Back to Outer Level:** Now, using the best $\alpha$ found in the inner loop, we train the final SVM for this outer fold on *all* 200 training trials and test it on the held-out 40 trials[cite: 189].
4.  **Repeat:** We repeat steps 2 and 3 for all 6 outer folds.

This process ensures that our parameter tuning ($\alpha$ selection) doesn't "see" the final test data for that fold, giving us a less biased estimate of how well the model will generalize. We evaluate performance using ROC curves and accuracy[cite: 18, 110].

*[Visualization: Diagram illustrating two-level cross-validation (similar to image from 186)]*

# Results

Now, let's look at what we expect to find when we run these methods on our EEG data. We'll analyze four main scenarios as required[cite: 17, 20]:

1.  **Same-Train: Overt Movement:** Train and test using only the Overt movement data.
2.  **Same-Train: Imagined Movement:** Train and test using only the Imagined movement data.
3.  **Cross-Train: Overt to Imagined:** Train using Overt data, test using Imagined data.
4.  **Cross-Train: Imagined to Overt:** Train using Imagined data, test using Overt data.

## Classification Performance

For the "Same-Train" scenarios, we'll look at the cross-validated performance[cite: 18]. We expect the SVM classifier to distinguish between the two movement types reasonably well. The original document mentions promising results with SVMs for similar tasks [@hearst_support_1998]. We'll present performance using:

* **ROC Curves:** These plots show the trade-off between correctly identifying one class (true positive rate) and incorrectly identifying the other class (false positive rate) across different decision thresholds. We'll show the ROC curve for each cross-validation fold and the average ROC curve[cite: 110, 115, 117].
* **Accuracy:** The overall percentage of trials classified correctly, typically calculated at a specific threshold (e.g., $\beta=0$). We'll report accuracy for each fold and the average cross-validated accuracy[cite: 110, 116, 118].

*[Visualization: Combined plot showing 6 individual-fold ROC curves and the average ROC curve for the Same-Train Overt scenario]*
*[Visualization: Combined plot showing 6 individual-fold ROC curves and the average ROC curve for the Same-Train Imagined scenario]*

For the "Cross-Train" scenarios, we'll train the SVM on the entire training dataset (using the best $\alpha$ found via cross-validation on that training set) and then evaluate it on the *other* dataset type[cite: 19]. We'll present ROC curves and accuracy for these cases as well.

*[Visualization: Plot comparing ROC curves for Overt-to-Imagined and Imagined-to-Overt cross-training scenarios]*

We anticipate differences in performance:
* Overt data might yield better results than Imagined data due to stronger signals[cite: 156].
* Cross-training performance might be lower than same-training performance, indicating differences between overt and imagined brain activity patterns.

## Feature (Channel) Importance

SVMs don't just classify; they also provide insights into which features (EEG channels, in our case) are most important for making the classification decision. The SVM calculates a weight for each channel[cite: 184]. Channels with larger magnitude weights (positive or negative) contribute more significantly to the decision boundary[cite: 184].

We'll analyze these weights:
* **Spatial Distribution:** We can visualize the magnitude of these weights across the scalp to see which brain regions seem most informative for distinguishing left vs. right movements[cite: 80, 112, 113]. This might align with known brain areas involved in motor control.
* **Top Channels:** We'll identify the specific channels with the largest weights (e.g., top 6)[cite: 112, 113].

*[Visualization: Brain surface plot showing SVM channel weight magnitudes for an example fold (Overt data) (similar to image from 183)]*
*[Visualization: Stem plot showing signed SVM weights for all 204 channels for an example fold (Overt data), highlighting the top 6]*
*[Visualization: Brain surface plot showing SVM channel weight magnitudes for an example fold (Imagined data)]*
*[Visualization: Stem plot showing signed SVM weights for all 204 channels for an example fold (Imagined data), highlighting the top 6]*

Comparing the weight patterns between Overt and Imagined conditions might also reveal interesting differences in brain activity.

# Discussion

So, what does it all mean? Our results, based on the outlined analysis plan, should demonstrate the potential of using SVMs to decode movement intentions from EEG signals. It shows that machine learning can, indeed, learn to interpret these complex brain patterns.

**Key Takeaways (Expected):**
* **SVM Effectiveness:** SVMs, particularly with a linear kernel as a baseline, appear to be a viable approach for this BCI classification task [@hearst_support_1998].
* **Overt vs. Imagined:** We likely see better performance when training and testing on Overt movement data compared to Imagined movement data, probably because the signals are stronger and clearer[cite: 156]. The cross-validation results (individual fold consistency) might also be more stable for Overt data.
* **Cross-Domain Challenge:** Performance likely drops when we train on one type of movement (e.g., Overt) and test on the other (Imagined). This highlights that the brain patterns aren't identical and that building BCIs that work well for imagined movements (often the target application) requires careful consideration[cite: 121]. It might suggest that training data ideally should match the intended use case, or that more advanced techniques are needed for better generalization.
* **Informative Brain Regions:** The SVM weight analysis should point towards specific EEG channels (and thus underlying brain regions) that are crucial for distinguishing left/right intentions[cite: 121]. We'd expect these to relate to motor cortex areas, consistent with neuroscience. Differences in weight patterns between Overt and Imagined conditions could also be informative.
* **Regularization Insights:** Comparing the optimal regularization parameters ($\alpha$) chosen during cross-validation for the Overt vs. Imagined datasets might offer clues about the data characteristics[cite: 121]. For instance, a different optimal $\alpha$ might suggest differences in noise levels or the inherent complexity of the separation task between the two conditions.

**Limitations & Challenges:**
It's important to be realistic. While promising, this approach has limitations:
1.  **Inter-Subject Variability:** Brain signals vary significantly from person to person (and even day to day for the same person). A model trained on one subject might not work well for another without recalibration.
2.  **Real-Time Performance:** For a practical BCI, decoding needs to happen quickly. The computational demands of SVM training and prediction, especially with cross-validation, need to be optimized for real-time use.
3.  **Generalization:** Our current setup focuses on left vs. right movement. How well would it generalize to other movements or tasks? This often requires more data and potentially different model architectures.
4.  **Noise Sensitivity:** EEG is inherently noisy [@uts_bci_landscape_2025]. While SVMs have some robustness, performance can still be affected by artifacts and signal quality.

## Comparison with Previous Work

Our work builds upon a rich history of BCI research [@emotiv_bci_guide_nodate; @pubmed_eeg_bci_overview_2010]. Many studies have explored EEG-based movement decoding using various algorithms. SVMs have been a popular choice due to their theoretical grounding and practical performance [@hearst_support_1998; @jakkula_svm_tutorial]. We aim to contribute by systematically evaluating linear SVMs with rigorous two-level cross-validation across overt and imagined movement conditions, providing detailed performance metrics and channel importance analysis. Our results should be comparable to, and hopefully advance upon, similar studies in the field.

# Conclusion

This project explores the exciting possibility of decoding movement intentions directly from brain activity using EEG and SVMs. By analyzing data from both actual and imagined movements, we aim to understand the capabilities and limitations of this approach for building practical Brain-Computer Interfaces.

Our analysis, focusing on linear SVMs evaluated through two-level cross-validation, is expected to show that distinguishing movement intentions is feasible, although performance may vary between overt and imagined conditions and across different training/testing scenarios. The analysis of SVM weights should also shed light on the neural underpinnings of movement intention as captured by EEG.

While challenges like inter-subject variability and real-time implementation remain, the findings underscore the potential of machine learning for developing powerful assistive technologies and neurorehabilitation tools [@emotiv_bci_guide_nodate; @uts_bci_landscape_2025; @frontiers_bci_prosthesis_2017; @pubmed_eeg_bci_overview_2010; @mdpi_bci_rehab_2023].

Future directions could involve:
1.  Improving real-time processing speed.
2.  Developing methods to reduce the need for extensive user-specific calibration.
3.  Exploring more complex models, such as non-linear SVM kernels or deep learning architectures.
4.  Testing and adapting these methods for individuals with motor impairments who could benefit most from BCI technology.

Ultimately, work like this moves us closer to a future where technology can seamlessly interface with the human brain to restore function and enhance capabilities.

# References

::: {#refs}
:::